
```{r echo=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  comment = NA,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
```
:::{.callout-note}
## Learning objective:

- Build a ML model for predicting whether a person has diabetes or not;

:::

:::{.callout-tip}
## Exercise:

In this case study, you could make predictions about whether a patient will develop diabetes or not based on their medical and demographic variables. What kind of model will you build?
:::

:::{.callout-caution collapse="true"}
### Solution

Unlike the first case study, when we built regression models to predict a numeric or continuous variable, in this case study we are going to build classification models, to predict the class: diabetes or no diabetes.
:::

## What is a classifier?

---

A classifier is some kind of rule / black box / widget that you can feed a new example and it will spit out whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.

![A classifier for cats and not cats.](../fig/CatNotCat.jpg)

You can have classifiers for anything you can have a yes/no answer to, e.g.

- Is this a cat? ðŸ±
- Do these test results indicate cancer? ðŸš‘
- Is this email spam or not spam? ðŸ“§

You can also have classifiers that categorise things into multiple (more than two) categories e.g.

- Which animal is this, out of the 12 animals I have trained my model on? ðŸ±
- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? ðŸš‘
- Is this email important, not important but not spam, or spam? ðŸ“§

It is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.

## Model evaluation (classification)

---

For now, let's imagine we have a classifier already. How can we test it to see how good it is?
A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.

![Demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/_CatConfusion.jpg)

### Confusion Matrix

---

When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares "the truth" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and if we correctly label something which is not a cat, as not a cat, then this is a true negative.
In our case, the confusion matrix will look like this:

- **true positive (TP)** : Diabetic correctly identified as diabetic
- **true negative (TN)** : Healthy correctly identified as healthy
- **false positive (FP)** : Healthy incorrectly identified as diabetic
- **false negative (FN)** : Diabetic incorrectly identified as healthy

### Some common classification metrics

---

Don't worry if you forget some of these - there are so many different words used to describe different ways to divide up the confusion matrix, it can get very confusing. I swear each time [I just look up wikipedia again](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) to figure out which part of the confusion matrix to look at. There are even more there that we won't even bother talking about here.

:::{.border}
#### **Accuracy**:
How often does the classifier label examples correctly? Objective: maximize. Example:

$$\frac{TP+TN}{TP+TN+FP+FN} = \frac{\text{Correctly labelled examples}}{\text{All examples}}=\frac{31+52}{31+52+10+7}=83\%$$

Accuracy is the opposite of the misclassification rate. So,

$$\text{Misclassification rate} = 1 - \text{Accuracy} = \frac{\text{Incorrectly labelled examples}}{\text{All examples}}$$
:::


:::{.border}
#### **Precision**:
What fraction of things labelled as a cat were actually cats? Objective: maximize. Example:

$$\frac{TP}{TP+FP} = \frac{\text{Correctly labelled cats}}{\text{All things labelled as cats}}=\frac{31}{31+10}=76\%$$
:::


:::{.border}
#### **Sensitivity / Recall**:
How often does the classifier label a cat as a cat? Objective: maximize. Example:

$$\frac{TP}{TP+FN} = \frac{\text{Correctly labelled cats}}{\text{All true cats}}=\frac{31}{31+7}=81\%$$
:::


:::{.border}
#### **Specificity**:
How often does it label a not-cat as a not-cat? Objective: maximize. Example:

$$\frac{TN}{TN+FP} = \frac{\text{Correctly labelled not-cats}}{\text{All true not-cats}}=\frac{52}{52+10}=84\%$$
:::


:::{.border}
#### **F1-score**:

This is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;

$$\frac{1}{F_1} = \frac{1}{2}\left(\frac{1}{\text{Precision}}+\frac{1}{\text{Sensitivity}}\right)$$


So that

$$F_1 = 2\cdot\left(\frac{1}{\frac{1}{81\%}+\frac{1}{83\%}}\right) = 82\%$$
:::


### AUC: Area under the curve

---

A good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.

To capture this balance, we often use a Reciever Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.

![A Reciever Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/_CatArea.jpg)

>For additional discussion of classification error metrics, see [Tharwat 2018](https://doi.org/10.1016/j.aci.2018.08.003), for example.

:::{.callout-tip}
### Challenge 1

- In the case of patients with a rare disease, what can be the problem of using accuracy to evaluate the performance of a machine learning model.
:::

:::{.callout-caution collapse="true"}
### Solution
Accuracy is calculated as the (TP + TN)/(total) number of cases in the dataset. If you have very few positive cases, such as when working with a rare disease, the numerator of this fraction will be dominated by the true negatives you accurately predict in your dataset - so not very informative when assessing whether your classifier predicts the disease well at all!
:::

```{r libraries, message=FALSE}
library(tidyverse)
library(tidymodels)
library(workflows)
library(tune)
theme_set(theme_minimal())

diabetes_rec <- readRDS("../_models/diabetes_rec.rds")
diabetes_folds <- readRDS("../_models/diabetes_folds.rds")
```

### Tree-based models

### Logistic regression

## Tune model hyperparameters

---

Some model parameters cannot be learned directly from a dataset during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of randomly selected variables to be considered at each split in a tree-based model (called `mtry` in tidymodels).

Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on a resampled data set (like the cross-validation folds we have previously created) and measuring how well all these models perform. This process is called ***tuning**.

You can identify which parameters to `tune()` in a model specification.

We will focus on 

```{r tune}
rf_model_diabetes <- 
  # specify that the model is a random forest
  rand_forest(mtry = tune()) %>%
  # specify that the `mtry` parameter needs to be tuned
  # select the engine/package that underlies the model
  set_engine("ranger") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("classification") 
```

> **Note** Nothing about this model specification is specific to the diabetes dataset.

### Find which parameters will give the model its best accuracy

:::{.borders}
- Try different values and measure their performance;
- Find good values for these parameters;
- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.
:::

You have a couple of options for how to choose which possible values for the tuning parameters to try. 

- We can use the function `tune_grid()` to tune either a workflow or a model specification with a set of resampled data, such as the cross-validation we created. Grid search, combined with resampling, requires fitting a lot of models!
These models donâ€™t depend on one another and can be run in parallel.

```{r tune_grid}



#Speed up computation
cores <- parallel::detectCores(logical = FALSE) #detect the number of available CPU cores on your machine 
cl <- parallel::makePSOCKcluster(cores) #creates a cluster of worker processes
doParallel::registerDoParallel(cl) #enable parallel computing

set.seed(245)

rf_grid <- tune_grid(
  rf_model_diabetes,  #your model
  diabetes_rec,       #your recipe
  resamples = diabetes_folds, #your resampling
  metrics = metric_set(accuracy, roc_auc, sensitivity, specificity))

# Shut down parallel backend with:
foreach::registerDoSEQ()
parallel::stopCluster(cl)

rf_grid

```


Use `collect_metrics` to extract the metrics calculated from the cross-validation performance across the different values of the parameters:
```{r collect tuning metrics}
#collect metrics
rf_grid %>%
  collect_metrics()

#see which model performed the best, in terms of some given metric
rf_grid %>%
  show_best("roc_auc")

rf_grid %>%
  show_best("accuracy")
```


:::{.callout-tip}
### Challenge X

Use `tune_grid` and `collect_metrics` to tune a workflow. Hints:

Use `workflow()` to define the workflow:
```{r challenge x worflow}
#set the workflow

#add the recipe

#add the model
```
:::

:::{.callout-caution collapse="true"}
### Solution

```{r challenge x worflow solution}
#set the workflow
rf_worflow <- workflow() %>%
#add the recipe
add_recipe(diabetes_rec) %>%
#add the model
  add_model(rf_model_diabetes)

#tune the workflow
set.seed(22)

rf_tune_diabetes <- rf_worflow %>%
  tune_grid(resamples = diabetes_folds,
            metrics = metric_set(accuracy, roc_auc, sensitivity, specificity))

rf_tune_diabetes %>%
  collect_metrics()
```
:::

Let's visualise our results:

```{r autoplot}
autoplot(rf_grid)
autoplot(rf_tune_diabetes)
```
### Updating the workflow

---




