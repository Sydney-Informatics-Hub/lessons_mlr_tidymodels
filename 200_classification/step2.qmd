
:::{.callout-note}
## Learning objective:

- Build a ML model for predicting whether a person has diabetes or not;

:::

:::{.callout-tip}
## Exercise:

In this case study, you could make predictions about whether a patient will develop diabetes or not based on their medical and demographic variables. What kind of model will you build?
:::

:::{.callout-caution collapse="true"}
### Solution

Unlike the first case study, when we built regression models to predict a numeric or continuous variable, in this case study we are going to build classification models, to predict the class: diabetes or no diabetes.
:::

## What is a classifier?

---

A classifier is some kind of rule / black box / widget that you can feed a new example and it will spit out whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.

![A classifier for cats and not cats.](../fig/CatNotCat.jpg)

You can have classifiers for anything you can have a yes/no answer to, e.g.

- Is this a cat? 🐱
- Do these test results indicate cancer? 🚑
- Is this email spam or not spam? 📧

You can also have classifiers that categorise things into multiple (more than two) categories e.g.

- Which animal is this, out of the 12 animals I have trained my model on? 🐱
- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? 🚑
- Is this email important, not important but not spam, or spam? 📧

It is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.

## Model evaluation (classification)

---

For now, let's imagine we have a classifier already. How can we test it to see how good it is?
A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.

![Demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/_CatConfusion.jpg)

### Confusion Matrix

---

When applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares "the truth" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and if we correctly label something which is not a cat, as not a cat, then this is a true negative.
In our case, the confusion matrix will look like this:

- **true positive (TP)** : Diabetic correctly identified as diabetic
- **true negative (TN)** : Healthy correctly identified as healthy
- **false positive (FP)** : Healthy incorrectly identified as diabetic
- **false negative (FN)** : Diabetic incorrectly identified as healthy

### Some common classification metrics

---

Don't worry if you forget some of these - there are so many different words used to describe different ways to divide up the confusion matrix, it can get very confusing. I swear each time [I just look up wikipedia again](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) to figure out which part of the confusion matrix to look at. There are even more there that we won't even bother talking about here.

:::{.border}
#### **Accuracy**:
How often does the classifier label examples correctly? Objective: maximize. Example:

$$\frac{TP+TN}{TP+TN+FP+FN} = \frac{\text{Correctly labelled examples}}{\text{All examples}}=\frac{31+52}{31+52+10+7}=83\%$$

Accuracy is the opposite of the misclassification rate. So,

$$\text{Misclassification rate} = 1 - \text{Accuracy} = \frac{\text{Incorrectly labelled examples}}{\text{All examples}}$$
:::


:::{.border}
#### **Precision**:
What fraction of things labelled as a cat were actually cats? Objective: maximize. Example:

$$\frac{TP}{TP+FP} = \frac{\text{Correctly labelled cats}}{\text{All things labelled as cats}}=\frac{31}{31+10}=76\%$$
:::


:::{.border}
#### **Sensitivity / Recall**:
How often does the classifier label a cat as a cat? Objective: maximize. Example:

$$\frac{TP}{TP+FN} = \frac{\text{Correctly labelled cats}}{\text{All true cats}}=\frac{31}{31+7}=81\%$$
:::


:::{.border}
#### **Specificity**:
How often does it label a not-cat as a not-cat? Objective: maximize. Example:

$$\frac{TN}{TN+FP} = \frac{\text{Correctly labelled not-cats}}{\text{All true not-cats}}=\frac{52}{52+10}=84\%$$
:::


:::{.border}
#### **F1-score**:

This is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;

$$\frac{1}{F_1} = \frac{1}{2}\left(\frac{1}{\text{Precision}}+\frac{1}{\text{Sensitivity}}\right)$$


So that

$$F_1 = 2\cdot\left(\frac{1}{\frac{1}{81\%}+\frac{1}{83\%}}\right) = 82\%$$
:::


### AUC: Area under the curve

---

A good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.

To capture this balance, we often use a Reciever Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.

![A Reciever Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/_CatArea.jpg)

>For additional discussion of classification error metrics, see [Tharwat 2018](https://doi.org/10.1016/j.aci.2018.08.003), for example.

:::{.callout-tip}
### Challenge 1

- In the case of patients with a rare disease, what can be the problem of using accuracy to evaluate the performance of a machine learning model.
:::

:::{.callout-caution collapse="true"}
### Solution
Accuracy is calculated as the (TP + TN)/(total) number of cases in the dataset. If you have very few positive cases, such as when working with a rare disease, the numerator of this fraction will be dominated by the true negatives you accurately predict in your dataset - so not very informative when assessing whether your classifier predicts the disease well at all!
:::

```{r libraries, message=FALSE}
library(tidyverse)
library(tidymodels)
library(workflows)
library(tune)
library(vip)
library(ParallelLogger)
library(doParallel)
library(workflowsets)
theme_set(theme_minimal())

diabetes_rec <- readRDS("../_models/diabetes_rec.rds")
diabetes_folds <- readRDS("../_models/diabetes_folds.rds")
d_na_train <- readRDS("../_models/d_na_train.Rds")
d_na_test <- readRDS("../_models/d_na_test.Rds")
diabetes_split <- readRDS("../_models/diabetes_split.Rds")
```

```{r setup, echo=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  comment = NA,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
#Speed up computation
cores <- parallel::detectCores(logical = FALSE) #detect the number of available CPU cores on your machine 
cl <- parallel::makePSOCKcluster(cores) #creates a cluster of worker processes 
doParallel::registerDoParallel(cl)
```

## Some classification models

### Tree-based models

---

A tree-based model is a type of algorithm that creates a tree-like structure to make predictions about a certain outcome, such as whether a customer will buy a product or not. The tree structure consists of nodes that represent different features, and the algorithm uses these features to split the data into smaller and smaller subsets. Each subset is then assigned a label based on the majority of its observations, and this process continues until the algorithm reaches a stopping criterion or has created a fully-grown tree. Once the tree is created, it can be used to make predictions by following the path through the tree that corresponds to a given set of input features. Tree-based models are simple and intuitive to understand, and can be used for both classification and regression tasks.

- **Decision trees** are a simple type of tree-based model that use a hierarchical structure of nodes to make predictions about a certain outcome. The process continues until a stopping criterion is met, such as a maximum tree depth or a minimum number of observations per leaf node, and it can predict the outcome. A single decision tree may not be accurate enough for many real-world problems;

- **Random forest** overcomes this limitation by building many decision trees, each using a randomly selected subset of the data and features, and then combining their predictions to make a final prediction. 


### Logistic regression

---

Logistic regression is a type of algorithm that is used to predict a binary outcome, such as whether a patient is likely to develop diabetes or no. It works by creating a mathematical function that predicts the probability of an observation belonging to a certain class (e.g., diabetes or not diabetes). The function takes into account one or more input variables, such as the patients's age, gender, or body mass index. The output of the function is a value between 0 and 1, which represents the probability of the observation belonging to the positive class (e.g., developing diabetes). To make a prediction, the algorithm compares the predicted probability to a threshold value (e.g., 0.5), and assigns the observation to the positive class if the probability is greater than the threshold, and to the negative class otherwise. 

*Regularization* is a technique that can be used to prevent overfitting of the model. A regularized logistic regression model, is a logistic classifier that has been modified to include a regularization term. This is done by adding a penalty to the model that discourages it from giving too much importance to any variable.

There are several regularized regression models, defined with the `mixture` parameter:

- **Ridge** regularization encourages the model to have small coefficient values (`mixture = 0`);
- **Lasso** regularization encourages the model to set some of the coefficients to zero, which performs feature selection. This can help improve the model's interpretability and reduce the impact of irrelevant features on the model's performance (`mixture = 1`);
- **Elastic Net** regularization combines Ridge and Lasso regularization by adding a penalty term that is a weighted average of both penalties. This approach can provide the benefits of both Ridge and Lasso regularization, such as feature selection and coefficient shrinkage (`mixture` between 0 and 1).

## Tune model hyperparameters

---

Some model parameters cannot be learned directly from a dataset during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of randomly selected variables to be considered at each split in a tree-based model (called `mtry` in tidymodels).

Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on a resampled data set (like the cross-validation folds we have previously created) and measuring how well all these models perform. This process is called ***tuning**.

You can identify which parameters to `tune()` in a model specification.

We can specify a random forest classifier with the following hyperparameters:

- **mtry**: the number of predictors that will be randomly sampled at each split when creating the tree models;
- **trees**: the number of decision trees to fit and ultimately average;
- **min_n**: The minimum number of data points in a node that are required for the node to be split further.

To specify a random forest model with tidymodels, we need the `rand_forest()` function. The hyperparameters of the model are arguments within the `rand_forest()` function and may be set to specific values. However, if tuning is required, then each of these parameters must be set to `tune()`.

We will be using the ranger engine. This engine has an optional importance argument which can be used to track variable importance measures. In order to make a variable importance plot with `vip()`, we must add `importance = 'impurity'` inside our `set_engine()` function:

```{r tune models}
rf_model_diabetes <- 
  # specify that the model is a random forest and which hyperparameters need to be tuned
  rand_forest(mtry = tune(),
              trees = tune(),
              min_n = tune()) %>%
  # select the engine/package that underlies the model
  set_engine("ranger", importance = "impurity") %>% #get variable importance scores
  # choose either the continuous regression or binary classification mode
  set_mode("classification") 

rlr_model_diabetes <- 
  logistic_reg(mixture = tune(), penalty = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")
```

> **Note** Nothing about this model specification is specific to the diabetes dataset.

### Find which parameters will give the model its best accuracy

:::{.borders}
- Try different values and measure their performance;
- Find good values for these parameters;
- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.
:::

You have a couple of options for how to choose which possible values for the tuning parameters to try. One of these options is **creating a random grid of values**. Random grid search is implemented with the `grid_random()` function in tidymodels, taking a sequence of hyperparameter names to create the grid. It also has a size parameter that specifies the number of random combinations to create.

The `mtry()` hyperparameter requires a pre-set range of values to test since it cannot exceed the number of columns in our data. When we add this to `grid_random()` we can pass `mtry()` into the `range_set()` function and set a range for the hyperparameter with a numeric vector.

In the code below, we set the range from 3 to 6. This is because we have 9 columns in diabetes_data and we would like to test `mtry()` values somewhere in the middle between 1 and 9, trying to avoid values close to the ends.

When using `grid_random()`, it is suggested to use set.seed() for reproducibility.

We can then use the function `tune_grid()` to tune either a workflow or a model specification with a set of resampled data, such as the cross-validation we created. Grid search, combined with resampling, requires fitting a lot of models!
These models don’t depend on one another and can be run in parallel.

```{r tune_grid}
set.seed(314)

rf_grid <- grid_random(mtry() %>% range_set(c(3, 6)),
                       trees(),
                       min_n(),
                       size = 10)

#View grid
rf_grid


#Tune random forest model 
rf_tune_model <- tune_grid(
  rf_model_diabetes,  #your model
  diabetes_rec,       #your recipe
  resamples = diabetes_folds, #your resampling
  metrics = metric_set(accuracy, roc_auc, sensitivity, specificity),
  grid = rf_grid)

rf_tune_model
```


Use `collect_metrics` to extract the metrics calculated from the cross-validation performance across the different values of the parameters:
```{r collect tuning metrics}
#collect metrics
rf_tune_model %>%
  collect_metrics()

#see which model performed the best, in terms of some given metric
rf_tune_model %>%
  show_best("roc_auc")
```


:::{.callout-tip}
### Challenge X

Use `tune_grid` and `collect_metrics` to tune a workflow. Hints:

Use `workflow()` to define the workflow:
```{r challenge x worflow}
#set the workflow

#add the recipe

#add the model
```
:::

:::{.callout-caution collapse="true"}
### Solution

```{r challenge x worflow solution}
#set the workflow
rf_workflow <- workflow() %>%
#add the recipe
add_recipe(diabetes_rec) %>%
#add the model
  add_model(rf_model_diabetes)

#tune the workflow
set.seed(22)

rf_tune_wf <- rf_workflow %>%
  tune_grid(resamples = diabetes_folds,
            metrics = metric_set(accuracy, roc_auc, sensitivity, specificity),
            grid = rf_grid)

rf_tune_wf %>%
  collect_metrics()

rf_tune_wf %>%
  show_best("roc_auc")
```
:::

Let's visualise our results:

```{r autoplot}
autoplot(rf_tune_model)
autoplot(rf_tune_wf)
```

We can also specify the values of the parameters to tune with an tuning grid, entered as a data frame. It contains all the combinations of parameters to be tested. For regularized logistic regression, we test eleven values of mixture:

```{r tune rlr}
#set the grid
rlr_grid <- data.frame(mixture = seq(0, 1, 0.1),
                       penalty = seq(0, 1, 0.1))
rlr_grid

set.seed(435)

##use tune_grid() for hyperparameters tuning, doing cross validation for each row of the tuning grid
rlr_tune_model <- tune_grid(
  rlr_model_diabetes,  #your model
  diabetes_rec,       #your recipe
  resamples = diabetes_folds, #your resampling
  grid = rlr_grid)

rlr_tune_model %>%
  collect_metrics()

rlr_tune_model %>%
  show_best("roc_auc")
```

### The workflowsets package

---

Tidymodels allows us to perform all of the above steps in a much faster way with the workflowsets package:

```{r workflow_set}
diabetes_wf_set <- workflow_set(list(diabetes_rec),  #list of recipes
             list(rf_model_diabetes, rlr_model_diabetes), #list of models
             cross = TRUE) #all combinations of the preprocessors and models are used to create the workflows
  
diabetes_wf_set

# setting the parameters on each workflow seperately
rf_params <- diabetes_wf_set %>%
  extract_workflow("recipe_rand_forest") %>%
  parameters() %>%
  update(mtry = mtry(c(3,6)),
         trees = trees(c(1, 10)),
         min_n = min_n(c(1, 10)))

rlr_params <- diabetes_wf_set %>%
  extract_workflow("recipe_logistic_reg") %>%
  parameters() %>%
  update(mixture = mixture(c(0, 1)),
         penalty = penalty(c(1,2)))
  
diabetes_wf_set <- diabetes_wf_set %>%
  workflow_map("tune_grid", # the first argument is a function name from the tune package (tune_grid(), fit_resamples()..)
               resamples = diabetes_folds,
               verbose = TRUE) 


diabetes_wf_set
```
The results column contains the results of each call to `tune_grid()` for the workflows. From these results, we can get quick assessments of how well these models classified the data:

```{r wf_set results}
#To get the rankings of the models (and their tuning parameter sub-models) as a data frame:
rank_results(diabetes_wf_set, rank_metric = "roc_auc")

#plot the results
autoplot(diabetes_wf_set, metric = "roc_auc")
```
This shows the results for all tuning parameter combinations for each model. It looks like the random forest model did well. We can
use the `extract_workflow_set_result()` function to extract the tuning results:
```{r set results}
best_results <- diabetes_wf_set %>%
  extract_workflow_set_result("recipe_rand_forest") %>%
  select_best(metric="roc_auc")

best_results
```


### Update and fit the workflow

---

The last step in hyperparameter tuning is to use `finalize_workflow()` to add our optimal model to our workflow object, and apply the `last_fit()` function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set:

```{r final workflow rf}
final_diabetes_fit <- diabetes_wf_set %>%
  extract_workflow("recipe_rand_forest") %>%
  finalize_workflow(best_results) %>%
  last_fit(diabetes_split)

final_diabetes_fit
```


Since we supplied the train/test object when we fit the workflow, the metrics are evaluated on the test set. Now when we use the `collect_metrics()` function (the same we used when tuning our parameters) to extract the performance of the final model (since `rf_fit_final` now consists of a single final model) applied to the test set:

```{r model performance}
test_performance <- final_diabetes_fit %>% collect_metrics()
test_performance
```

We can plot the ROC curve to visualize test set performance of our random forest model, and generate a confusion matrix:


**Note** In R, factor levels are ordered alphabetically by default, which means that "no" comes first before "yes" and is considered the level of interest or positive case. Use the argument `event_level = "second"` to alter this as needed.

```{r visualise performance}
#ROC curve
  collect_predictions(final_diabetes_fit) %>%
  roc_curve(truth  = diabetes, event_level="second", estimate = .pred_pos) %>%  #specify which level of truth to                                                                                       consider as the "event"
                autoplot()
#confusion matrix
conf_matrix_rf <- final_diabetes_fit %>%
  collect_predictions() %>%
  conf_mat(truth = diabetes, estimate = .pred_class) 

conf_matrix_rf

conf_matrix_rf %>%
  autoplot()
```

### Variable importance 

---

In order to visualize the variable importance scores of our random forest model, we will need to manually train our workflow object with the `fit()` function on the training data, then extract the trained model with the `pull_workflow_fit()` function, and next passing the trained model to the `vip()` function:

```{r fit rf}
#extract the final workflow
final_workflow <- diabetes_wf_set %>%
  extract_workflow("recipe_rand_forest") %>%
  finalize_workflow(best_results)

#fit on the training data
wf_fit <- final_workflow %>%
  fit(data = d_na_train)
#extract the trained model
wf_fit <- wf_fit %>% 
          pull_workflow_fit()
#plot variable importance
vip(wf_fit)
```

This returns a ggplot object with the variable importance scores from our model. 

We see from the results below, that the glucose concentration, body mass index and age are the most important predictors of diabetes.

- *


