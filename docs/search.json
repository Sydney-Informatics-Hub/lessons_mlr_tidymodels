[
  {
    "objectID": "400_analysis/step1.html",
    "href": "400_analysis/step1.html",
    "title": "New SIH project",
    "section": "",
    "text": "This is step 1"
  },
  {
    "objectID": "400_analysis/step1.html#quarto",
    "href": "400_analysis/step1.html#quarto",
    "title": "New SIH project",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "400_analysis/step1.html#running-code",
    "href": "400_analysis/step1.html#running-code",
    "title": "New SIH project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\nCode\n2 * 2\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "400_analysis/step2.html",
    "href": "400_analysis/step2.html",
    "title": "New SIH project",
    "section": "",
    "text": "This is step 2"
  },
  {
    "objectID": "400_analysis/step2.html#quarto",
    "href": "400_analysis/step2.html#quarto",
    "title": "New SIH project",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "400_analysis/step2.html#running-code",
    "href": "400_analysis/step2.html#running-code",
    "title": "New SIH project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\nCode\n2 * 2\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with R and the tidyverse",
    "section": "",
    "text": "This is the landing page for the Sydney Informatics Hub’s “Machine learning with R and the tidyverse”."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "The Sydney Informatics Hub is a Core Research Facility of the University of Sydney.\nThe use of the SIH services including the Artemis HPC and associated support and training warrants acknowledgement in any publications, conference proceedings or posters describing work facilitated by these services.\nThe continued acknowledgement of the use of SIH facilities ensures the sustainability of our services."
  },
  {
    "objectID": "about.html#suggested-wording",
    "href": "about.html#suggested-wording",
    "title": "Sydney Informatics Hub",
    "section": "Suggested wording",
    "text": "Suggested wording\n\nGeneral acknowledgement:\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\n\n\nAcknowledging specific staff:\nThe authors acknowledge the technical assistance of (name of staff) of the Sydney Informatics Hub, a Core Research Facility of the University of Sydney.\nFor further information about acknowledging the Sydney Informatics Hub, please contact us at sih.info@sydney.edu.au."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step1.html",
    "href": "100_data_cleaning_scripts_EDA/step1.html",
    "title": "New SIH project",
    "section": "",
    "text": "This is step 1"
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step1.html#quarto",
    "href": "100_data_cleaning_scripts_EDA/step1.html#quarto",
    "title": "New SIH project",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step1.html#running-code",
    "href": "100_data_cleaning_scripts_EDA/step1.html#running-code",
    "title": "New SIH project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\nCode\n2 * 2\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step2.html",
    "href": "100_data_cleaning_scripts_EDA/step2.html",
    "title": "New SIH project",
    "section": "",
    "text": "This is step 2"
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step2.html#quarto",
    "href": "100_data_cleaning_scripts_EDA/step2.html#quarto",
    "title": "New SIH project",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "100_data_cleaning_scripts_EDA/step2.html#running-code",
    "href": "100_data_cleaning_scripts_EDA/step2.html#running-code",
    "title": "New SIH project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\nCode\n2 * 2\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "000_scoping/scoping.html",
    "href": "000_scoping/scoping.html",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "",
    "text": "Clients: X, Y, Z\nFaculty: X\nSchool: ?\nResearch centre: ?\nCollaborators and their affilications: ?"
  },
  {
    "objectID": "000_scoping/scoping.html#project-scoping-details",
    "href": "000_scoping/scoping.html#project-scoping-details",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "Project scoping details",
    "text": "Project scoping details\n\nScope prepared by X\nScope prepared on\nProject manager\n\n\nResearcher Availability\n\nWho is the active person helping?\nHow available are they to participate in this study?"
  },
  {
    "objectID": "000_scoping/scoping.html#project-summary",
    "href": "000_scoping/scoping.html#project-summary",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "2. Project summary",
    "text": "2. Project summary\n\nResearch context\nThis research aims to:\n\nX\nB\n\n\n\nClient needs\n\nA\nB\nC\n\n\n\nCurrent data\n\nX"
  },
  {
    "objectID": "000_scoping/scoping.html#project-implementation",
    "href": "000_scoping/scoping.html#project-implementation",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "3. Project Implementation",
    "text": "3. Project Implementation\n\nProject plan\n\nWhat needs to be done?\nWhat’s the timeline for each step?\n\n\n\nScheduling\n\nAny notes?\n\n\n\nDeliverables\n\nA\nB\nC\n\n\n\nSIH skills required\n\nPython/R/SQL?\nXXX\nyyyy\n\n\n\nIn scope\n\n\nOut of scope\n\nMore than X weeks FTE work\n\n\n\nAcceptance Criteria\n\nx\n\n\n\nHandover plan\n\nX"
  },
  {
    "objectID": "000_scoping/scoping.html#project-evaluation",
    "href": "000_scoping/scoping.html#project-evaluation",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "4. Project Evaluation",
    "text": "4. Project Evaluation\n\nSIH Benefits (Summary)\n\nX/\n\n\n\nCost (FTE weeks)\n\nX weeks FTE\n\n\nSIH staff will fill out the below"
  },
  {
    "objectID": "000_scoping/scoping.html#essential-criteria",
    "href": "000_scoping/scoping.html#essential-criteria",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "5. Essential criteria",
    "text": "5. Essential criteria\n\n\n\nSydney Researcher\nY:\n\n\nClear project statement\nY\n\n\nData quality\nY\n\n\nSolvable\nY\n\n\nDomain Expert\nY\n\n\nPlanned\nY\n\n\nStrategic Plan\nY\n\n\nData Science\nY\n\n\nPriority\nY: Health\n\n\nDeliverables\nY\n\n\nInfrastructure\nY"
  },
  {
    "objectID": "000_scoping/scoping.html#value-and-impact",
    "href": "000_scoping/scoping.html#value-and-impact",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "6. Value and Impact",
    "text": "6. Value and Impact\n\nDrives the success of larger groups, larger collaborations or high-profile projects.\n\nScore: X/10\nDetails:\n\nName of Research Group/Centre:\nName of Collaboration:\n\n\n\n\nPublications that are high-impact - foundational, highly cited, with potential for wider adoption of methods and knock-on outcomes.\n\nScore: X/10\n\n\nTarget Journal: Nature (Impact Factor 40000)\n\n\nName of Authors: Amazing researcher, (name of SIH data analyst?)\nProposed Paper Title: How to write a Science paper in 5 days.\n\n\n\nPotential for patents, start-ups, open-source code, policy influence.\n\nPolicy influence X/10\n\n\n\nDevelop funding opportunities for the university and/or SIH, such as large CoE-scale grants and further involvement with successful and well funded groups.\n\nScore: X/10\nName of Grant Application:\nDate of Proposed Submission:\n\n\n\nTotal score\nX/40 (refer to Rubric for scoring guide)"
  },
  {
    "objectID": "000_scoping/scoping.html#kpi",
    "href": "000_scoping/scoping.html#kpi",
    "title": "PIPE-XXX: PROJECT TITLE",
    "section": "7. KPI",
    "text": "7. KPI\n\n\n\nPapers SIH Coauthors (*measure only, not KPI)\nN\n\n\nNumber of Papers: Papers SIH Acknowledged\nY - Number of Papers: 1000\n\n\nCollaborators who publish\nName: Amazing Researcher\n\n\nCollaborators who have grants\nName: Amazing Researcher\n\n\nGrant applications created\nY:\n\n\nGrants project supporting\nName of Grant:\n\n\nSoftware released for public use\nY/N: TBA\n\n\nNew Clients\nY\n\n\nKPI Score\n?/6"
  },
  {
    "objectID": "100_dataset1/step1.html",
    "href": "100_dataset1/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nUse tidyverse functions for exploratory data analysis;\nIntroduce and explore the Ames Housing dataset.\nFirst, let’s load the required packages. We will use the tidyverse for general data processing and visualisation.\nWe will use the Ames housing data to explore different ML approaches to regression. This dataset was “designed” by Dean De Cock as an alternative to the “classic” Boston housing dataset, and has been extensively used in ML teaching. It is also available from kaggle as part of its advanced regression practice competition.\nThe Ames Housing Data Documentation file describes the independent variables presented in the data. This includes:\nWe will explore both the “uncleaned” data available from kaggle/UCI, and the processed data available in the AmesHousing package in R, for which documentation is available here. It can be useful for understanding what each of the independent variables mean."
  },
  {
    "objectID": "100_dataset1/step1.html#quarto",
    "href": "100_dataset1/step1.html#quarto",
    "title": "Sydney Informatics Hub",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "100_dataset1/step1.html#running-code",
    "href": "100_dataset1/step1.html#running-code",
    "title": "Sydney Informatics Hub",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n2 * 2\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "100_dataset1/step2.html",
    "href": "100_dataset1/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nFrom base R to tidymodels;\nSplit our data into training and test sets;\nPreprocess the training data;\nSpecify a linear regression model;\nTrain our model on the training data;\nTransform the test data and obtain predictions using our trained model.\nLoad in the packages we’ll be using for modelling:"
  },
  {
    "objectID": "100_dataset1/step2.html#quarto",
    "href": "100_dataset1/step2.html#quarto",
    "title": "Sydney Informatics Hub",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "100_dataset1/step2.html#running-code",
    "href": "100_dataset1/step2.html#running-code",
    "title": "Sydney Informatics Hub",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\nlibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n2 * 2\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "400_dataset4/step1.html",
    "href": "400_dataset4/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Predict whether a cancer is malignant or benign from biopsy details\n\n\n\n\n\n\nExercise:\n\n\n\nThis dataset, called the Breast Cancer Wisconsin (Diagnostic) Data Set, includes features computed from digitized images of biopsies. If you want to predict whether a cancer is malignant or benign from biopsies details, which model can you build?\n\n\nLoad the libraries:\n\nlibrary(RCurl)\n\nLoad the data:\n\n#load the data into a tibble using the RCurl package\nUCI_data_URL <- getURL('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data') \n\n#create a list with the appropriate column names \nnames <- c('id_number', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst') \n\n#load the column names into a data frame and set the column names\nbreast_cancer <- read.table(textConnection(UCI_data_URL), sep = ',', col.names = names) \n\n#discard id_number column\nbreast_cancer$id_number <- NULL"
  },
  {
    "objectID": "400_dataset4/step1.html#quarto",
    "href": "400_dataset4/step1.html#quarto",
    "title": "Sydney Informatics Hub Machine Learning with R and tidymodels",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "400_dataset4/step1.html#running-code",
    "href": "400_dataset4/step1.html#running-code",
    "title": "Sydney Informatics Hub Machine Learning with R and tidymodels",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\nCodelibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\nCode1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\nCode2 * 2\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "400_dataset4/step2.html",
    "href": "400_dataset4/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:"
  },
  {
    "objectID": "400_dataset4/step2.html#quarto",
    "href": "400_dataset4/step2.html#quarto",
    "title": "Sydney Informatics Hub Machine Learning with R and tidymodels",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "400_dataset4/step2.html#running-code",
    "href": "400_dataset4/step2.html#running-code",
    "title": "Sydney Informatics Hub Machine Learning with R and tidymodels",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\nCodelibrary(ggplot2)\nggplot(data = mtcars, \n       aes(x = mpg, y = hp, \n           col = as.factor(cyl))) + \n  geom_point() + \n  theme_minimal()\n\n\n\nCode1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\nCode2 * 2\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Machine learning with R and the tidyverse",
    "section": "Schedule",
    "text": "Schedule\nThis course is designed to be delivered either as 2 day face-to-face workshop or as four half day sessions delivered online.\n\n\n\nSession 1\n\n\n\n1:30 pm - 3:00 pm\nIntroduction to machine learning - Regrassion and Classification\n\n\n3:30 pm - 4:30 pm\nPredict the house sale price in Iowa - Exploratory data analysis (EDA)\n\n\n4:30 pm - 5:00 pm\nGet started with tidymodels\n\n\nSession 2\n\n\n\n1:30 pm - 2:00 pm\nPredict what makes a developer more likely to work remotely - EDA\n\n\n2:00 pm - 5:00 pm\nPreprocess with a recipe\n\n\nSession 3\n\n\n\n1:30 pm - 2:00 pm\nPredict which wine features are the best quality wine indicators - EDA\n\n\n2:00 pm - 5:00 pm\nCreate a modeling workflow\n\n\nSession 4\n\n\n\n1:30 pm - 2:00 pm\nPredict whether a cancer is malignant or benign from biopsy details - EDA\n\n\n2:00 pm - 5:00 pm\nWork with model hyperparameters\n\n\n\n\n\n\n\n\n\nPrerequisites\n\n\n\n\nThis course assumes intermediate R knowledge. This workshop is for you if: +You can use the magrittr pipe %>%; +You are familiar with functions from dplyr, tidyr, and ggplot2; +You can read data into R, transform and reshape data, and make a wide variety of graphs.\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning; -You need your own laptop with R and a few key packages installed. See setup instructions for more details."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nuse exploratory data analysis to prepare for predictive modeling\nexplore which modeling approaches to use for different kinds of data\npractice implementing supervised machine learning for classification and regression"
  },
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Question\n\n\n\n\nWhat packages do I need to install in order to attend the SIH’s Machine Learning with R course?"
  },
  {
    "objectID": "00_setup.html#installation-instructions",
    "href": "00_setup.html#installation-instructions",
    "title": "Sydney Informatics Hub",
    "section": "Installation instructions",
    "text": "Installation instructions\n\nif (getRversion() < 3.6){\n  stop(\"You need R >= 3.6 to attend the workshop. Please update R!\")\n}\n\nlist_of_pkgs <- c(\n  \"AmesHousing\",\n  \"tidyverse\",\n  \"readr\",\n  \"ggplot2\",\n  \"RCurl\"\n  )\n\nau_repo <- \"https://mirror.aarnet.edu.au/pub/CRAN/\"\n\n#install installr and rtools if windows\nif (.Platform$OS.type == \"windows\"){\n  install.packages(c(\"installr\"), repos = au_repo)\n  installr::install.Rtools(choose_version = F,\n                           check = T,\n                           GUI = T)\n  win_only <- c(\"installr\")\n} else {\n  win_only <- c()\n}\n\n# run the following line of code to install the packages you currently do not have\nnew_pkgs <- list_of_pkgs[!(list_of_pkgs %in% installed.packages()[,\"Package\"])]\nif(length(new_pkgs)) install.packages(new_pkgs,repos = au_repo)\n\n# install helper function to plot metrics\nlist_of_pkgs <- c(list_of_pkgs,\n                  win_only)\n\n#check everything is installed and write out an error message if it ain't.\nmissing_pkg <- list_of_pkgs[!(list_of_pkgs %in% installed.packages()[,\"Package\"])]\nif(length(missing_pkg)) {\n  print(\"=======================================================\")\n  print(\"Packages which didn't install properly:\")\n  print(missing_pkg)\n  print(\"Try to install them again or ask a helper for assistance.\")\n  print(\"=======================================================\")\n} else {\n  print(\"=======================================================\")\n  print(\"All packages installed properly! :)\")\n  print(\"=======================================================\")\n}\n\n[1] \"=======================================================\"\n[1] \"All packages installed properly! :)\"\n[1] \"=======================================================\"\n\n\n\n\n\n\n\n\nTo do before the workshop begins\n\n\n\n\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/;\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download;\n\n\nPlease copy and paste the code above in your Rstudio, run it and if there are any errors please let us know via email (sih.training@sydney.edu.au) BEFORE the workshop begins"
  },
  {
    "objectID": "300_dataset3/step1.html",
    "href": "300_dataset3/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Predict which wine features are the best quality wine indicators\n\n\n\n\n\n\nExercise:\n\n\n\nThe Wine Quality dataset contains information about various chemical properties of different wines, including their acidity, residual sugar, and alcohol content. This dataset can be used to build a model to predict which features are the best quality red wine indicators. What kind of model will you build?\n\n\nLoad the libraries:\n\nlibrary(readr)\n\nLoad the data:\n\nlibrary(readr)\nwine_data <- read_delim(\"../datasets/winequality-red.csv\", show_col_types = FALSE)"
  },
  {
    "objectID": "200_dataset2/step1.html",
    "href": "200_dataset2/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Predict what makes a developer more likely to work remotely\nStack Overflow is the world’s largest, most trusted online community for developers (I bet you have used it!) and every year there is a large survey of developers, to learn about developers’ opinions on different technologies, work habits, and so forth. You can find the link to the surveys here:\n\n\n\n\n\n\nLearning objective:\n\n\n\n\nYou are going to use data from the annual Developer Survey to build predictive models;\nYou’ll do exploratory data analysis to understand what’s in the dataset and how some of the quantities in the survey are distributed; -You’ll practice your machine learning skills by training classification/regression models.\n\n\n\n\n\n\n\n\n\nExercise:\n\n\n\nIn this case study, you will predict whether a developer works remotely or not (i.e., in their company offices) from characteristics of these developers, like experience and size of the company. In this analysis, we will assume that a software developer can either work remotely, or not. What kind of model will you build?\n\n\nLoad the libraries:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nLoad the data:\n\nlibrary(readr)\nStackSurvey <- read_csv(\"../datasets/stack_overflow.csv\", show_col_types = FALSE)\n\nStart off this modeling analysis by checking out how many remote and non-remote developers you have to work with, where they live, and how much experience they have:\n\n# Take a look at StackSurvey\nglimpse(StackSurvey)\n\nRows: 6,991\nColumns: 22\n$ respondent                           <dbl> 3, 15, 18, 19, 26, 55, 62, 71, 73…\n$ country                              <chr> \"United Kingdom\", \"United Kingdom…\n$ salary                               <dbl> 113750.000, 100000.000, 130000.00…\n$ years_coded_job                      <dbl> 20, 20, 20, 3, 16, 4, 1, 1, 20, 2…\n$ open_source                          <lgl> TRUE, FALSE, TRUE, FALSE, FALSE, …\n$ hobby                                <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, FAL…\n$ company_size_number                  <dbl> 10000, 5000, 1000, 10000, 10000, …\n$ remote                               <chr> \"Not remote\", \"Remote\", \"Remote\",…\n$ career_satisfaction                  <dbl> 8, 8, 9, 5, 7, 9, 5, 8, 8, 10, 7,…\n$ data_scientist                       <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ database_administrator               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ desktop_applications_developer       <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ developer_with_stats_math_background <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ dev_ops                              <lgl> FALSE, FALSE, TRUE, FALSE, FALSE,…\n$ embedded_developer                   <lgl> FALSE, TRUE, TRUE, FALSE, FALSE, …\n$ graphic_designer                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ graphics_programming                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ machine_learning_specialist          <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ mobile_developer                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ quality_assurance_engineer           <lgl> FALSE, FALSE, TRUE, FALSE, FALSE,…\n$ systems_administrator                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE…\n$ web_developer                        <lgl> FALSE, FALSE, TRUE, TRUE, TRUE, T…\n\n# First count for `remote`\nStackSurvey %>% \n    count(remote, sort = TRUE)\n\n# A tibble: 2 × 2\n  remote         n\n  <chr>      <int>\n1 Not remote  6273\n2 Remote       718\n\n# then count for `country`\nStackSurvey %>% \n    count(country, sort = TRUE)\n\n# A tibble: 5 × 2\n  country            n\n  <chr>          <int>\n1 United States   3486\n2 United Kingdom  1270\n3 Germany          950\n4 India            666\n5 Canada           619\n\n\nUse the appropriate column from the data set so you can plot a boxplot with remote status on the x-axis and professional experience on the y-axis:\n\nggplot(StackSurvey, aes(remote, years_coded_job)) +\n    geom_boxplot() +\n    labs(x = NULL,\n         y = \"Years of professional coding experience\")"
  },
  {
    "objectID": "200_dataset2/step2.html",
    "href": "200_dataset2/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nDealing with imbalanced data;\nprep()/bake()/juice();\nconfusion matrix;"
  },
  {
    "objectID": "300_dataset3/step2.html",
    "href": "300_dataset3/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\ntune model hyperparameters;\nworkflow();"
  },
  {
    "objectID": "100_dataset1/step1.html#exploratory-data-analysis",
    "href": "100_dataset1/step1.html#exploratory-data-analysis",
    "title": "Sydney Informatics Hub",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nExploratory data analysis involves looking at:\n\nThe distribution of variables in your dataset;\nWhether any data is missing;\nData skewness;\nCorrelated variables.\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\nExplore the Ames Housing dataset.\n\nWhat can you figure out about the different variables?\nWhich do you think are more or less important?\n\n\nCompare the ameshousing variable, which is from the AmesHousing package in R and has been cleaned, with the ameshousing_uncleaned dataset, which is the raw data from the UCI machine learning repository.\n\nWhat was missing in the raw data?\nWhat are some of the approaches that have been taken to deal with missingness?\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can see that the “uncleaned” dataset has a lot of missing data, whereas it has been cleaned up for us in the “cleaned” one. In the interests of time, we will not focus here on how every variable in that dataset has been explored and cleaned up - however, it presents a good example of “messy” real-world data, so we would encourage you to try and look at a handful of variables at home, to see how they’ve been processed.\n\ndim(ameshousing)\n\n[1] 2930   81\n\nglimpse(ameshousing)\n\nRows: 2,930\nColumns: 81\n$ MS_SubClass        <fct> One_Story_1946_and_Newer_All_Styles, One_Story_1946…\n$ MS_Zoning          <fct> Residential_Low_Density, Residential_High_Density, …\n$ Lot_Frontage       <dbl> 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,…\n$ Lot_Area           <int> 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005…\n$ Street             <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav…\n$ Alley              <fct> No_Alley_Access, No_Alley_Access, No_Alley_Access, …\n$ Lot_Shape          <fct> Slightly_Irregular, Regular, Slightly_Irregular, Re…\n$ Land_Contour       <fct> Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L…\n$ Utilities          <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All…\n$ Lot_Config         <fct> Corner, Inside, Corner, Corner, Inside, Inside, Ins…\n$ Land_Slope         <fct> Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G…\n$ Neighborhood       <fct> North_Ames, North_Ames, North_Ames, North_Ames, Gil…\n$ Condition_1        <fct> Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No…\n$ Condition_2        <fct> Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ Bldg_Type          <fct> OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn…\n$ House_Style        <fct> One_Story, One_Story, One_Story, One_Story, Two_Sto…\n$ Overall_Qual       <fct> Above_Average, Average, Above_Average, Good, Averag…\n$ Overall_Cond       <fct> Average, Above_Average, Above_Average, Average, Ave…\n$ Year_Built         <int> 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199…\n$ Year_Remod_Add     <int> 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199…\n$ Roof_Style         <fct> Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G…\n$ Roof_Matl          <fct> CompShg, CompShg, CompShg, CompShg, CompShg, CompSh…\n$ Exterior_1st       <fct> BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Exterior_2nd       <fct> Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Mas_Vnr_Type       <fct> Stone, None, BrkFace, None, None, BrkFace, None, No…\n$ Mas_Vnr_Area       <dbl> 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6…\n$ Exter_Qual         <fct> Typical, Typical, Typical, Good, Typical, Typical, …\n$ Exter_Cond         <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ Foundation         <fct> CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc…\n$ Bsmt_Qual          <fct> Typical, Typical, Typical, Typical, Good, Typical, …\n$ Bsmt_Cond          <fct> Good, Typical, Typical, Typical, Typical, Typical, …\n$ Bsmt_Exposure      <fct> Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,…\n$ BsmtFin_Type_1     <fct> BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U…\n$ BsmtFin_SF_1       <dbl> 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, …\n$ BsmtFin_Type_2     <fct> Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U…\n$ BsmtFin_SF_2       <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0…\n$ Bsmt_Unf_SF        <dbl> 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,…\n$ Total_Bsmt_SF      <dbl> 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, …\n$ Heating            <fct> GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas…\n$ Heating_QC         <fct> Fair, Typical, Typical, Excellent, Good, Excellent,…\n$ Central_Air        <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ Electrical         <fct> SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB…\n$ First_Flr_SF       <int> 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, …\n$ Second_Flr_SF      <int> 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,…\n$ Low_Qual_Fin_SF    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Gr_Liv_Area        <int> 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616…\n$ Bsmt_Full_Bath     <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, …\n$ Bsmt_Half_Bath     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Full_Bath          <int> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, …\n$ Half_Bath          <int> 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ Bedroom_AbvGr      <int> 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, …\n$ Kitchen_AbvGr      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Kitchen_Qual       <fct> Typical, Typical, Good, Excellent, Typical, Good, G…\n$ TotRms_AbvGrd      <int> 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,…\n$ Functional         <fct> Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T…\n$ Fireplaces         <int> 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Fireplace_Qu       <fct> Good, No_Fireplace, No_Fireplace, Typical, Typical,…\n$ Garage_Type        <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att…\n$ Garage_Finish      <fct> Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F…\n$ Garage_Cars        <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, …\n$ Garage_Area        <dbl> 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4…\n$ Garage_Qual        <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ Garage_Cond        <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ Paved_Drive        <fct> Partial_Pavement, Paved, Paved, Paved, Paved, Paved…\n$ Wood_Deck_SF       <int> 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48…\n$ Open_Porch_SF      <int> 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0…\n$ Enclosed_Porch     <int> 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Three_season_porch <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Screen_Porch       <int> 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, …\n$ Pool_Area          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Pool_QC            <fct> No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo…\n$ Fence              <fct> No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini…\n$ Misc_Feature       <fct> None, None, Gar2, None, None, None, None, None, Non…\n$ Misc_Val           <int> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, …\n$ Mo_Sold            <int> 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, …\n$ Year_Sold          <int> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Sale_Type          <fct> WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W…\n$ Sale_Condition     <fct> Normal, Normal, Normal, Normal, Normal, Normal, Nor…\n$ Sale_Price         <int> 215000, 105000, 172000, 244000, 189900, 195500, 213…\n$ Longitude          <dbl> -93.61975, -93.61976, -93.61939, -93.61732, -93.638…\n$ Latitude           <dbl> 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4…\n\ncolSums(is.na(ameshousing_uncleaned))\n\n          Order             PID     MS SubClass       MS Zoning    Lot Frontage \n              0               0               0               0             490 \n       Lot Area          Street           Alley       Lot Shape    Land Contour \n              0               0            2732               0               0 \n      Utilities      Lot Config      Land Slope    Neighborhood     Condition 1 \n              0               0               0               0               0 \n    Condition 2       Bldg Type     House Style    Overall Qual    Overall Cond \n              0               0               0               0               0 \n     Year Built  Year Remod/Add      Roof Style       Roof Matl    Exterior 1st \n              0               0               0               0               0 \n   Exterior 2nd    Mas Vnr Type    Mas Vnr Area      Exter Qual      Exter Cond \n              0              23              23               0               0 \n     Foundation       Bsmt Qual       Bsmt Cond   Bsmt Exposure  BsmtFin Type 1 \n              0              80              80              83              80 \n   BsmtFin SF 1  BsmtFin Type 2    BsmtFin SF 2     Bsmt Unf SF   Total Bsmt SF \n              1              81               1               1               1 \n        Heating      Heating QC     Central Air      Electrical      1st Flr SF \n              0               0               0               1               0 \n     2nd Flr SF Low Qual Fin SF     Gr Liv Area  Bsmt Full Bath  Bsmt Half Bath \n              0               0               0               2               2 \n      Full Bath       Half Bath   Bedroom AbvGr   Kitchen AbvGr    Kitchen Qual \n              0               0               0               0               0 \n  TotRms AbvGrd      Functional      Fireplaces    Fireplace Qu     Garage Type \n              0               0               0            1422             157 \n  Garage Yr Blt   Garage Finish     Garage Cars     Garage Area     Garage Qual \n            159             159               1               1             159 \n    Garage Cond     Paved Drive    Wood Deck SF   Open Porch SF  Enclosed Porch \n            159               0               0               0               0 \n     3Ssn Porch    Screen Porch       Pool Area         Pool QC           Fence \n              0               0               0            2917            2358 \n   Misc Feature        Misc Val         Mo Sold         Yr Sold       Sale Type \n           2824               0               0               0               0 \n Sale Condition       SalePrice \n              0               0 \n\ncolSums(is.na(ameshousing))\n\n       MS_SubClass          MS_Zoning       Lot_Frontage           Lot_Area \n                 0                  0                  0                  0 \n            Street              Alley          Lot_Shape       Land_Contour \n                 0                  0                  0                  0 \n         Utilities         Lot_Config         Land_Slope       Neighborhood \n                 0                  0                  0                  0 \n       Condition_1        Condition_2          Bldg_Type        House_Style \n                 0                  0                  0                  0 \n      Overall_Qual       Overall_Cond         Year_Built     Year_Remod_Add \n                 0                  0                  0                  0 \n        Roof_Style          Roof_Matl       Exterior_1st       Exterior_2nd \n                 0                  0                  0                  0 \n      Mas_Vnr_Type       Mas_Vnr_Area         Exter_Qual         Exter_Cond \n                 0                  0                  0                  0 \n        Foundation          Bsmt_Qual          Bsmt_Cond      Bsmt_Exposure \n                 0                  0                  0                  0 \n    BsmtFin_Type_1       BsmtFin_SF_1     BsmtFin_Type_2       BsmtFin_SF_2 \n                 0                  0                  0                  0 \n       Bsmt_Unf_SF      Total_Bsmt_SF            Heating         Heating_QC \n                 0                  0                  0                  0 \n       Central_Air         Electrical       First_Flr_SF      Second_Flr_SF \n                 0                  0                  0                  0 \n   Low_Qual_Fin_SF        Gr_Liv_Area     Bsmt_Full_Bath     Bsmt_Half_Bath \n                 0                  0                  0                  0 \n         Full_Bath          Half_Bath      Bedroom_AbvGr      Kitchen_AbvGr \n                 0                  0                  0                  0 \n      Kitchen_Qual      TotRms_AbvGrd         Functional         Fireplaces \n                 0                  0                  0                  0 \n      Fireplace_Qu        Garage_Type      Garage_Finish        Garage_Cars \n                 0                  0                  0                  0 \n       Garage_Area        Garage_Qual        Garage_Cond        Paved_Drive \n                 0                  0                  0                  0 \n      Wood_Deck_SF      Open_Porch_SF     Enclosed_Porch Three_season_porch \n                 0                  0                  0                  0 \n      Screen_Porch          Pool_Area            Pool_QC              Fence \n                 0                  0                  0                  0 \n      Misc_Feature           Misc_Val            Mo_Sold          Year_Sold \n                 0                  0                  0                  0 \n         Sale_Type     Sale_Condition         Sale_Price          Longitude \n                 0                  0                  0                  0 \n          Latitude \n                 0 \n\n\n\n\n\nVisualise missingness\n\nWhen working with missing data, it can be helpful to look for “co-missingness”, i.e. multiple variables missing together. For example, when working with patient data, number of pregnancies, age at onset of menstruation and menopause may all be missing - which, when observed together, may indicate that these samples come from male patients for whom this data is irrelevant. “Gender” may or may not be a variable coded in the dataset.\nA way of visualising missing data in the tidy context has been proposed @tierney2018expanding. See this web page for more options for your own data.\nLet’s look at the missing variables in our housing data:\n\ngg_miss_var(ameshousing_uncleaned)\n\n\n\n\nWe can see that the most missingness is observed in the Pool_QC, Misc_Feature, Alley, Fence and Fireplace_QC variables. This is most likely due to many houses not having pools, alleys, fences, and fireplaces, and not having any features that the real estate agent considers to be notable enough to be added to the “miscellaneous” category.\nAn upset plot will give us more idea about the co-missingness of these variables:\n\ngg_miss_upset(ameshousing_uncleaned, nsets = 10)\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\nWhich variables are most frequently missing together?\nDoes this “co-missingness” make sense?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFence, Alley, Misc feature and Pool QC are most often missing together. This probably means that a house doesn’t have an alley, a fence, a pool or any other miscellaneous features.\nSimilarly, the second most frequent “co-missingess” involves these plus missing “fireplace quality”, most likely due to the house not having fireplace.\nWe can also see that Garage_Yr_Blt, Garage_Finish, Garage_Qual and Garage Cond “co-miss” the same number of times - probably because these represent houses without garages.\n\n\n\n\nNext, let’s create two “helper” vectors with the names of the numeric and categorical variables from the ameshousing dataset, which we can then use to batch subset our dataset prior to EDA/visualisation:\n\n# pull out all of the numerical variables\nnumVars <- ameshousing %>% \n  select_if(is.numeric) %>%\n  names()\n\n# use Negate(is.numeric) to pull out all of the categorical variables\ncatVars <- ameshousing %>% \n  select_if(Negate(is.numeric)) %>%\n  names()\n\nLet’s then use the ggpairs() function to generate a plot of the first 10 numeric variables (and sale price, which is 33) against each other. We can repeat this for variables 11-20 and 21-33.\n\nggpairs(data = ameshousing, \n        columns = numVars[c(1:10, 33)], \n        title = \"Numeric variables 1 - 10\")\n\n\n\n# ggpairs(ameshousing, numVars[c(11:20, 33)], title = \"Numeric variables 11 - 20\")\n# ggpairs(ameshousing, numVars[c(21:33)], title = \"Numeric variables 21 - 33\")\nggpairs(data = ameshousing, \n        columns = c(catVars[2:5], \"Sale_Price\"), \n        title = \"Some categorical variables\")\n\n\n\n\nNext, we can generate a correlation plot between all of our numeric variables. By default, the cor() method will calculate the Pearson correlation between the Sale_Price and the other variables, and we can specify how we’d like to handle missing data when calculating this correlation.\nIn this case, we use pairwise.complete.obs, which calculates the correlation between each pair of variables using all complete pairs of observations on those variables.\nWe then plot the correlation using the corrplot library, which has several options for how to visualise a correlation plot. See here for some examples of the visualisations it can produce.\n\n# pairs.panels(ameshousing[ , names(ameshousing)[c(3, 16, 23, 27,37)]], scale=TRUE)\nameshousingCor <- cor(ameshousing[,numVars],\n                      use = \"pairwise.complete.obs\")\n\nameshousingCor_pvalues <- cor_pmat(ameshousingCor)\nggcorrplot(ameshousingCor, type = \"lower\")\n\n\n\n\nWe can also make a dynamic visualisation using plotly.\n\n#Bonus: interactive corrplot with zoom and mouseover\nggcorrplot(ameshousingCor, type = \"lower\") %>% ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\nWhat variables are the most correlated with SalePrice?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nas_tibble(ameshousingCor, rownames = \"rowname\") %>%\n  gather(pair, value, -rowname) %>%\n  filter(rowname != pair) %>% #remove self correlation\n  filter(rowname == \"Sale_Price\") %>%\n  arrange(desc(abs(value))) %>%\n  head()\n\n# A tibble: 6 × 3\n  rowname    pair          value\n  <chr>      <chr>         <dbl>\n1 Sale_Price Gr_Liv_Area   0.707\n2 Sale_Price Garage_Cars   0.648\n3 Sale_Price Garage_Area   0.640\n4 Sale_Price Total_Bsmt_SF 0.633\n5 Sale_Price First_Flr_SF  0.622\n6 Sale_Price Year_Built    0.558\n\n\nWe can also plot this, using a slightly different representation:\n\nCircles instead of only colour to represent correlation levels\nFilter out correlations less than 0.5\n\n\nall_numVar <- ameshousing[, numVars]\ncor_numVar <- cor(all_numVar, use=\"pairwise.complete.obs\") \nCorHigh <- as_tibble(\n  data.frame(correlation = cor_numVar[,'Sale_Price']), rownames = \"rownames\")  %>% \n  filter(abs(correlation) >= 0.5) %>% \n  .$rownames\nggcorrplot(cor_numVar[CorHigh, CorHigh], type = \"lower\", \"circle\")\n\n\n\n\n\n\n\nLet’s plot one of these relationships:\n\ngra <- ameshousing %>%\n  ggplot(aes(x = Gr_Liv_Area, y = Sale_Price/1000)) + \n  geom_point(alpha = 0.1) + \n  labs(y = \"Sale Price/$1000\",\n       x = \"Living Area (sq.ft)\",\n       title = \"Ames Housing Data\") +\n  geom_smooth(method= \"lm\")  +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngra %>% \n  ggplotly()\n\n\n\n\n\nWe can see that there are five houses with an area > 4000 square feet that seem to be outliers in the data. We should filter them out. Next, let’s generate a violin and boxplot by Quality:\n\nameshousing_filt <-\n  ameshousing %>%\n  filter(Gr_Liv_Area <= 4000)\n\np <- ameshousing_filt %>%\n  ggplot(aes(x = Overall_Qual, y = Sale_Price)) +\n  geom_violin() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nggplotly(p)\n\n\n\n\np <-ameshousing_filt %>%\n  mutate(Quality = as.factor(Overall_Qual)) %>%\n  ggplot(aes(x = Quality,\n             y = Sale_Price / 1000,\n             fill = Quality)) +\n  labs(y = \"Sale Price in $k's\",\n       x = \"Overall Quality of House\",\n       title = \"Ames Housing Data\") +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \nggplotly(p)"
  },
  {
    "objectID": "100_dataset1/step1.html#eda-of-outcome-variable",
    "href": "100_dataset1/step1.html#eda-of-outcome-variable",
    "title": "Sydney Informatics Hub",
    "section": "EDA of outcome variable",
    "text": "EDA of outcome variable\n\nYou also need to do EDA on the outcome variable to:\n\nidentify outliers\nexplore whether there is any skew in its distribution\nidentify a transformation to use when modelling the data (if appropriate)\n\nThis is because many models, including ordinary linear regression, assume that prediction errors (and hence the response) are normally distributed.\n\nameshousing_filt %>% \n  ggplot(aes(x = Sale_Price/1000)) + \n  geom_histogram(bins = 50) + \n  labs(x = \"Sale Price in $k's\",\n       y = \"Number of Houses sold\")\n\n\n\n\nLet’s explore different ways of transforming the Sale Price.\n\n#No transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = Sale_Price)) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#Sqrt transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = sqrt(Sale_Price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#natural log transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = log(Sale_Price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#log10 transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = log10(Sale_Price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\nIf you were working with this dataset, which of the above would you prefer?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe log10 transformation seems best, as it both helps the distribution look more normal and helps keep our error metrics and final predictions easily interpretable. It also means that the errors of predicting the values of inexpensive and expensive houses will affect the prediction equally.\n\nbestNormalize::bestNormalize(\n  ameshousing_filt$Sale_Price,\n  allow_orderNorm = FALSE)\n\nBest Normalizing transformation with 2925 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.5968\n - Box-Cox: 1.6314\n - Center+scale: 5.2811\n - Log_b(x+a): 1.5968\n - sqrt(x + a): 2.7969\n - Yeo-Johnson: 1.6314\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 2925 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 12.71303 \n - sd (before standardization) = 0.4060161 \n\n\nThe bestNormalize library can be used to identify the best normalising transformation. Note that in this case, the arcsinh(x) and logarithmic transformations both achieve best normalisation results. To make interpretation a bit easier, we choose the logarithmic transformation.\n\n\n\n\nameshousing_filt <- ameshousing_filt %>% mutate(Sale_Price = log10(Sale_Price))"
  },
  {
    "objectID": "100_dataset1/step1.html#feature-engineering",
    "href": "100_dataset1/step1.html#feature-engineering",
    "title": "Sydney Informatics Hub",
    "section": "Feature engineering",
    "text": "Feature engineering\nThe year in which the house was built and the year when it was remodelled are not really the most relevant parameters we look at when buying a house: instead, buyers usually care a lot more about the age of the house and the time since the last remodel. Let’s engineer these features:\n\nameshousing_filt_engineered <-\n  ameshousing_filt %>%\n  mutate(Time_Since_Remodel = Year_Sold - Year_Remod_Add, \n         House_Age = Year_Sold - Year_Built) %>%\n  select(-Year_Remod_Add, -Year_Built)\n\nsaveRDS(ameshousing_filt_engineered, \"../_models/ames_dataset_filt.rds\")\n\nNote\nMake sure to create a “models” folder in your project working directory!\nBefore you can save your data as .Rds objects, you will actually need to create a folder for these files to go into. Do this by clicking on the “new folder” button in the files window in R studio. Rename your new folder to “models”."
  },
  {
    "objectID": "Introduction.html#what-is-machine-learning",
    "href": "Introduction.html#what-is-machine-learning",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "What is Machine Learning",
    "text": "What is Machine Learning\n\nMachine Learning (ML) is the field of study that gives computers the ability to learn from data without being explicitly programmed\n\nA ML system is trained rather than explicitly programmed;\nIt is a subset of the Artificial Intelligence;\nAlso known as predictive modeling/statistical learning."
  },
  {
    "objectID": "Introduction.html#learning-by-experience",
    "href": "Introduction.html#learning-by-experience",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Learning by experience",
    "text": "Learning by experience\nWe are never certain something will happen, but we usually know (or can estimate rather well) how likely it is to happen or, at least, what is most likely to happen, based on the experience we have acquired throughout our life.\nExperience in ML means: Data in the form of examples\nWe explore data to find patterns to understand the hidden laws that regulates the domain/make predictions about the world around us."
  },
  {
    "objectID": "Introduction.html#what-do-we-mean-by-learning",
    "href": "Introduction.html#what-do-we-mean-by-learning",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "What do we mean by learning?",
    "text": "What do we mean by learning?\nLearning is our means of attaining the ability to perform automatically a task.\n\nThe main goal of the ML process is to find an algorithm \\(f(x)\\) that most accurately predicts future values \\(y\\) (or outcome) based on a set of inputs \\(X\\) (or predictors), where each entry \\(x^{i}\\) is a different feature:\n\n\nFeatures of an image are usually the values of the pixels in the image;\nWe want to predict the price of an house on the basis of some characteristics (n° rooms, garden, position, floor, etc..) that are the features of each house predictors from which we learn."
  },
  {
    "objectID": "Introduction.html#tidyverse",
    "href": "Introduction.html#tidyverse",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Tidyverse",
    "text": "Tidyverse\nA hugely important part of any modeling approach is exploratory data analysis. In this course, we’ll be using tidyverse packages for getting to know your data, manipulating it, and visualizing it. The tidyverse is a collection of R packages designed for data science that share common APIs and an underlying philosophy. When you type library(tidyverse), what you’re doing is loading this collection of related packages for handling data using tidy data principles. These packages include ggplot2 for data visualization, and dplyr and tidyr for data manipulation and transformation. During this course, we’ll point out when we use functions from these different packages.\nVisit this page to learn more about tidyverse: https://www.tidyverse.org/learn/"
  },
  {
    "objectID": "Introduction.html#tidymodels",
    "href": "Introduction.html#tidymodels",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Tidymodels",
    "text": "Tidymodels\nTidymodels is a collection of R packages that provides a modern and consistent approach to building and validating machine learning models. Compared to older packages such as caret, tidymodels offers several benefits:\n\nit uses a consistent and intuitive syntax across all of its packages, which makes it easier to learn and use compared to the varied and sometimes complex syntax of older packages;\nit is built on top of the tidyverse (dplyr, ggplot2), for a seamless data analysis workflow;\nit includes a number of packages (recipes, rsample) that provide tools for data preprocessing, making it easy to perform common data preprocessing tasks, such as feature engineering, and to integrate these tasks into the machine learning workflow;\nit includes packages (parsnip, tune) that provide a more flexible and modern approach to model tuning and selection. These packages allow for easy cross-validation, hyperparameter tuning, and model selection, and provide a more transparent and reproducible workflow;\nit is actively developed and has a growing community of users and contributors. This means that there are many resources available for learning and troubleshooting, and that the packages are likely to continue to evolve and improve over time.\n\nOverall, tidymodels offers a more modern and consistent approach to building and validating machine learning models, and provides a number of tools for data preprocessing, model tuning and selection, and workflow integration. These features make it a powerful and user-friendly tool."
  },
  {
    "objectID": "100_dataset1/step2.html#feature-engineering",
    "href": "100_dataset1/step2.html#feature-engineering",
    "title": "Sydney Informatics Hub",
    "section": "Feature engineering",
    "text": "Feature engineering\nIn tidymodels, you can preprocess your data using the recipes package. Typical preprocessing steps include:\n\nScaling and centering numeric predictors;\nRemoving skewness from numeric variables;\nOne-hot and dummy variable encoding for categorical variables;\nRemoving correlated predictors and zero variance variables;\nImputing missing data.\n\nEach successive step() function adds a preprocessing step to our recipe object in the order that they are provided:\n\names_rec <-\n  recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_other(all_nominal(), threshold = 0.01) %>% #useful when you have some factor levels with very few observations, all_nominal selects both characters and factors, pools infrequently occurring values (frequency less than 0.01) into an \"other\" category\n  step_nzv(all_nominal()) %>% #remove variables that are highly sparse and unbalanced\n  step_center(all_numeric(), -all_outcomes()) %>% #subtracts the column mean from a variable\n  step_scale(all_numeric(), -all_outcomes()) %>% #divides by the standard deviation\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) #create dummy variables for all nominal variables except the outcome variable \n\names_rec\n\nRecipe\nInputs:\n      role #variables\n   outcome          1\n predictor         80\nOperations:\nCollapsing factor levels for all_nominal()\nSparse, unbalanced variable filter on all_nominal()\nCentering for all_numeric(), -all_outcomes()\nScaling for all_numeric(), -all_outcomes()\nDummy variables from all_nominal(), -all_outcomes()\n\n\nThe preprocessing recipe ames_rec has been defined but no values have been estimated.\n\nThe prep() function takes that defined object and computes everything so that the preprocessing steps can be executed. Note that This is done with the training data.\n\n\names_prep <- prep(ames_rec)\n\names_prep\n\nRecipe\nInputs:\n      role #variables\n   outcome          1\n predictor         80\nTraining data contained 2339 data points and no missing data.\nOperations:\nCollapsing factor levels for MS_SubClass, MS_Zoning, Street, Lot_Shape, Util... [trained]\nSparse, unbalanced variable filter removed Street, Alley, Land_Contour, Utilities,... [trained]\nCentering for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nScaling for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nDummy variables from MS_SubClass, MS_Zoning, Lot_Shape, Lot_Config, Neighborho... [trained]\n\n\nThe bake() and juice() functions both return data, not a preprocessing recipe object.  - Thebake()function takes a prepped recipe (one that has had all quantities estimated from training data) and applies it tonew_data`. That new_data could be the training data again or it could be the testing data (with the TRAINING parameters)\n\names_test_proc <- bake(ames_prep, new_data = ames_test)\n\n\nThe juice() function is a nice little shortcut. When we juice() the recipe, we squeeze that training data back out, transformed in the ways we specified. Let’s compare the bake() and juice() outputs:\n\n\nbake(ames_prep, new_data = ames_train)\n\n# A tibble: 2,339 × 231\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n\njuice(ames_prep) \n\n# A tibble: 2,339 × 231\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n\n\nIt is the same as bake(ames_rep, new_data = ames_train) and is just a shortcut that we are going to use later.\n\n\n\n\n\n\nChallenge X\n\n\n\nDoes it make sense to apply these preprocessing steps to the test set?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, it doesn’t. You want the set test to look like new data that your model will see in the future, so you don’t want to mess with the class balance there; you want to see how your model will perform on imbalanced data, even if you have trained it on artificially balanced data.\n\n\n\nBuild the model\nIn tidymodels, you specify models using three concepts.\n\nModel type differentiates models such as logistic regression, decision tree models, and so forth;\nModel mode includes common options like regression and classification, some model types support either of these while some only have one mode;\nModel engine is the computational tool which will be used to fit the model.\n\nWe will specify the model using the parsnip package - Many functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values.\n\n#a linear regression model specification\names_model <- linear_reg() %>% #pick a model\n  set_engine(\"lm\")           #set the engine\n                             #set_mode(\"regression\") we don't need this as the model linear_reg() only does regression\n\n#view model properties\names_model\n\nLinear Regression Model Specification (regression)\nComputational engine: lm \n\n\nNow we are ready to train our model object on the training data. We can do this using the fit() function from the parsnip package. The fit() function takes the following arguments:\n\na parnsip model object specification;\na model formula\na data frame with the training data\n\nThe code below trains our linear regression model on the prepped training data. In our formula, we have specified that Sale_Price is the response variable and included all the rest as our predictor variables.\n\names_fit <- ames_model %>%\n  fit(Sale_Price ~ .,\n      data=juice(ames_prep))\n\n# View lm_fit properties\names_fit\n\nparsnip model object\nCall:\nstats::lm(formula = Sale_Price ~ ., data = data)\nCoefficients:\n                                          (Intercept)  \n                                            4.2344150  \n                                         Lot_Frontage  \n                                            0.0010509  \n                                             Lot_Area  \n                                            0.0056766  \n                                         Mas_Vnr_Area  \n                                            0.0033526  \n                                         BsmtFin_SF_1  \n                                            0.0288579  \n                                         BsmtFin_SF_2  \n                                           -0.0024142  \n                                          Bsmt_Unf_SF  \n                                           -0.0096332  \n                                        Total_Bsmt_SF  \n                                            0.0293221  \n                                         First_Flr_SF  \n                                            0.0428570  \n                                        Second_Flr_SF  \n                                            0.0514152  \n                                      Low_Qual_Fin_SF  \n                                            0.0043645  \n                                          Gr_Liv_Area  \n                                                   NA  \n                                       Bsmt_Full_Bath  \n                                            0.0046797  \n                                       Bsmt_Half_Bath  \n                                            0.0008760  \n                                            Full_Bath  \n                                            0.0075337  \n                                            Half_Bath  \n                                            0.0063545  \n                                        Bedroom_AbvGr  \n                                           -0.0042524  \n                                        Kitchen_AbvGr  \n                                           -0.0032240  \n                                        TotRms_AbvGrd  \n                                            0.0010374  \n                                           Fireplaces  \n                                            0.0048444  \n                                          Garage_Cars  \n                                            0.0088917  \n                                          Garage_Area  \n                                            0.0054821  \n                                         Wood_Deck_SF  \n                                            0.0028177  \n                                        Open_Porch_SF  \n                                            0.0018956  \n                                       Enclosed_Porch  \n                                            0.0035592  \n                                   Three_season_porch  \n                                            0.0006254  \n                                         Screen_Porch  \n                                            0.0057138  \n                                            Pool_Area  \n                                           -0.0001730  \n                                             Misc_Val  \n                                            0.0008810  \n                                              Mo_Sold  \n                                           -0.0003984  \n                                            Year_Sold  \n                                           -0.0019209  \n                                            Longitude  \n                                           -0.0097179  \n                                             Latitude  \n                                           -0.0059391  \n                                   Time_Since_Remodel  \n                                           -0.0057059  \n                                            House_Age  \n                                           -0.0214642  \n      MS_SubClass_One_Story_1946_and_Newer_All_Styles  \n                                            0.0061643  \n                 MS_SubClass_One_Story_1945_and_Older  \n                                           -0.0206969  \n     MS_SubClass_One_and_Half_Story_Finished_All_Ages  \n                                            0.0282382  \n                 MS_SubClass_Two_Story_1946_and_Newer  \n                                           -0.0138448  \n                 MS_SubClass_Two_Story_1945_and_Older  \n                                            0.0158489  \n                      MS_SubClass_Split_or_Multilevel  \n                                           -0.0153464  \n                              MS_SubClass_Split_Foyer  \n                                            0.0073930  \n               MS_SubClass_Duplex_All_Styles_and_Ages  \n                                            0.0123223  \n             MS_SubClass_One_Story_PUD_1946_and_Newer  \n                                            0.0186726  \n             MS_SubClass_Two_Story_PUD_1946_and_Newer  \n                                           -0.0290154  \nMS_SubClass_Two_Family_conversion_All_Styles_and_Ages  \n                                            0.0126693  \n                                    MS_SubClass_other  \n                                                   NA  \n               MS_Zoning_Floating_Village_Residential  \n                                            0.0369229  \n                    MS_Zoning_Residential_Low_Density  \n                                            0.0295723  \n                 MS_Zoning_Residential_Medium_Density  \n                                            0.0224024  \n                                      MS_Zoning_other  \n                                                   NA  \n                                    Lot_Shape_Regular  \n                                            0.0047757  \n                         Lot_Shape_Slightly_Irregular  \n                                            0.0045442  \n                       Lot_Shape_Moderately_Irregular  \n                                            0.0167009  \n                                      Lot_Shape_other  \n                                                   NA  \n                                    Lot_Config_Corner  \n                                            0.0061984  \n                                   Lot_Config_CulDSac  \n                                            0.0124304  \n                                       Lot_Config_FR2  \n                                           -0.0034930  \n                                    Lot_Config_Inside  \n                                            0.0041820  \n                                     Lot_Config_other  \n                                                   NA  \n                              Neighborhood_North_Ames  \n                                           -0.0144052  \n                           Neighborhood_College_Creek  \n                                           -0.0403143  \n                                Neighborhood_Old_Town  \n                                           -0.0395437  \n                                 Neighborhood_Edwards  \n                                           -0.0485293  \n                                Neighborhood_Somerset  \n                                            0.0158820  \n                      Neighborhood_Northridge_Heights  \n                                            0.0152055  \n                                 Neighborhood_Gilbert  \n                                           -0.0076228  \n                                  Neighborhood_Sawyer  \n                                           -0.0245480  \n                          Neighborhood_Northwest_Ames  \n                                           -0.0147878  \n                             Neighborhood_Sawyer_West  \n                                           -0.0382249  \n                                Neighborhood_Mitchell  \n                                           -0.0254694  \n                               Neighborhood_Brookside  \n                                           -0.0110661  \n                                Neighborhood_Crawford  \n                                            0.0154749  \n                  Neighborhood_Iowa_DOT_and_Rail_Road  \n                                           -0.0481840  \n                              Neighborhood_Timberland  \n                                           -0.0246553  \n                              Neighborhood_Northridge  \n                                            0.0109785  \n                             Neighborhood_Stone_Brook  \n                                            0.0376989  \n Neighborhood_South_and_West_of_Iowa_State_University  \n                                           -0.0339271  \n                             Neighborhood_Clear_Creek  \n                                           -0.0170590  \n                          Neighborhood_Meadow_Village  \n                                           -0.0629992  \n                                   Neighborhood_other  \n                                                   NA  \n                                   Condition_1_Artery  \n                                           -0.0179882  \n                                    Condition_1_Feedr  \n                                           -0.0119543  \n                                     Condition_1_Norm  \n                                            0.0063617  \n                                     Condition_1_PosN  \n                                            0.0101914  \n                                     Condition_1_RRAn  \n                                           -0.0142278  \n                                    Condition_1_other  \n                                                   NA  \n                                     Bldg_Type_OneFam  \n                                            0.0352264  \n                                   Bldg_Type_TwoFmCon  \n                                            0.0217637  \n                                     Bldg_Type_Duplex  \n                                                   NA  \n                                      Bldg_Type_Twnhs  \n                                           -0.0164880  \n                                     Bldg_Type_TwnhsE  \n                                                   NA  \n                         House_Style_One_and_Half_Fin  \n                                           -0.0334739  \n                                House_Style_One_Story  \n                                           -0.0113905  \n                                   House_Style_SFoyer  \n                                           -0.0045098  \n                                     House_Style_SLvl  \n                                            0.0080461  \n                                House_Style_Two_Story  \n                                           -0.0068768  \n                                    House_Style_other  \n                                                   NA  \n                                    Overall_Qual_Fair  \n                                           -0.0090491  \n                           Overall_Qual_Below_Average  \n                                           -0.0081678  \n                                 Overall_Qual_Average  \n                                            0.0135609  \n                           Overall_Qual_Above_Average  \n                                            0.0228156  \n                                    Overall_Qual_Good  \n                                            0.0317333  \n                               Overall_Qual_Very_Good  \n                                            0.0506332  \n                               Overall_Qual_Excellent  \n                                            0.0543703  \n                                   Overall_Qual_other  \n                                                   NA  \n                                    Overall_Cond_Fair  \n                                            0.0861003  \n                           Overall_Cond_Below_Average  \n                                            0.1382136  \n                                 Overall_Cond_Average  \n                                            0.1659276  \n                           Overall_Cond_Above_Average  \n                                            0.1795836  \n                                    Overall_Cond_Good  \n                                            0.1982810  \n                               Overall_Cond_Very_Good  \n                                            0.2021602  \n                               Overall_Cond_Excellent  \n                                            0.2216273  \n                                   Overall_Cond_other  \n                                                   NA  \n                                     Roof_Style_Gable  \n                                            0.0041041  \n                                       Roof_Style_Hip  \n                                            0.0013475  \n                                     Roof_Style_other  \n                                                   NA  \n                                 Exterior_1st_AsbShng  \n                                           -0.0197379  \n                                 Exterior_1st_BrkFace  \n                                            0.0218545  \n                                 Exterior_1st_CemntBd  \n                                           -0.0675824  \n                                 Exterior_1st_HdBoard  \n                                           -0.0177946  \n                                 Exterior_1st_MetalSd  \n                                           -0.0041764  \n                                 Exterior_1st_Plywood  \n                                           -0.0147857  \n                                  Exterior_1st_Stucco  \n                                           -0.0150856  \n                                 Exterior_1st_VinylSd  \n                                           -0.0269952  \n                                 Exterior_1st_Wd.Sdng  \n                                           -0.0127229  \n                                 Exterior_1st_WdShing  \n                                           -0.0219538  \n                                   Exterior_1st_other  \n                                                   NA  \n                                 Exterior_2nd_AsbShng  \n                                           -0.0294565  \n                                 Exterior_2nd_BrkFace  \n                                           -0.0252434  \n                                 Exterior_2nd_CmentBd  \n                                            0.0511306  \n                                 Exterior_2nd_HdBoard  \n                                           -0.0066992  \n                                 Exterior_2nd_MetalSd  \n                                           -0.0094713  \n                                 Exterior_2nd_Plywood  \n                                           -0.0088774  \n                                  Exterior_2nd_Stucco  \n                                            0.0028778  \n                                 Exterior_2nd_VinylSd  \n                                            0.0061450  \n                                 Exterior_2nd_Wd.Sdng  \n                                           -0.0049424  \n                                 Exterior_2nd_Wd.Shng  \n                                           -0.0005624  \n                                   Exterior_2nd_other  \n                                                   NA  \n                                 Mas_Vnr_Type_BrkFace  \n                                            0.0119788  \n                                    Mas_Vnr_Type_None  \n                                            0.0132996  \n                                   Mas_Vnr_Type_Stone  \n                                            0.0202174  \n                                   Mas_Vnr_Type_other  \n                                                   NA  \n                                 Exter_Qual_Excellent  \n                                            0.0412905  \n                                      Exter_Qual_Fair  \n                                           -0.0100946  \n                                      Exter_Qual_Good  \n                                            0.0067359  \n                                   Exter_Qual_Typical  \n                                                   NA  \n                                      Exter_Cond_Fair  \n                                           -0.0537435  \n                                      Exter_Cond_Good  \n                                           -0.0271431  \n                                   Exter_Cond_Typical  \n                                           -0.0190455  \n                                     Exter_Cond_other  \n                                                   NA  \n                                    Foundation_BrkTil  \n                                           -0.0221335  \n                                    Foundation_CBlock  \n                                           -0.0197832  \n                                     Foundation_PConc  \n                                           -0.0113600  \n                                      Foundation_Slab  \n                                           -0.0024282  \n                                     Foundation_other  \n                                                   NA  \n                                  Bsmt_Qual_Excellent  \n                                            0.0296809  \n                                       Bsmt_Qual_Fair  \n                                            0.0030970  \n                                       Bsmt_Qual_Good  \n                                            0.0160700  \n                                Bsmt_Qual_No_Basement  \n                                            0.0188896  \n                                    Bsmt_Qual_Typical  \n                                            0.0147692  \n                                      Bsmt_Qual_other  \n                                                   NA  \n                                     Bsmt_Exposure_Av  \n                                            0.0038457  \n                                     Bsmt_Exposure_Gd  \n                                            0.0268377  \n                                     Bsmt_Exposure_Mn  \n                                            0.0001947  \n                                     Bsmt_Exposure_No  \n                                           -0.0028258  \n                            Bsmt_Exposure_No_Basement  \n                                                   NA  \n                                   BsmtFin_Type_1_ALQ  \n                                            0.0837734  \n                                   BsmtFin_Type_1_BLQ  \n                                            0.0662634  \n                                   BsmtFin_Type_1_GLQ  \n                                            0.0596601  \n                                   BsmtFin_Type_1_LwQ  \n                                            0.0329057  \n                           BsmtFin_Type_1_No_Basement  \n                                                   NA  \n                                   BsmtFin_Type_1_Rec  \n                                            0.0072722  \n                                   BsmtFin_Type_1_Unf  \n                                                   NA  \n                                 Heating_QC_Excellent  \n                                            0.7700744  \n                                      Heating_QC_Fair  \n                                            0.7444348  \n                                      Heating_QC_Good  \n                                            0.7633523  \n                                   Heating_QC_Typical  \n                                            0.7575195  \n                                     Heating_QC_other  \n                                                   NA  \n                                        Central_Air_N  \n                                           -0.0232337  \n                                        Central_Air_Y  \n                                                   NA  \n                                     Electrical_FuseA  \n                                           -0.0134919  \n                                     Electrical_FuseF  \n                                           -0.0188211  \n                                     Electrical_SBrkr  \n                                           -0.0171300  \n                                     Electrical_other  \n                                                   NA  \n                               Kitchen_Qual_Excellent  \n                                            0.0303544  \n                                    Kitchen_Qual_Fair  \n                                           -0.0005397  \n                                    Kitchen_Qual_Good  \n                                            0.0050291  \n                                 Kitchen_Qual_Typical  \n                                                   NA  \n                                   Kitchen_Qual_other  \n                                                   NA  \n                               Fireplace_Qu_Excellent  \n                                           -0.0115950  \n                                    Fireplace_Qu_Fair  \n                                           -0.0047983  \n                                    Fireplace_Qu_Good  \n                                            0.0043227  \n                            Fireplace_Qu_No_Fireplace  \n                                           -0.0029386  \n                                    Fireplace_Qu_Poor  \n                                           -0.0089229  \n                                 Fireplace_Qu_Typical  \n                                                   NA  \n                                   Garage_Type_Attchd  \n                                            0.0277848  \n                                  Garage_Type_Basment  \n                                            0.0183001  \n                                  Garage_Type_BuiltIn  \n                                            0.0259155  \n                                   Garage_Type_Detchd  \n                                            0.0246522  \n                                Garage_Type_No_Garage  \n                                            0.0012371  \n                                    Garage_Type_other  \n                                                   NA  \n                                    Garage_Finish_Fin  \n                                            0.0004798  \n                              Garage_Finish_No_Garage  \n                                           -0.0064210  \n                                    Garage_Finish_RFn  \n                                           -0.0033819  \n                                    Garage_Finish_Unf  \n                                                   NA  \n                                     Garage_Qual_Fair  \n                                           -0.0243010  \n                                Garage_Qual_No_Garage  \n                                                   NA  \n                                  Garage_Qual_Typical  \n                                           -0.0147644  \n                                    Garage_Qual_other  \n                                                   NA  \n                                     Garage_Cond_Fair  \n                                           -0.0288340  \n                                Garage_Cond_No_Garage  \n                                                   NA  \n                                  Garage_Cond_Typical  \n                                           -0.0058730  \n                                    Garage_Cond_other  \n                                                   NA  \n                              Paved_Drive_Dirt_Gravel  \n                                           -0.0044010  \n                         Paved_Drive_Partial_Pavement  \n                                           -0.0080910  \n                                    Paved_Drive_Paved  \n                                                   NA  \n                                   Fence_Good_Privacy  \n                                           -0.0068390  \n                                      Fence_Good_Wood  \n                                           -0.0129793  \n                                Fence_Minimum_Privacy  \n                                           -0.0040916  \n                                       Fence_No_Fence  \n                                           -0.0050151  \n                                          Fence_other  \n                                                   NA  \n                                        Sale_Type_COD  \n                                           -0.0202250  \n                                        Sale_Type_New  \n                                            0.0181473  \n                                        Sale_Type_WD.  \n                                           -0.0182848  \n                                      Sale_Type_other  \n                                                   NA  \n                               Sale_Condition_Abnorml  \n                                           -0.0451496  \n                                Sale_Condition_Family  \n                                           -0.0225266  \n                                Sale_Condition_Normal  \n                                           -0.0017995  \n                               Sale_Condition_Partial  \n                                           -0.0222955  \n                                 Sale_Condition_other  \n                                                   NA  \n\n\nTo obtain the detailed results from our trained linear regression model in a data frame, we can use the tidy() and glance() functions directly on our trained parsnip model, ames_fit. - The tidy() function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values; - The glance() function will return performance metrics obtained on the training data such as the R2 value (r.squared) and the RMSE (sigma). - We can also use the vip() function to plot the variable importance for each predictor in our model. The importance value is determined based on the F-statistics and estimate coefficents in our trained model object.\n\n# Data frame of estimated coefficients\ntidy(ames_fit)\n\n# A tibble: 231 × 5\n   term          estimate std.error statistic   p.value\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)    4.23      0.0959     44.2   1.44e-303\n 2 Lot_Frontage   0.00105   0.00121     0.868 3.85e-  1\n 3 Lot_Area       0.00568   0.00129     4.40  1.12e-  5\n 4 Mas_Vnr_Area   0.00335   0.00167     2.01  4.50e-  2\n 5 BsmtFin_SF_1   0.0289    0.0247      1.17  2.42e-  1\n 6 BsmtFin_SF_2  -0.00241   0.00118    -2.04  4.13e-  2\n 7 Bsmt_Unf_SF   -0.00963   0.00208    -4.63  3.82e-  6\n 8 Total_Bsmt_SF  0.0293    0.00325     9.03  3.65e- 19\n 9 First_Flr_SF   0.0429    0.00291    14.7   7.55e- 47\n10 Second_Flr_SF  0.0514    0.00354    14.5   9.84e- 46\n# … with 221 more rows\n\n# Performance metrics on training data\nglance(ames_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹  sigma stati…² p.value    df logLik    AIC    BIC devia…³\n      <dbl>      <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.935      0.929 0.0476    161.       0   191  3905. -7424. -6313.    4.86\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n# Plot variable importance\nvip(ames_fit)\n\n\n\n\n#Evaluating the model\nTo assess the accuracy of our trained linear regression model, ames_fit, we must use it to make predictions on our test data, ames_test_proc. This is done with the predict() function from parnsip. This function takes two important arguments:\n\na trained parnsip model object;\nnew_data for which to generate predictions.\n\nThe code below uses the predict() function to generate a data frame with a single column, .pred, which contains the predicted Sale Price values on the ames_test data.\n\npredict(ames_fit, new_data = ames_test_proc)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 586 × 1\n   .pred\n   <dbl>\n 1  5.04\n 2  5.22\n 3  5.39\n 4  5.14\n 5  5.23\n 6  4.99\n 7  4.99\n 8  4.98\n 9  5.14\n10  5.76\n# … with 576 more rows\n\n\nWarning: prediction from a rank-deficient fit may be misleading\nOne reason this warning occurs is that you have more model parameters than observations in the dataset. We refer to this as high dimensional data. With high dimensional data, it becomes impossible to find a model that can describe the relationship between the predictor variables and the response variable because we don’t have enough observations to train the model on. The easiest way to resolve this issue is to use a simpler model with less coefficients to estimate.We are not worrying about this today.\nGenerally it’s best to combine the test data set and the predictions into a single data frame. We create a data frame with the predictions on the ames_test data and then use bind_cols() to add the ames_test data to the results.\n\names_test_results <- predict(ames_fit, new_data = ames_test_proc) %>% \n  bind_cols(ames_test_proc)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n# View results\names_test_results\n\n# A tibble: 586 × 232\n   .pred Lot_F…¹ Lot_A…² Mas_V…³ BsmtF…⁴ BsmtF…⁵ Bsmt_…⁶ Total…⁷ First…⁸ Secon…⁹\n   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.04   0.675   0.172 -0.569    0.819   0.552 -0.657  -0.388   -0.689  -0.786\n 2  5.22   0.705   0.488  0.0527  -1.42   -0.298 -0.347   0.670    0.456  -0.786\n 3  5.39   0.916   0.145  1.45    -0.523  -0.298 -0.336   1.92     1.85   -0.786\n 4  5.14   0.224  -0.207 -0.569   -0.970  -0.298 -0.632   0.0242  -0.266  -0.786\n 5  5.23  -0.949  -0.516 -0.569   -0.523  -0.298 -0.466   0.850    0.477  -0.786\n 6  4.99  -1.10   -1.01   2.33     0.819  -0.298 -0.527  -1.33    -1.78    0.392\n 7  4.99  -1.10   -1.01   2.26     0.819  -0.298 -0.759  -1.23    -1.67    0.539\n 8  4.98  -1.10   -1.01   1.62     1.27   -0.298 -0.0770 -1.23    -1.67    0.539\n 9  5.14  -1.01   -0.943 -0.569   -1.42   -0.298 -0.495  -0.452   -0.797   0.618\n10  5.76   1.58    0.492  5.74    -0.523  -0.298  2.08    4.26     4.07   -0.786\n# … with 576 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n\n\nNow we have the model results and the test data in a single data frame.\nCalculating rmse and rsq on the Test Data\nTo obtain the rmse and rsq values on our test set results, we can use the rmse() and rsq() functions. Both functions take the following arguments:\n\na data frame with columns that have the true values and predictions;\nthe column with the true response values;\nthe column with predicted values.\n\nIn the examples below we pass our ames_test_results to these functions to obtain these values for our test set. Results are always returned as a data frame with the following columns: .metric, .estimator, and .estimate.\n\n#RMSE on test set\ntest_rmse <- rmse(ames_test_results, \n     truth = Sale_Price,\n     estimate = .pred)\n\n#rsq on test set\ntest_rsq<- rsq(ames_test_results,\n    truth = Sale_Price,\n    estimate = .pred)\n\n\n\n\n\n\n\nChallenge X\n\n\n\nWe mentioned earlier that the bake() function takes a prepped recipe (ames_prep) and applies it to new_data. The new_data could be the training data again or it could be the testing data. We just evaluated our model on the test data, let’s try to apply the bake() and predict() functions on the training data and compare the results.\nInstructions\n\n#bake() training data\n\n#predict() selling price on the training data\n\n#combine the training data set and the predictions into a single data frame\n\n#RMSE on training set\n\n#rsq on training set\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#bake() training data\names_train_proc <- bake(ames_prep, new_data = ames_train)\n#predict() selling price on the training data\names_train_results <-predict(ames_fit, new_data = ames_train_proc)\n#combine the training data set and the predictions into a single data frame\names_train_results <- ames_train_results %>%\n  bind_cols(ames_train_proc)\n#RMSE on training set\ntrain_rmse <- rmse(ames_train_results, \n     truth = Sale_Price,\n     estimate = .pred)\n#rsq on training set\ntrain_rsq <- rsq(ames_train_results,\n    truth = Sale_Price,\n    estimate = .pred)\n\n\n\n\nLet’s have a look at all the metrics for both our training and test datasets:\n\n#plot metrics for training and test datasets\ntrain_rsq %>%\n  mutate(dataset = \"training\") %>%\n  bind_rows(train_rmse %>%\n              mutate(dataset = \"training\")) %>%\n  bind_rows(test_rsq %>%\n              mutate(dataset = \"test\") %>%\n              bind_rows(test_rmse %>%\n                          mutate(dataset = \"test\")))\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate dataset \n  <chr>   <chr>          <dbl> <chr>   \n1 rsq     standard      0.935  training\n2 rmse    standard      0.0456 training\n3 rsq     standard      0.862  test    \n4 rmse    standard      0.0647 test    \n\n\nIf we look at the testing data, the rmse is higher than the training data. Our training data is not giving us a good idea of how our model is going to perform. In this situation, our algorithm fits our existing data very well, but doesn’t generalise well on new data.\nLet’s visualise the situation with an R2 plot:\n\names_test_results %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(ames_train_results %>%\n              mutate(train = \"training\")) %>%\n  ggplot(aes(Sale_Price, .pred, color = train)) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", size = 0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Actual Selling Price\",\n    y = \"Predicted Selling Price\",\n    color = \"Test/Training data\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis is a plot that can be used for any regression model. It plots the actual values (Sale Prices) versus the model predictions (.pred) as a scatter plot. It also plot the line y = x through the origin. This line is a visually representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit. The reason this plot is called an R2 plot, is because the R2 is the squared correlation between the true and predicted values, which are plotted as paired in the plot.\nResampling\nWe made not such a great decision in the previous section; we expected the model evaluated once on the whole training set to help us understand something about how it would perform on new data. Fortunately, we have some options. We can resample the training set to produce an estimate of how the model will perform.The idea of resampling is to create simulated data sets that can be used to estimate the performance of your model, say, because you want to compare models. You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice). One of these resampling methods is cross-validation.\nCross-validation means taking your training set and randomly dividing it up evenly into subsets, sometimes called “folds”. A fold here means a group or subset or partition.\nYou use one of the folds for validation and the rest for training, then you repeat these steps with all the subsets and combine the results, usually by taking the mean. Cross-validation allows you to get a more accurate estimate of how your model will perform on new data.\n\n\n\n\n\n\nChallenge X\n\n\n\nWhen you implement 10-fold cross-validation repeated 5 times, you:\n\nrandomly divide your training data into 50 subsets and train on 49 at a time (assessing on the other subset), iterating through all 50 subsets for assessment.\nrandomly divide your training data into 10 subsets and train on 9 at a time (assessing on the other subset), iterating through all 10 subsets for assessment. Then you repeat that process 5 times.\nrandomly divide your training data into 5 subsets and train on 4 at a time (assessing on the other subset), iterating through all 5 subsets. Then you repeat that process 10 times.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimulations and practical experience show that 10-fold cross-validation repeated 5 times is a great resampling approach for many situations. This approach involves randomly dividing your training data into 10 folds, or subsets or groups, and training on only 9 while using the other fold for assessment. You iterate through all 10 folds being used for assessment; this is one round of cross-validation. You can then repeat the whole process multiple, perhaps 5, times.\n\n\n\n\names_folds <- vfold_cv(ames_train, v=10, repeats = 5)\n\nglimpse(ames_folds)\n\nRows: 50\nColumns: 3\n$ splits <list> [<vfold_split[2105 x 234 x 2339 x 81]>], [<vfold_split[2105 x …\n$ id     <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1…\n$ id2    <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06\", \"Fo…\n\n\nIn the next steps, we won’t not use prep() or bake(). The ames_rec recipe will be automatically applied in a later step using the workflow() and last_fit() functions.\nCreate a Workflow\nIn the previous section, we trained a linear regression model to the housing data step-by-step. In this section, we will go over how to combine all of the modeling steps into a single workflow. The workflow package was designed to combine models and recipes into a single object. To create a workflow, we start with workflow() to create an empty workflow and then add out model and recipe with add_model() and add_recipe().\n\names_wf <- workflow() %>%\n  add_model(ames_model) %>% \n  add_recipe(ames_rec)\n\names_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n• step_other()\n• step_nzv()\n• step_center()\n• step_scale()\n• step_dummy()\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\nComputational engine: lm \n\n\nOnce we have created a set of resamples, we can use the function fit_resamples() to fit a model to each resample and compute performance metrics for each.\n\nset.seed(234)\names_res <- ames_wf %>%\n  fit_resamples(\n    ames_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n! Fold01, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\nglimpse(ames_res)\n\nRows: 50\nColumns: 6\n$ splits       <list> [<vfold_split[2105 x 234 x 2339 x 81]>], [<vfold_split[2…\n$ id           <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"R…\n$ id2          <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     <list> [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>],…\n$ .notes       <list> [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>],…\n$ .predictions <list> [<tbl_df[234 x 4]>], [<tbl_df[234 x 4]>], [<tbl_df[234 x…\n\nsaveRDS(ames_res, \"../_models/ames_res.rds\")\n\nThe column .metric contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n# Obtain performance metrics on resampled training data\names_res %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0537    50 0.00122 Preprocessor1_Model1\n2 rsq     standard   0.909     50 0.00383 Preprocessor1_Model1\n\n\n\n\nvfold_cv() creates folds for cross-validation;\n\nfit_resamples() fits models to resamples;\n\ncollect_metrics() obtains performance metrics from the results.\n\nNotice that now we have a realistic estimate from the training data that is closer to the testing data!\nIf we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance.\nLet’s use the last_fit() function to evaluate once on the testing set:\n\n#Final fit on test dataset\names_final <- ames_wf %>%\n  last_fit(ames_split)\n\n! train/test split: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n# Obtain performance metrics on test data\ncollect_metrics(ames_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0647 Preprocessor1_Model1\n2 rsq     standard      0.862  Preprocessor1_Model1\n\n\nThe R-squared (rsq) and root mean squared error (RMSE) metrics are similar for both the training and testing datasets in our linear regression model. This is a good sign that the model is not over-fitting and can be used for making predictions on new data.\nWe can save the test set predictions by using the collect_predictions() function. This function returns a data frame which will have the response variables values from the test set and a column named .pred with the model predictions.\n\n# Obtain test set predictions data frame\names_results_final <- ames_final %>% \n                 collect_predictions()\n# View results\names_results_final\n\n# A tibble: 586 × 5\n   id               .pred  .row Sale_Price .config             \n   <chr>            <dbl> <int>      <dbl> <chr>               \n 1 train/test split  5.04     2       5.02 Preprocessor1_Model1\n 2 train/test split  5.22     3       5.24 Preprocessor1_Model1\n 3 train/test split  5.39    18       5.60 Preprocessor1_Model1\n 4 train/test split  5.14    26       5.15 Preprocessor1_Model1\n 5 train/test split  5.23    29       5.26 Preprocessor1_Model1\n 6 train/test split  4.99    30       4.98 Preprocessor1_Model1\n 7 train/test split  4.99    31       5.02 Preprocessor1_Model1\n 8 train/test split  4.98    32       4.94 Preprocessor1_Model1\n 9 train/test split  5.14    34       5.18 Preprocessor1_Model1\n10 train/test split  5.76    47       5.70 Preprocessor1_Model1\n# … with 576 more rows\n\n\nFinally, let’s use this data frame to make an R2 plot to visualize our model performance on the test data set:\n\nggplot(data = ames_results_final,\n       mapping = aes(x = .pred, y = Sale_Price)) +\n  geom_point(color = '#006EA1', alpha = 0.25) +\n  geom_abline(intercept = 0, slope = 1, color = 'black', size=0.5, linetype=\"dotted\") +\n  labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')"
  },
  {
    "objectID": "Introduction.html#exploratory-data-analysis-eda-with-the-tidyverse",
    "href": "Introduction.html#exploratory-data-analysis-eda-with-the-tidyverse",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Exploratory Data Analysis (EDA) with the Tidyverse",
    "text": "Exploratory Data Analysis (EDA) with the Tidyverse\n\nA hugely important part of any modeling approach is exploratory data analysis. In this course, we’ll be using tidyverse packages for getting to know your data, manipulating it, and visualizing it. The tidyverse is a collection of R packages designed for data science that share common APIs and an underlying philosophy. When you type library(tidyverse), what you’re doing is loading this collection of related packages for handling data using tidy data principles. These packages include ggplot2 for data visualization, and dplyr and tidyr for data manipulation and transformation. During this course, we’ll point out when we use functions from these different packages.\nVisit this page to learn more about tidyverse: https://www.tidyverse.org/learn/"
  },
  {
    "objectID": "Introduction.html#machine-learning-ml-with-tidymodels",
    "href": "Introduction.html#machine-learning-ml-with-tidymodels",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Machine Learning (ML) with Tidymodels",
    "text": "Machine Learning (ML) with Tidymodels\n\nTidymodels is a collection of R packages that provides a modern and consistent approach to building and validating machine learning models. Compared to older packages such as caret, tidymodels offers several benefits:\n\nit uses a consistent and intuitive syntax across all of its packages, which makes it easier to learn and use compared to the varied and sometimes complex syntax of older packages;\nit is built on top of the tidyverse (dplyr, ggplot2), for a seamless data analysis workflow;\nit includes a number of packages (recipes, rsample) that provide tools for data preprocessing, making it easy to perform common data preprocessing tasks, such as feature engineering, and to integrate these tasks into the machine learning workflow;\nit includes packages (parsnip, tune) that provide a more flexible and modern approach to model tuning and selection. These packages allow for easy cross-validation, hyperparameter tuning, and model selection, and provide a more transparent and reproducible workflow;\nit is actively developed and has a growing community of users and contributors. This means that there are many resources available for learning and troubleshooting, and that the packages are likely to continue to evolve and improve over time.\n\nOverall, tidymodels offers a more modern and consistent approach to building and validating machine learning models, and provides a number of tools for data preprocessing, model tuning and selection, and workflow integration. These features make it a powerful and user-friendly tool."
  },
  {
    "objectID": "Introduction.html#supervised-ml",
    "href": "Introduction.html#supervised-ml",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Supervised ML",
    "text": "Supervised ML\n\nThe dataset contains a series of inputs, based on which we are trying to predict a predifined outcome, which we know for the original data in the dataset. The outcome can be numerical (in which case the problem is called regression) or categorical (classification).\n\nRegression:\nThink of regression as predicting numbers (or divide the ties by length). You are asking to predict a numerical value given some input. For example:\n\nGiven the house features we want to predict the price. It is a pure numerical and unbounded value (at least positive unbounded);\nTemperature of a city considering several factors (pulling, humidity, season,etc..);\nTo solve this task, the learning algorithm is asked to produce a function so that the model takes an example x as input and after some processing \\(f(x)\\) it returns a value y that can be any real value. \n\n\n\nClassification:\nThink of classification as predicting a category (or divide the socks by color). For example:\n\nGiven a sentence (maybe a tweet) the system should determines if it express a positive or negative or neutral feeling;\nGiven an image where it can be a dog or a cat, we want to determine with the system which one is present;\nTo solve this task, the learning algorithm is usually asked to produce a function \\(y= f(x)\\)\nSo the model takes an example x as input and after some processing \\(f(x)\\) it returns a value y that is one of the categories the example x should belong to. \n\n\n\nData splitting and spending\n\nFor machine learning, we typically split data into training and test sets: - The training set is used to estimate model parameters; - The test set is used to find an independent assessment of model performance.\nOnce we have built a ML model we might want to measure its performance, for example the accuracy in classification task (proportion of correct predicted examples) or the average error in a regression task. Our goal is to build a model that is able to really understand the task and that is able to generalize. For this reason we need to separate our dataset into training and testing sets.  The relative proportions of the training and testing set depend on the total of number of observations, and the variability observed in the data. The trade off to be considered is:\n\nif there is too much data in the training set, the assessment of the prediction error will be carried out on a very small test set, therefore we might find a model that fits the existing data very well, but generalizes very poorly;\nif there is too much data in the testing set, this means that we might not have enough data in the training set to accurately estimate the model parameters - so the model won’t be very accurate.\n\nSome commonly used cutoffs include:\n\n60% training / 40% testing\n70% training / 30% testing\n80% training / 20% testing"
  },
  {
    "objectID": "100_dataset1/step2.html#build-a-simple-linear-regression-model-using-base-r",
    "href": "100_dataset1/step2.html#build-a-simple-linear-regression-model-using-base-r",
    "title": "Sydney Informatics Hub",
    "section": "Build a simple linear regression model using base R",
    "text": "Build a simple linear regression model using base R\n\nIn a linear model, we assume that there is a linear relationship between the input variable(s) and the output variable. This means that as the input variable(s) increase or decrease, the output variable changes in a straight line.\nImagine you have a scatter plot with your data points all over it. A linear model is like drawing a straight line through the scatter plot that best fits all the points. The slope and intercept of this line are chosen in such a way that the distance between the line and all the points is minimized. This line is then used to predict the output for new input values.\n\n\nExample of a linear model\n\n\nThe straight red dotted line represents the linear model equation \\(y=mx+c\\), where \\(c\\) is the y-intercept of the regression line, \\(m\\) is the slope of the regression line, and \\(y\\) is the expected value for y for the given \\(x\\) value.\n\n#fit a linear model\names_lm <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_data)\n\n#Print the summary of the model\nsummary(ames_lm)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_data)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94258 -0.06622  0.01359  0.07298  0.39246 \nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.835e+00  7.406e-03  652.80   <2e-16 ***\nGr_Liv_Area 2.579e-04  4.714e-06   54.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.124 on 2923 degrees of freedom\nMultiple R-squared:  0.506, Adjusted R-squared:  0.5058 \nF-statistic:  2994 on 1 and 2923 DF,  p-value: < 2.2e-16\n\n\nR-squared value explains the variability of y with respect to x:\n\nvaries between 0 to 1 (0-100%);\nR-squared values closer to 0 mean the regression relationship is very low;\nR-squared values closer to 1 mean the regression relationship is very strong.\n\nLet’s plot our linear regression model:\n\nplot(ames_data$Gr_Liv_Area, ames_data$Sale_Price,\n     xlab=\"Gr_Liv_Area\",\n     ylab=\"Sale_Price\", \n     col = \"blue\")\nabline(ames_lm, col = \"red\")"
  },
  {
    "objectID": "100_dataset1/step2.html#build-a-linear-regression-model-using-tidymodels",
    "href": "100_dataset1/step2.html#build-a-linear-regression-model-using-tidymodels",
    "title": "Sydney Informatics Hub",
    "section": "Build a linear regression model using Tidymodels",
    "text": "Build a linear regression model using Tidymodels\n\nWhen you type library(tidymodels), you load a collection of packages for modeling and machine learning using tidyverse principles. All the packages are designed to be consistent, modular, and to support good modeling practices. The first thing we are going to practice is splitting your data into a training set and a testing set. The tidymodels package rsample has functions that help you specify training and testing sets.\n\nset.seed(42) #so we all get the same results\names_split <- ames_data %>%\n    initial_split(prop = 0.8,\n                  strata = Sale_Price) #stratification\n\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\nsaveRDS(ames_train, \"../_models/ames_train.Rds\")\nsaveRDS(ames_test, \"../_models/ames_test.Rds\")\n\nStratified sampling would split within each quartile: \nThe code here takes an input data set and puts 80% of it into a training dataset and 20% of it into a testing dataset; it chooses the individual cases so that both sets are balanced in selling price.\nLet’s check if the distribution of the selling price is the same in the testing and training datasets:\n\names_train %>% \n  ggplot(aes(x = log(Sale_Price),  col = \"red\", fill = NULL)) + \n  geom_density() + theme_minimal() +\n  geom_line(data = ames_test,\n            stat = \"density\",\n            col = \"blue\") + theme(legend.position=\"none\")\n\n\n\n\nFeature engineering\n\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format;\nThe model needs certain data qualities;\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a “feature engineering”).\n\n\nIn tidymodels, you can use the recipes package, an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\n\nSome of these steps can include:\n\nScaling and centering numeric predictors;\nRemoving skewness from numeric variables;\nOne-hot and dummy variable encoding for categorical variables;\nRemoving correlated predictors and zero variance variables;\nImputing missing data.\n\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\n\n\nThe resulting processed output can be used as inputs for statistical or machine learning models.\n\n\names_rec <-\n  recipe(Sale_Price ~ ., data = ames_train) %>% #assigns columns to roles of “outcome” or “predictor” using the formula\n  step_other(all_nominal(), threshold = 0.01) %>% #useful when you have some factor levels with very few observations, all_nominal selects both characters and factors, pools infrequently occurring values (frequency less than 0.01) into an \"other\" category\n  step_nzv(all_predictors()) %>% #remove predictors that are highly sparse and unbalanced\n  step_center(all_numeric_predictors()) %>% #subtracts the column mean from predictors\n  step_scale(all_numeric_predictors()) %>% #divides by the standard deviation\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% #for any nominal predictor, make binary indicators\n  step_lincomb(all_numeric_predictors()) #remove redundancies in the predictors, if present\n\names_rec\n\nRecipe\nInputs:\n      role #variables\n   outcome          1\n predictor         80\nOperations:\nCollapsing factor levels for all_nominal()\nSparse, unbalanced variable filter on all_predictors()\nCentering for all_numeric_predictors()\nScaling for all_numeric_predictors()\nDummy variables from all_nominal_predictors()\nLinear combination filter on all_numeric_predictors()\n\n\nNote that each successive step() function adds a preprocessing step to our recipe object in the order that they are provided. The preprocessing recipe ames_rec has been defined but no values have been estimated.\n\n\nThe prep() function takes that defined object and computes everything so that the preprocessing steps can be executed. Note that This is done with the training data.\n\n\n\names_prep <- prep(ames_rec)\n\names_prep\n\nRecipe\nInputs:\n      role #variables\n   outcome          1\n predictor         80\nTraining data contained 2339 data points and no missing data.\nOperations:\nCollapsing factor levels for MS_SubClass, MS_Zoning, Street, Lot_Shape, Util... [trained]\nSparse, unbalanced variable filter removed Street, Alley, Land_Contour, Utilities,... [trained]\nCentering for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nScaling for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nDummy variables from MS_SubClass, MS_Zoning, Lot_Shape, Lot_Config, Neighborho... [trained]\nLinear combination filter removed MS_Zoning_other, Lot_Shape_other, Lot_Con... [trained]\n\n\nThe bake() and juice() functions both return data, not a preprocessing recipe object.\n\n\nThe bake() function takes a prepped recipe (one that has had all quantities estimated from training data) and applies it to new_data. That new_data could be the training data again or it could be the testing data (with the TRAINING parameters)\n\n\n\nbake(ames_prep, new_data = ames_test)\n\n# A tibble: 586 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1        0.675    0.172 -0.569    0.819   0.552 -0.657  -0.388   -0.689  -0.786\n 2        0.705    0.488  0.0527  -1.42   -0.298 -0.347   0.670    0.456  -0.786\n 3        0.916    0.145  1.45    -0.523  -0.298 -0.336   1.92     1.85   -0.786\n 4        0.224   -0.207 -0.569   -0.970  -0.298 -0.632   0.0242  -0.266  -0.786\n 5       -0.949   -0.516 -0.569   -0.523  -0.298 -0.466   0.850    0.477  -0.786\n 6       -1.10    -1.01   2.33     0.819  -0.298 -0.527  -1.33    -1.78    0.392\n 7       -1.10    -1.01   2.26     0.819  -0.298 -0.759  -1.23    -1.67    0.539\n 8       -1.10    -1.01   1.62     1.27   -0.298 -0.0770 -1.23    -1.67    0.539\n 9       -1.01    -0.943 -0.569   -1.42   -0.298 -0.495  -0.452   -0.797   0.618\n10        1.58     0.492  5.74    -0.523  -0.298  2.08    4.26     4.07   -0.786\n# … with 576 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n\n\n\n\nThe juice() function is a nice little shortcut. When we juice() the recipe, we squeeze that training data back out, transformed in the ways we specified.\n\n\nLet’s compare the bake() and juice() outputs:\n\nbake(ames_prep, new_data = ames_train)\n\n# A tibble: 2,339 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n\njuice(ames_prep) \n\n# A tibble: 2,339 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n\n\nNote that the juice() output is the same as bake(ames_rep, new_data = ames_train) and is just a shortcut that we are going to use later.\n\n\n\n\n\n\nChallenge X\n\n\n\nDoes it make sense to apply these preprocessing steps to the test set?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, it doesn’t. You want the set test to look like new data that your model will see in the future, so you don’t want to mess with the class balance there; you want to see how your model will perform on imbalanced data, even if you have trained it on artificially balanced data.\n\n\n\nBuild the model\n\nIn tidymodels, you specify models using three concepts.\n\nModel type differentiates models such as logistic regression, decision tree models, and so forth;\nModel mode includes common options like regression and classification, some model types support either of these while some only have one mode;\nModel engine is the computational tool which will be used to fit the model.\n\nWe will specify the model using the parsnip package - Many functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values.\n\n#a linear regression model specification\names_model <- linear_reg() %>% #pick a model\n  set_engine(\"lm\")           #set the engine\n                             #set_mode(\"regression\") we don't need this as the model linear_reg() only does regression\n\n#view model properties\names_model\n\nLinear Regression Model Specification (regression)\nComputational engine: lm \n\n\nNow we are ready to train our model object on the training data. We can do this using the fit() function from the parsnip package. The fit() function takes the following arguments:\n\na parnsip model object specification;\na model formula\na data frame with the training data\n\nThe code below trains our linear regression model on the prepped training data. In our formula, we have specified that Sale_Price is the response variable and included all the rest as our predictor variables.\n\names_fit <- ames_model %>%\n  fit(Sale_Price ~ .,\n      data=juice(ames_prep))\n\n# View lm_fit properties\names_fit\n\nparsnip model object\nCall:\nstats::lm(formula = Sale_Price ~ ., data = data)\nCoefficients:\n                                          (Intercept)  \n                                            4.225e+00  \n                                         Lot_Frontage  \n                                            1.510e-03  \n                                             Lot_Area  \n                                            5.634e-03  \n                                         Mas_Vnr_Area  \n                                            3.178e-03  \n                                         BsmtFin_SF_1  \n                                            2.910e-02  \n                                         BsmtFin_SF_2  \n                                           -2.438e-03  \n                                          Bsmt_Unf_SF  \n                                           -9.865e-03  \n                                        Total_Bsmt_SF  \n                                            3.052e-02  \n                                         First_Flr_SF  \n                                            4.657e-03  \n                                        Second_Flr_SF  \n                                            9.635e-03  \n                                          Gr_Liv_Area  \n                                            4.895e-02  \n                                       Bsmt_Full_Bath  \n                                            4.946e-03  \n                                       Bsmt_Half_Bath  \n                                            1.160e-03  \n                                            Full_Bath  \n                                            7.528e-03  \n                                            Half_Bath  \n                                            6.568e-03  \n                                        Bedroom_AbvGr  \n                                           -3.780e-03  \n                                        TotRms_AbvGrd  \n                                           -4.477e-05  \n                                           Fireplaces  \n                                            5.691e-03  \n                                          Garage_Cars  \n                                            8.626e-03  \n                                          Garage_Area  \n                                            6.286e-03  \n                                         Wood_Deck_SF  \n                                            1.943e-03  \n                                        Open_Porch_SF  \n                                            1.901e-03  \n                                              Mo_Sold  \n                                           -3.097e-04  \n                                            Year_Sold  \n                                           -2.016e-03  \n                                            Longitude  \n                                           -8.325e-03  \n                                             Latitude  \n                                           -4.980e-03  \n                                   Time_Since_Remodel  \n                                           -5.556e-03  \n                                            House_Age  \n                                           -1.929e-02  \n      MS_SubClass_One_Story_1946_and_Newer_All_Styles  \n                                            6.196e-03  \n                 MS_SubClass_One_Story_1945_and_Older  \n                                           -2.177e-02  \n     MS_SubClass_One_and_Half_Story_Finished_All_Ages  \n                                            2.813e-02  \n                 MS_SubClass_Two_Story_1946_and_Newer  \n                                           -1.093e-02  \n                 MS_SubClass_Two_Story_1945_and_Older  \n                                            1.847e-02  \n                      MS_SubClass_Split_or_Multilevel  \n                                           -2.027e-02  \n                              MS_SubClass_Split_Foyer  \n                                            3.816e-03  \n               MS_SubClass_Duplex_All_Styles_and_Ages  \n                                           -2.177e-03  \n             MS_SubClass_One_Story_PUD_1946_and_Newer  \n                                            2.046e-02  \n             MS_SubClass_Two_Story_PUD_1946_and_Newer  \n                                           -2.533e-02  \nMS_SubClass_Two_Family_conversion_All_Styles_and_Ages  \n                                            2.991e-03  \n                                    MS_SubClass_other  \n                                                   NA  \n               MS_Zoning_Floating_Village_Residential  \n                                            3.984e-02  \n                    MS_Zoning_Residential_Low_Density  \n                                            3.224e-02  \n                 MS_Zoning_Residential_Medium_Density  \n                                            2.443e-02  \n                                    Lot_Shape_Regular  \n                                            4.462e-03  \n                         Lot_Shape_Slightly_Irregular  \n                                            4.294e-03  \n                       Lot_Shape_Moderately_Irregular  \n                                            1.805e-02  \n                                    Lot_Config_Corner  \n                                            5.998e-03  \n                                   Lot_Config_CulDSac  \n                                            1.298e-02  \n                                       Lot_Config_FR2  \n                                           -5.176e-03  \n                                    Lot_Config_Inside  \n                                            3.934e-03  \n                              Neighborhood_North_Ames  \n                                           -1.547e-02  \n                           Neighborhood_College_Creek  \n                                           -3.736e-02  \n                                Neighborhood_Old_Town  \n                                           -3.894e-02  \n                                 Neighborhood_Edwards  \n                                           -4.494e-02  \n                                Neighborhood_Somerset  \n                                            1.495e-02  \n                      Neighborhood_Northridge_Heights  \n                                            1.344e-02  \n                                 Neighborhood_Gilbert  \n                                           -8.690e-03  \n                                  Neighborhood_Sawyer  \n                                           -2.252e-02  \n                          Neighborhood_Northwest_Ames  \n                                           -1.568e-02  \n                             Neighborhood_Sawyer_West  \n                                           -3.405e-02  \n                                Neighborhood_Mitchell  \n                                           -2.449e-02  \n                               Neighborhood_Brookside  \n                                           -1.037e-02  \n                                Neighborhood_Crawford  \n                                            1.911e-02  \n                  Neighborhood_Iowa_DOT_and_Rail_Road  \n                                           -4.515e-02  \n                              Neighborhood_Timberland  \n                                           -2.225e-02  \n                              Neighborhood_Northridge  \n                                            1.195e-02  \n                             Neighborhood_Stone_Brook  \n                                            3.834e-02  \n Neighborhood_South_and_West_of_Iowa_State_University  \n                                           -3.175e-02  \n                             Neighborhood_Clear_Creek  \n                                           -1.395e-02  \n                          Neighborhood_Meadow_Village  \n                                           -6.325e-02  \n                                   Condition_1_Artery  \n                                           -1.792e-02  \n                                    Condition_1_Feedr  \n                                           -1.227e-02  \n                                     Condition_1_Norm  \n                                            6.575e-03  \n                                     Condition_1_PosN  \n                                            1.180e-02  \n                                     Condition_1_RRAn  \n                                           -1.178e-02  \n                                     Bldg_Type_OneFam  \n                                            3.635e-02  \n                                   Bldg_Type_TwoFmCon  \n                                            2.470e-02  \n                                      Bldg_Type_Twnhs  \n                                           -1.657e-02  \n                         House_Style_One_and_Half_Fin  \n                                           -3.279e-02  \n                                House_Style_One_Story  \n                                           -9.496e-03  \n                                   House_Style_SFoyer  \n                                            5.423e-04  \n                                     House_Style_SLvl  \n                                            1.420e-02  \n                                House_Style_Two_Story  \n                                           -8.472e-03  \n                                    Overall_Qual_Fair  \n                                           -9.709e-03  \n                           Overall_Qual_Below_Average  \n                                           -7.711e-03  \n                                 Overall_Qual_Average  \n                                            1.465e-02  \n                           Overall_Qual_Above_Average  \n                                            2.453e-02  \n                                    Overall_Qual_Good  \n                                            3.288e-02  \n                               Overall_Qual_Very_Good  \n                                            5.065e-02  \n                               Overall_Qual_Excellent  \n                                            5.328e-02  \n                                    Overall_Cond_Fair  \n                                            8.424e-02  \n                           Overall_Cond_Below_Average  \n                                            1.386e-01  \n                                 Overall_Cond_Average  \n                                            1.647e-01  \n                           Overall_Cond_Above_Average  \n                                            1.786e-01  \n                                    Overall_Cond_Good  \n                                            1.976e-01  \n                               Overall_Cond_Very_Good  \n                                            2.004e-01  \n                               Overall_Cond_Excellent  \n                                            2.172e-01  \n                                     Roof_Style_Gable  \n                                            3.064e-03  \n                                       Roof_Style_Hip  \n                                            1.400e-03  \n                                 Exterior_1st_AsbShng  \n                                           -2.269e-02  \n                                 Exterior_1st_BrkFace  \n                                            1.762e-02  \n                                 Exterior_1st_CemntBd  \n                                           -6.648e-02  \n                                 Exterior_1st_HdBoard  \n                                           -2.029e-02  \n                                 Exterior_1st_MetalSd  \n                                           -9.188e-03  \n                                 Exterior_1st_Plywood  \n                                           -1.778e-02  \n                                  Exterior_1st_Stucco  \n                                           -2.030e-02  \n                                 Exterior_1st_VinylSd  \n                                           -3.227e-02  \n                                 Exterior_1st_Wd.Sdng  \n                                           -1.530e-02  \n                                 Exterior_1st_WdShing  \n                                           -2.727e-02  \n                                 Exterior_2nd_AsbShng  \n                                           -2.730e-02  \n                                 Exterior_2nd_BrkFace  \n                                           -2.295e-02  \n                                 Exterior_2nd_CmentBd  \n                                            4.882e-02  \n                                 Exterior_2nd_HdBoard  \n                                           -5.905e-03  \n                                 Exterior_2nd_MetalSd  \n                                           -5.823e-03  \n                                 Exterior_2nd_Plywood  \n                                           -7.550e-03  \n                                  Exterior_2nd_Stucco  \n                                            7.010e-03  \n                                 Exterior_2nd_VinylSd  \n                                            9.093e-03  \n                                 Exterior_2nd_Wd.Sdng  \n                                           -3.418e-03  \n                                 Exterior_2nd_Wd.Shng  \n                                            3.114e-03  \n                                 Mas_Vnr_Type_BrkFace  \n                                            1.391e-02  \n                                    Mas_Vnr_Type_None  \n                                            1.470e-02  \n                                   Mas_Vnr_Type_Stone  \n                                            2.233e-02  \n                                 Exter_Qual_Excellent  \n                                            4.355e-02  \n                                      Exter_Qual_Fair  \n                                           -1.018e-02  \n                                      Exter_Qual_Good  \n                                            7.081e-03  \n                                      Exter_Cond_Fair  \n                                           -5.441e-02  \n                                      Exter_Cond_Good  \n                                           -2.790e-02  \n                                   Exter_Cond_Typical  \n                                           -2.038e-02  \n                                    Foundation_BrkTil  \n                                           -2.178e-02  \n                                    Foundation_CBlock  \n                                           -1.965e-02  \n                                     Foundation_PConc  \n                                           -1.085e-02  \n                                      Foundation_Slab  \n                                            6.116e-05  \n                                  Bsmt_Qual_Excellent  \n                                            3.459e-02  \n                                       Bsmt_Qual_Fair  \n                                            6.404e-03  \n                                       Bsmt_Qual_Good  \n                                            2.045e-02  \n                                Bsmt_Qual_No_Basement  \n                                            2.428e-02  \n                                    Bsmt_Qual_Typical  \n                                            1.874e-02  \n                                     Bsmt_Exposure_Av  \n                                            3.624e-03  \n                                     Bsmt_Exposure_Gd  \n                                            2.669e-02  \n                                     Bsmt_Exposure_Mn  \n                                            5.688e-05  \n                                     Bsmt_Exposure_No  \n                                           -4.150e-03  \n                                   BsmtFin_Type_1_ALQ  \n                                            8.386e-02  \n                                   BsmtFin_Type_1_BLQ  \n                                            6.682e-02  \n                                   BsmtFin_Type_1_GLQ  \n                                            5.941e-02  \n                                   BsmtFin_Type_1_LwQ  \n                                            3.318e-02  \n                                   BsmtFin_Type_1_Rec  \n                                            7.956e-03  \n                                 Heating_QC_Excellent  \n                                            7.722e-01  \n                                      Heating_QC_Fair  \n                                            7.472e-01  \n                                      Heating_QC_Good  \n                                            7.654e-01  \n                                   Heating_QC_Typical  \n                                            7.598e-01  \n                                        Central_Air_N  \n                                           -2.408e-02  \n                                     Electrical_FuseA  \n                                           -1.545e-02  \n                                     Electrical_FuseF  \n                                           -1.968e-02  \n                                     Electrical_SBrkr  \n                                           -1.951e-02  \n                               Kitchen_Qual_Excellent  \n                                            3.178e-02  \n                                    Kitchen_Qual_Fair  \n                                           -3.273e-03  \n                                    Kitchen_Qual_Good  \n                                            5.608e-03  \n                               Fireplace_Qu_Excellent  \n                                           -1.090e-02  \n                                    Fireplace_Qu_Fair  \n                                           -5.742e-03  \n                                    Fireplace_Qu_Good  \n                                            3.799e-03  \n                            Fireplace_Qu_No_Fireplace  \n                                           -2.870e-03  \n                                    Fireplace_Qu_Poor  \n                                           -8.559e-03  \n                                   Garage_Type_Attchd  \n                                            2.717e-02  \n                                  Garage_Type_Basment  \n                                            1.841e-02  \n                                  Garage_Type_BuiltIn  \n                                            2.514e-02  \n                                   Garage_Type_Detchd  \n                                            2.309e-02  \n                                Garage_Type_No_Garage  \n                                            4.121e-04  \n                                    Garage_Finish_Fin  \n                                            9.351e-04  \n                              Garage_Finish_No_Garage  \n                                           -5.710e-03  \n                                    Garage_Finish_RFn  \n                                           -2.171e-03  \n                                     Garage_Qual_Fair  \n                                           -2.731e-02  \n                                  Garage_Qual_Typical  \n                                           -1.877e-02  \n                                     Garage_Cond_Fair  \n                                           -2.552e-02  \n                                  Garage_Cond_Typical  \n                                           -1.678e-03  \n                              Paved_Drive_Dirt_Gravel  \n                                           -4.599e-03  \n                         Paved_Drive_Partial_Pavement  \n                                           -7.736e-03  \n                                   Fence_Good_Privacy  \n                                           -4.646e-03  \n                                      Fence_Good_Wood  \n                                           -1.104e-02  \n                                Fence_Minimum_Privacy  \n                                           -1.837e-03  \n                                       Fence_No_Fence  \n                                           -3.494e-03  \n                                        Sale_Type_COD  \n                                           -1.792e-02  \n                                        Sale_Type_New  \n                                            1.829e-02  \n                                        Sale_Type_WD.  \n                                           -1.673e-02  \n                               Sale_Condition_Abnorml  \n                                           -4.424e-02  \n                                Sale_Condition_Family  \n                                           -2.159e-02  \n                                Sale_Condition_Normal  \n                                           -1.563e-03  \n                               Sale_Condition_Partial  \n                                           -2.112e-02  \n\n\nTo obtain the detailed results from our trained linear regression model in a data frame, we can use the tidy() and glance() functions directly on our trained parsnip model, ames_fit. - The tidy() function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values; - The glance() function will return performance metrics obtained on the training data such as the R2 value (r.squared) and the RMSE (sigma). - We can also use the vip() function to plot the variable importance for each predictor in our model. The importance value is determined based on the F-statistics and estimate coefficents in our trained model object.\n\n# Data frame of estimated coefficients\ntidy(ames_fit)\n\n# A tibble: 187 × 5\n   term          estimate std.error statistic   p.value\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)    4.22      0.0965     43.8   8.49e-300\n 2 Lot_Frontage   0.00151   0.00121     1.24  2.14e-  1\n 3 Lot_Area       0.00563   0.00129     4.35  1.42e-  5\n 4 Mas_Vnr_Area   0.00318   0.00168     1.89  5.90e-  2\n 5 BsmtFin_SF_1   0.0291    0.0248      1.17  2.41e-  1\n 6 BsmtFin_SF_2  -0.00244   0.00119    -2.05  4.03e-  2\n 7 Bsmt_Unf_SF   -0.00986   0.00209    -4.71  2.61e-  6\n 8 Total_Bsmt_SF  0.0305    0.00326     9.36  2.01e- 20\n 9 First_Flr_SF   0.00466   0.00910     0.512 6.09e-  1\n10 Second_Flr_SF  0.00963   0.0100      0.960 3.37e-  1\n# … with 177 more rows\n\n# Performance metrics on training data\nglance(ames_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹  sigma stati…² p.value    df logLik    AIC    BIC devia…³\n      <dbl>      <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.934      0.928 0.0479    164.       0   185  3884. -7395. -6318.    4.94\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n# Plot variable importance\nvip(ames_fit)\n\n\n\n\nEvaluating the model\n\nTo assess the accuracy of our trained linear regression model, ames_fit, we must use it to make predictions on our test data, ames_test_proc. This is done with the predict() function from parnsip. This function takes two important arguments:\n\na trained parnsip model object;\nnew_data for which to generate predictions.\n\nThe code below uses the predict() function to generate a data frame with a single column, .pred, which contains the predicted Sale Price values on the ames_train data.\n\npredict(ames_fit, new_data = juice(ames_prep))\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n\n# A tibble: 2,339 × 1\n   .pred\n   <dbl>\n 1  5.09\n 2  5.01\n 3  5.13\n 4  5.06\n 5  5.08\n 6  5.07\n 7  4.90\n 8  5.10\n 9  5.06\n10  5.16\n# … with 2,329 more rows\n\n\nGenerally it’s best to combine the test data set and the predictions into a single data frame. We create a data frame with the predictions on the ames_test data and then use bind_cols() to add the ames_test data to the results.\n\names_train_results <- predict(ames_fit, new_data = juice(ames_prep)) %>% \n  bind_cols(juice(ames_prep))\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n\n# View results\names_train_results\n\n# A tibble: 2,339 × 188\n   .pred Lot_F…¹ Lot_A…² Mas_V…³ BsmtF…⁴ BsmtF…⁵ Bsmt_…⁶ Total…⁷ First…⁸ Secon…⁹\n   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.09  0.374  -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2  5.01  0.374   0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3  5.13 -0.137  -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4  5.06 -1.01   -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5  5.08 -0.0770 -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6  5.07 -0.227  -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7  4.90  0.374  -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8  5.10  0.314  -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9  5.06 -1.73   -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10  5.16 -1.73   -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n\n\nNow we have the model results and the training data in a single data frame.\nMetrics for model performance\n\n\n\n\nR-squared (rsq): squared correlation between the predicted and observed values;\n\nRoot Mean Square Error (RMSE): difference between the predicted and observed values (loss of function);\n\n\nTo obtain the rmse and rsq values on our test set results, we can use the rmse() and rsq() functions. Both functions take the following arguments:\n\na data frame with columns that have the true values and predictions;\nthe column with the true response values;\nthe column with predicted values.\n\nIn the examples below we pass our ames_test_results to these functions to obtain these values for our test set. Results are always returned as a data frame with the following columns: .metric, .estimator, and .estimate.\n\n#RMSE on train set\ntrain_rmse <- rmse(ames_train_results, \n     truth = Sale_Price,\n     estimate = .pred)\n\n#rsq on train set\ntrain_rsq<- rsq(ames_train_results,\n    truth = Sale_Price,\n    estimate = .pred)\n\n\n\n\n\n\n\nChallenge X\n\n\n\nWe mentioned earlier that the bake() function takes a prepped recipe (ames_prep) and applies it to new_data. The new_data could be the training data again or it could be the testing data. We just evaluated our model on the training data, let’s try to apply the bake() and predict() functions on the test data and compare the results.\nInstructions\n\n#bake() test data\n\n#predict() selling price on the test data\n\n#combine the test data set and the predictions into a single data frame\n\n#RMSE on test set\n\n#rsq on test set\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#bake() test data\names_test_proc <- bake(ames_prep, new_data = ames_test)\n#predict() selling price on the test data\names_test_results <-predict(ames_fit, new_data = ames_test_proc)\n#combine the training data set and the predictions into a single data frame\names_test_results <- ames_test_results %>%\n  bind_cols(ames_test_proc)\n#RMSE on training set\ntest_rmse <- rmse(ames_test_results, \n     truth = Sale_Price,\n     estimate = .pred)\n#rsq on training set\ntest_rsq <- rsq(ames_test_results,\n    truth = Sale_Price,\n    estimate = .pred)\n\n\n\n\nLet’s have a look at all the metrics for both our training and test datasets:\n\n#plot metrics for training and test datasets\ntrain_rsq %>%\n  mutate(dataset = \"training\") %>%\n  bind_rows(train_rmse %>%\n              mutate(dataset = \"training\")) %>%\n  bind_rows(test_rsq %>%\n              mutate(dataset = \"test\") %>%\n              bind_rows(test_rmse %>%\n                          mutate(dataset = \"test\")))\n\n# A tibble: 4 × 4\n  .metric .estimator .estimate dataset \n  <chr>   <chr>          <dbl> <chr>   \n1 rsq     standard      0.934  training\n2 rmse    standard      0.0460 training\n3 rsq     standard      0.858  test    \n4 rmse    standard      0.0656 test    \n\n\nLet’s visualise the situation with an R2 plot:\n\names_test_results %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(ames_train_results %>%\n              mutate(train = \"training\")) %>%\n  ggplot(aes(Sale_Price, .pred, color = train)) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linewidth = 0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.15) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Actual Selling Price\",\n    y = \"Predicted Selling Price\",\n    color = \"Test/Training data\"\n  )\n\n\n\n\nThis is a plot that can be used for any regression model. It plots the actual values (Sale Prices) versus the model predictions (.pred) as a scatter plot. It also plot the line y = x through the origin. This line is a visually representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit. The reason this plot is called an R2 plot, is because the R2 is the squared correlation between the true and predicted values, which are plotted as paired in the plot.\nResampling\n\nYou just trained models one time on the whole training set and then evaluated them on the testing set. Statisticians have come up with a slew of approaches to evaluate models in better ways than this; many important ones fall under the category of resampling.\nWe can resample the training set to produce an estimate of how the model will perform.The idea of resampling is to create simulated data sets that can be used to estimate the performance of your model, say, because you want to compare models. You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice). One of these resampling methods is cross-validation.\nCross-validation means taking your training set and randomly dividing it up evenly into subsets, sometimes called “folds”. A fold here means a group or subset or partition.\nYou use one of the folds for validation and the rest for training, then you repeat these steps with all the subsets and combine the results, usually by taking the mean. Cross-validation allows you to get a more accurate estimate of how your model will perform on new data.\n\n\n\n\n\n\nChallenge X\n\n\n\nWhen you implement 10-fold cross-validation repeated 5 times, you:\n\nrandomly divide your training data into 50 subsets and train on 49 at a time (assessing on the other subset), iterating through all 50 subsets for assessment.\nrandomly divide your training data into 10 subsets and train on 9 at a time (assessing on the other subset), iterating through all 10 subsets for assessment. Then you repeat that process 5 times.\nrandomly divide your training data into 5 subsets and train on 4 at a time (assessing on the other subset), iterating through all 5 subsets. Then you repeat that process 10 times.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimulations and practical experience show that 10-fold cross-validation repeated 5 times is a great resampling approach for many situations. This approach involves randomly dividing your training data into 10 folds, or subsets or groups, and training on only 9 while using the other fold for assessment. You iterate through all 10 folds being used for assessment; this is one round of cross-validation. You can then repeat the whole process multiple, perhaps 5, times.\n\n\n\n\nset.seed(9)\n\names_folds <- vfold_cv(ames_train, v=10, repeats = 5, strata = Sale_Price)\n\nglimpse(ames_folds)\n\nRows: 50\nColumns: 3\n$ splits <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2103 x …\n$ id     <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1…\n$ id2    <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06\", \"Fo…\n\n\nIn the next steps, we won’t not use prep() or bake(). The ames_rec recipe will be automatically applied in a later step using the workflow() and last_fit() functions.\nCreate a Workflow\n\nIn the previous section, we trained a linear regression model to the housing data step-by-step. In this section, we will go over how to combine all of the modeling steps into a single workflow.\nThe workflow package was designed to capture the entire modeling process and combine models and recipes into a single object. To create a workflow, we start with workflow() to create an empty workflow and then add out model and recipe with add_model() and add_recipe().\n\names_wf <- workflow() %>%\n  add_model(ames_model) %>% \n  add_recipe(ames_rec)\n\names_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n• step_other()\n• step_nzv()\n• step_center()\n• step_scale()\n• step_dummy()\n• step_lincomb()\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\nComputational engine: lm \n\n\nOnce we have created a set of resamples, we can use the function fit_resamples() to fit a model to each resample and compute performance metrics for each.\n\nset.seed(234)\names_res <- ames_wf %>%\n  fit_resamples(\n    ames_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n! Fold01, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold01, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold02, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold03, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold04, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold05, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold06, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold07, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold08, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold09, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n\n! Fold10, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\nglimpse(ames_res)\n\nRows: 50\nColumns: 6\n$ splits       <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2…\n$ id           <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"R…\n$ id2          <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     <list> [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>],…\n$ .notes       <list> [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>],…\n$ .predictions <list> [<tbl_df[236 x 4]>], [<tbl_df[236 x 4]>], [<tbl_df[236 x…\n\nsaveRDS(ames_res, \"../_models/ames_res.rds\")\n\nThe column .metric contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n# Obtain performance metrics on resampled training data\names_res %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0540    50 0.00129 Preprocessor1_Model1\n2 rsq     standard   0.909     50 0.00368 Preprocessor1_Model1\n\n\n\n\nvfold_cv() creates folds for cross-validation;\n\nfit_resamples() fits models to resamples;\n\ncollect_metrics() obtains performance metrics from the results.\n\nWe can see that the regression relationship is very strong: 90.8% of the variability in the selling price can be explained by the predictors and, on average, each element in the predicted selling price differs from the actual selling price by 0.05.\nWe can reliably measure performance using only the training data.\nIf we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance.\n\names_res %>%\n  collect_predictions() %>%\n  ggplot(aes(Sale_Price, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", size = 1.5) +\n  geom_point(alpha = 0.15) +\n  coord_obs_pred()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBack to the testing data\n\nLet’s use the last_fit() function to evaluate once on the testing set:\n\n#Final fit on test dataset\names_final <- ames_wf %>%\n  last_fit(ames_split)\n\n! train/test split: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n\n# Obtain performance metrics on test data\ncollect_metrics(ames_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0656 Preprocessor1_Model1\n2 rsq     standard      0.858  Preprocessor1_Model1\n\n\nThe R2 and RMSE metrics are similar for both the training and testing datasets in our linear regression model. This is a good sign that the model is not over-fitting and can be used for making predictions on new data.\nWe can save the test set predictions by using the collect_predictions() function. This function returns a data frame which will have the response variables values from the test set and a column named .pred with the model predictions.\n\n# Obtain test set predictions data frame\names_results_final <- ames_final %>% \n                 collect_predictions()\n# View results\names_results_final\n\n# A tibble: 586 × 5\n   id               .pred  .row Sale_Price .config             \n   <chr>            <dbl> <int>      <dbl> <chr>               \n 1 train/test split  5.03     2       5.02 Preprocessor1_Model1\n 2 train/test split  5.19     3       5.24 Preprocessor1_Model1\n 3 train/test split  5.39    18       5.60 Preprocessor1_Model1\n 4 train/test split  5.13    26       5.15 Preprocessor1_Model1\n 5 train/test split  5.23    29       5.26 Preprocessor1_Model1\n 6 train/test split  4.99    30       4.98 Preprocessor1_Model1\n 7 train/test split  4.99    31       5.02 Preprocessor1_Model1\n 8 train/test split  4.98    32       4.94 Preprocessor1_Model1\n 9 train/test split  5.14    34       5.18 Preprocessor1_Model1\n10 train/test split  5.77    47       5.70 Preprocessor1_Model1\n# … with 576 more rows\n\n\nFinally, let’s use this data frame to make an R2 plot to visualize our model performance on the test data set:\n\nggplot(data = ames_results_final,\n       mapping = aes(x = .pred, y = Sale_Price)) +\n  geom_point(color = '#006EA1', alpha = 0.25) +\n  geom_abline(intercept = 0, slope = 1, color = 'black', size=0.5, linetype=\"dotted\") +\n  labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')"
  },
  {
    "objectID": "100_dataset1/step1.html#feature-transformation",
    "href": "100_dataset1/step1.html#feature-transformation",
    "title": "Sydney Informatics Hub",
    "section": "Feature transformation",
    "text": "Feature transformation\n\nThe year in which the house was built and the year when it was remodelled are not really the most relevant parameters we look at when buying a house: instead, buyers usually care a lot more about the age of the house and the time since the last remodel. Let’s transform these features:\n\nameshousing_filt_engineered <-\n  ameshousing_filt %>%\n  mutate(Time_Since_Remodel = Year_Sold - Year_Remod_Add, \n         House_Age = Year_Sold - Year_Built) %>%\n  select(-Year_Remod_Add, -Year_Built)\n\nsaveRDS(ameshousing_filt_engineered, \"../_models/ames_dataset_filt.rds\")\n\n\nNote Make sure to create a “models” folder in your project working directory! Before you can save your data as .Rds objects, you will actually need to create a folder for these files to go into. Do this by clicking on the “new folder” button in the files window in R studio. Rename your new folder to “models”."
  }
]