[
  {
    "objectID": "100_regression/step1.html",
    "href": "100_regression/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nUse Tidyverse functions for exploratory data analysis (EDA);\nExplore the Ames Housing dataset.\n\n\n\n\n\n\nFirst, let’s load the required packages. We will use the tidyverse for general data processing and visualisation.\n\nlibrary(tidyverse)\nlibrary(naniar) # for visualising missing data\nlibrary(GGally) # for EDA\nlibrary(ggcorrplot)\nlibrary(AmesHousing)\nlibrary(plotly) # dynamic visualisations\nlibrary(bestNormalize)\nlibrary(qs)\nlibrary(janitor)\ntheme_set(theme_minimal())\n\nWe will use the Ames housing data to explore different ML approaches to regression. This dataset was “designed” by Dean De Cock as an alternative to the “classic” Boston housing dataset, and has been extensively used in ML teaching. It is also available from kaggle as part of its advanced regression practice competition.\nThe independent variables presented in the data include:\n\n20 continuous variables relate to various area dimensions for each observation;\n14 discrete variables, which typically quantify the number of items occurring within the house;\n23 ordinal, 23 nominal categorical variables, with 2 (STREET: gravel or paved) - 28 (NEIGHBORHOOD) classes;\n\nWe will explore both the “uncleaned” data available from kaggle/UCI, and the processed data available in the AmesHousing package in R, for which documentation is available here. It can be useful for understanding what each of the independent variables mean.\n\nameshousing_temp <- AmesHousing::make_ames()\n\n# Use this function to make the names easier to type\nameshousing <- ameshousing_temp %>% \n  janitor::clean_names()\n\n# Read in the uncleaned data. \nameshousing_uncleaned <- AmesHousing::ames_raw\n\n\n\n\nExploratory data analysis involves looking at:\n\nThe distribution of variables in your dataset;\nWhether any data is missing;\nData skewness;\nCorrelated variables.\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\nExplore the Ames Housing dataset.\n\nWhat can you figure out about the different variables?\nWhich do you think are more or less important?\n\nCompare the ameshousing variable, which is from the AmesHousing package in R and has been cleaned, with the ameshousing_uncleaned dataset, which is the raw data from the UCI machine learning repository.\n\nWhat was missing in the raw data?\nWhat are some of the approaches that have been taken to deal with missingness?\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can see that the “uncleaned” dataset has a lot of missing data, whereas it has been cleaned up for us in the “cleaned” one. In the interests of time, we will not focus here on how every variable in that dataset has been explored and cleaned up - however, it presents a good example of “messy” real-world data, so we would encourage you to try and look at a handful of variables at home, to see how they’ve been processed.\n\ndim(ameshousing)\n\n[1] 2930   81\n\nglimpse(ameshousing)\n\nRows: 2,930\nColumns: 81\n$ ms_sub_class       <fct> One_Story_1946_and_Newer_All_Styles, One_Story_1946…\n$ ms_zoning          <fct> Residential_Low_Density, Residential_High_Density, …\n$ lot_frontage       <dbl> 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,…\n$ lot_area           <int> 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005…\n$ street             <fct> Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav…\n$ alley              <fct> No_Alley_Access, No_Alley_Access, No_Alley_Access, …\n$ lot_shape          <fct> Slightly_Irregular, Regular, Slightly_Irregular, Re…\n$ land_contour       <fct> Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L…\n$ utilities          <fct> AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All…\n$ lot_config         <fct> Corner, Inside, Corner, Corner, Inside, Inside, Ins…\n$ land_slope         <fct> Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G…\n$ neighborhood       <fct> North_Ames, North_Ames, North_Ames, North_Ames, Gil…\n$ condition_1        <fct> Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No…\n$ condition_2        <fct> Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ bldg_type          <fct> OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn…\n$ house_style        <fct> One_Story, One_Story, One_Story, One_Story, Two_Sto…\n$ overall_qual       <fct> Above_Average, Average, Above_Average, Good, Averag…\n$ overall_cond       <fct> Average, Above_Average, Above_Average, Average, Ave…\n$ year_built         <int> 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199…\n$ year_remod_add     <int> 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199…\n$ roof_style         <fct> Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G…\n$ roof_matl          <fct> CompShg, CompShg, CompShg, CompShg, CompShg, CompSh…\n$ exterior_1st       <fct> BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ exterior_2nd       <fct> Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ mas_vnr_type       <fct> Stone, None, BrkFace, None, None, BrkFace, None, No…\n$ mas_vnr_area       <dbl> 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6…\n$ exter_qual         <fct> Typical, Typical, Typical, Good, Typical, Typical, …\n$ exter_cond         <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ foundation         <fct> CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc…\n$ bsmt_qual          <fct> Typical, Typical, Typical, Typical, Good, Typical, …\n$ bsmt_cond          <fct> Good, Typical, Typical, Typical, Typical, Typical, …\n$ bsmt_exposure      <fct> Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,…\n$ bsmt_fin_type_1    <fct> BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U…\n$ bsmt_fin_sf_1      <dbl> 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, …\n$ bsmt_fin_type_2    <fct> Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U…\n$ bsmt_fin_sf_2      <dbl> 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0…\n$ bsmt_unf_sf        <dbl> 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,…\n$ total_bsmt_sf      <dbl> 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, …\n$ heating            <fct> GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas…\n$ heating_qc         <fct> Fair, Typical, Typical, Excellent, Good, Excellent,…\n$ central_air        <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ electrical         <fct> SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB…\n$ first_flr_sf       <int> 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, …\n$ second_flr_sf      <int> 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,…\n$ low_qual_fin_sf    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gr_liv_area        <int> 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616…\n$ bsmt_full_bath     <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, …\n$ bsmt_half_bath     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ full_bath          <int> 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, …\n$ half_bath          <int> 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ bedroom_abv_gr     <int> 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, …\n$ kitchen_abv_gr     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ kitchen_qual       <fct> Typical, Typical, Good, Excellent, Typical, Good, G…\n$ tot_rms_abv_grd    <int> 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,…\n$ functional         <fct> Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T…\n$ fireplaces         <int> 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ fireplace_qu       <fct> Good, No_Fireplace, No_Fireplace, Typical, Typical,…\n$ garage_type        <fct> Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att…\n$ garage_finish      <fct> Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F…\n$ garage_cars        <dbl> 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, …\n$ garage_area        <dbl> 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4…\n$ garage_qual        <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ garage_cond        <fct> Typical, Typical, Typical, Typical, Typical, Typica…\n$ paved_drive        <fct> Partial_Pavement, Paved, Paved, Paved, Paved, Paved…\n$ wood_deck_sf       <int> 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48…\n$ open_porch_sf      <int> 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0…\n$ enclosed_porch     <int> 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ three_season_porch <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ screen_porch       <int> 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, …\n$ pool_area          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ pool_qc            <fct> No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo…\n$ fence              <fct> No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini…\n$ misc_feature       <fct> None, None, Gar2, None, None, None, None, None, Non…\n$ misc_val           <int> 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, …\n$ mo_sold            <int> 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, …\n$ year_sold          <int> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ sale_type          <fct> WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W…\n$ sale_condition     <fct> Normal, Normal, Normal, Normal, Normal, Normal, Nor…\n$ sale_price         <int> 215000, 105000, 172000, 244000, 189900, 195500, 213…\n$ longitude          <dbl> -93.61975, -93.61976, -93.61939, -93.61732, -93.638…\n$ latitude           <dbl> 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4…\n\ncolSums(is.na(ameshousing_uncleaned))\n\n          Order             PID     MS SubClass       MS Zoning    Lot Frontage \n              0               0               0               0             490 \n       Lot Area          Street           Alley       Lot Shape    Land Contour \n              0               0            2732               0               0 \n      Utilities      Lot Config      Land Slope    Neighborhood     Condition 1 \n              0               0               0               0               0 \n    Condition 2       Bldg Type     House Style    Overall Qual    Overall Cond \n              0               0               0               0               0 \n     Year Built  Year Remod/Add      Roof Style       Roof Matl    Exterior 1st \n              0               0               0               0               0 \n   Exterior 2nd    Mas Vnr Type    Mas Vnr Area      Exter Qual      Exter Cond \n              0              23              23               0               0 \n     Foundation       Bsmt Qual       Bsmt Cond   Bsmt Exposure  BsmtFin Type 1 \n              0              80              80              83              80 \n   BsmtFin SF 1  BsmtFin Type 2    BsmtFin SF 2     Bsmt Unf SF   Total Bsmt SF \n              1              81               1               1               1 \n        Heating      Heating QC     Central Air      Electrical      1st Flr SF \n              0               0               0               1               0 \n     2nd Flr SF Low Qual Fin SF     Gr Liv Area  Bsmt Full Bath  Bsmt Half Bath \n              0               0               0               2               2 \n      Full Bath       Half Bath   Bedroom AbvGr   Kitchen AbvGr    Kitchen Qual \n              0               0               0               0               0 \n  TotRms AbvGrd      Functional      Fireplaces    Fireplace Qu     Garage Type \n              0               0               0            1422             157 \n  Garage Yr Blt   Garage Finish     Garage Cars     Garage Area     Garage Qual \n            159             159               1               1             159 \n    Garage Cond     Paved Drive    Wood Deck SF   Open Porch SF  Enclosed Porch \n            159               0               0               0               0 \n     3Ssn Porch    Screen Porch       Pool Area         Pool QC           Fence \n              0               0               0            2917            2358 \n   Misc Feature        Misc Val         Mo Sold         Yr Sold       Sale Type \n           2824               0               0               0               0 \n Sale Condition       SalePrice \n              0               0 \n\ncolSums(is.na(ameshousing))\n\n      ms_sub_class          ms_zoning       lot_frontage           lot_area \n                 0                  0                  0                  0 \n            street              alley          lot_shape       land_contour \n                 0                  0                  0                  0 \n         utilities         lot_config         land_slope       neighborhood \n                 0                  0                  0                  0 \n       condition_1        condition_2          bldg_type        house_style \n                 0                  0                  0                  0 \n      overall_qual       overall_cond         year_built     year_remod_add \n                 0                  0                  0                  0 \n        roof_style          roof_matl       exterior_1st       exterior_2nd \n                 0                  0                  0                  0 \n      mas_vnr_type       mas_vnr_area         exter_qual         exter_cond \n                 0                  0                  0                  0 \n        foundation          bsmt_qual          bsmt_cond      bsmt_exposure \n                 0                  0                  0                  0 \n   bsmt_fin_type_1      bsmt_fin_sf_1    bsmt_fin_type_2      bsmt_fin_sf_2 \n                 0                  0                  0                  0 \n       bsmt_unf_sf      total_bsmt_sf            heating         heating_qc \n                 0                  0                  0                  0 \n       central_air         electrical       first_flr_sf      second_flr_sf \n                 0                  0                  0                  0 \n   low_qual_fin_sf        gr_liv_area     bsmt_full_bath     bsmt_half_bath \n                 0                  0                  0                  0 \n         full_bath          half_bath     bedroom_abv_gr     kitchen_abv_gr \n                 0                  0                  0                  0 \n      kitchen_qual    tot_rms_abv_grd         functional         fireplaces \n                 0                  0                  0                  0 \n      fireplace_qu        garage_type      garage_finish        garage_cars \n                 0                  0                  0                  0 \n       garage_area        garage_qual        garage_cond        paved_drive \n                 0                  0                  0                  0 \n      wood_deck_sf      open_porch_sf     enclosed_porch three_season_porch \n                 0                  0                  0                  0 \n      screen_porch          pool_area            pool_qc              fence \n                 0                  0                  0                  0 \n      misc_feature           misc_val            mo_sold          year_sold \n                 0                  0                  0                  0 \n         sale_type     sale_condition         sale_price          longitude \n                 0                  0                  0                  0 \n          latitude \n                 0 \n\n\n\n\n\n\n\nWhen working with missing data, it can be helpful to look for “co-missingness”, i.e. multiple variables missing together. For example, when working with patient data, number of pregnancies, age at onset of menstruation and menopause may all be missing - which, when observed together, may indicate that these samples come from male patients for whom this data is irrelevant. “Gender” may or may not be a variable coded in the dataset.\nA way of visualising missing data in the tidy context has been proposed @tierney2018expanding. See this web page for more options for your own data.\nLet’s look at the missing variables in our housing data:\n\ngg_miss_var(ameshousing_uncleaned)\n\n\n\n\nWe can see that the most missingness is observed in the Pool_QC, Misc_Feature, Alley, Fence and Fireplace_QC variables. This is most likely due to many houses not having pools, alleys, fences, and fireplaces, and not having any features that the real estate agent considers to be notable enough to be added to the “miscellaneous” category.\nAn upset plot will give us more idea about the co-missingness of these variables:\n\ngg_miss_upset(ameshousing_uncleaned, nsets = 10)\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\nWhich variables are most frequently missing together?\nDoes this “co-missingness” make sense?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nFence, alley, misc feature and pool qc are most often missing together. This probably means that a house doesn’t have an alley, a fence, a pool or any other miscellaneous features.\nSimilarly, the second most frequent “co-missingess” involves these plus missing “fireplace quality”, most likely due to the house not having fireplace.\nWe can also see that garage_yr_blt, garage_finish, garage_qual and garage_cond “co-miss” the same number of times - probably because these represent houses without garages.\n\n\n\n\nNext, let’s create two “helper” vectors with the names of the numeric and categorical variables from the ameshousing dataset, which we can then use to batch subset our dataset prior to EDA/visualisation:\n\n# pull out all of the numerical variables\nnumVars <- ameshousing %>% \n  select_if(is.numeric) %>%\n  names()\n\n# use Negate(is.numeric) to pull out all of the categorical variables\ncatVars <- ameshousing %>% \n  select_if(Negate(is.numeric)) %>%\n  names()\n\nLet’s then use the ggpairs() function to generate a plot of the first 10 numeric variables (and sale price, which is 33) against each other. We can repeat this for variables 11-20 and 21-33.\n\nggpairs(data = ameshousing, \n        columns = numVars[c(1:10, 33)], \n        title = \"Numeric variables 1 - 10\")\n\n\n\n# ggpairs(ameshousing, numVars[c(11:20, 33)], title = \"Numeric variables 11 - 20\")\n# ggpairs(ameshousing, numVars[c(21:33)], title = \"Numeric variables 21 - 33\")\nggpairs(data = ameshousing, \n        columns = c(catVars[2:5], \"sale_price\"), \n        title = \"Some categorical variables\")\n\n\n\n\nNext, we can generate a correlation plot between all of our numeric variables. By default, the cor() method will calculate the Pearson correlation between the Sale_Price and the other variables, and we can specify how we’d like to handle missing data when calculating this correlation.\nIn this case, we use pairwise.complete.obs, which calculates the correlation between each pair of variables using all complete pairs of observations on those variables.\nWe then plot the correlation using the corrplot library, which has several options for how to visualise a correlation plot. See here for some examples of the visualisations it can produce.\n\n# pairs.panels(ameshousing[ , names(ameshousing)[c(3, 16, 23, 27,37)]], scale=TRUE)\nameshousingCor <- cor(ameshousing[,numVars],\n                      use = \"pairwise.complete.obs\")\n\nameshousingCor_pvalues <- cor_pmat(ameshousingCor)\nggcorrplot(ameshousingCor, type = \"lower\")\n\n\n\n\nWe can also make a dynamic visualisation using plotly.\n\n#Bonus: interactive corrplot with zoom and mouseover\nggcorrplot(ameshousingCor, type = \"lower\") %>% ggplotly()\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\n\nWhat variables are the most correlated with SalePrice?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nas_tibble(ameshousingCor, rownames = \"rowname\") %>%\n  gather(pair, value, -rowname) %>%\n  filter(rowname != pair) %>% #remove self correlation\n  filter(rowname == \"sale_price\") %>%\n  arrange(desc(abs(value))) %>%\n  head()\n\n# A tibble: 6 × 3\n  rowname    pair          value\n  <chr>      <chr>         <dbl>\n1 sale_price gr_liv_area   0.707\n2 sale_price garage_cars   0.648\n3 sale_price garage_area   0.640\n4 sale_price total_bsmt_sf 0.633\n5 sale_price first_flr_sf  0.622\n6 sale_price year_built    0.558\n\n\nWe can also plot this, using a slightly different representation:\n\nCircles instead of only colour to represent correlation levels\nFilter out correlations less than 0.5\n\n\nall_numVar <- ameshousing[, numVars]\ncor_numVar <- cor(all_numVar, use=\"pairwise.complete.obs\") \nCorHigh <- as_tibble(\n  data.frame(correlation = cor_numVar[,'sale_price']), rownames = \"rownames\")  %>% \n  filter(abs(correlation) >= 0.5) %>% \n  .$rownames\nggcorrplot(cor_numVar[CorHigh, CorHigh], type = \"lower\", \"circle\")\n\n\n\n\n\n\n\nLet’s plot one of these relationships:\n\nameshousing %>%\n  ggplot(aes(x = gr_liv_area, y = sale_price/1000)) + \n  geom_point(alpha = 0.1) + \n  labs(y = \"Sale Price/$1000\",\n       x = \"Living Area (sq.ft)\",\n       title = \"Ames Housing Data\") +\n  geom_smooth(method= \"lm\")  +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nOptional: Make Figures Interactive\n\n\n\n\nHow can we make our figures interactive?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nUsing the plotly package, we can turn any ggplot() object into an interactive plot using plotly::ggplotly()\n\nFor example:\n\n# First we save the ggplot as an object\nplot <- ameshousing %>%\n  ggplot(aes(x = gr_liv_area, y = sale_price / 1000)) +\n  geom_point(alpha = 0.1) +\n  labs(y = \"Sale Price/$1000\",\n       x = \"Living Area (sq.ft)\",\n       title = \"Ames Housing Data\") +\n  geom_smooth(method = \"lm\")  +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# then we pass it to ggplotly()\nggplotly(plot)\n\n\n\n\n\n\n\n\nWe can see that there are five houses with an area > 4000 square feet that seem to be outliers in the data. We should filter them out. Next, let’s generate a boxplot by Quality:\n\n# Create a filtered dataframe\nameshousing_filt <-\n  ameshousing %>%\n  filter(gr_liv_area <= 4000)\n\n# Make our ggplot object\np <- ameshousing_filt %>%\n  mutate(quality = as.factor(overall_qual)) %>%\n  ggplot(aes(x = quality,\n             y = sale_price / 1000,\n             fill = quality)) +\n  labs(y = \"Sale Price in $k's\",\n       x = \"Overall Quality of House\",\n       title = \"Ames Housing Data\") +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Now make it a plotly\nggplotly(p)\n\n\n\n\n\n\n\n\n\nYou also need to do EDA on the outcome variable to:\n\nidentify outliers\nexplore whether there is any skew in its distribution\nidentify a transformation to use when modelling the data (if appropriate)\n\nThis is because many models, including ordinary linear regression, assume that prediction errors (and hence the response) are normally distributed.\n\nameshousing_filt %>% \n  ggplot(aes(x = sale_price/1000)) + \n  geom_histogram(bins = 50) + \n  labs(x = \"Sale Price in $k's\",\n       y = \"Number of Houses sold\")\n\n\n\n\nLet’s explore different ways of transforming the Sale Price.\n\n#No transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = sale_price)) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#Sqrt transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = sqrt(sale_price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#natural log transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = log(sale_price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n#log10 transform\n\nameshousing_filt %>%\n  ggplot(aes( sample = log10(sale_price))) +\n  stat_qq() + stat_qq_line(col = \"blue\")\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\n\nIf you were working with this dataset, which of the above would you prefer?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe log10 transformation seems best, as it both helps the distribution look more normal and helps keep our error metrics and final predictions easily interpretable. It also means that the errors of predicting the values of inexpensive and expensive houses will affect the prediction equally.\n\nbestNormalize::bestNormalize(\n  ameshousing_filt$sale_price,\n  allow_orderNorm = FALSE)\n\nBest Normalizing transformation with 2925 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6306\n - Box-Cox: 1.6891\n - Center+scale: 5.1734\n - Double Reversed Log_b(x+a): 10.0059\n - Log_b(x+a): 1.6306\n - sqrt(x + a): 2.8254\n - Yeo-Johnson: 1.6932\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 2925 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 12.71303 \n - sd (before standardization) = 0.4060161 \n\n\nThe bestNormalize library can be used to identify the best normalising transformation. Note that in this case, the arcsinh(x) and logarithmic transformations both achieve best normalisation results. To make interpretation a bit easier, we choose the logarithmic transformation.\n\n\n\n\nameshousing_filt$sale_price <- log10(ameshousing_filt$sale_price)\n\n\n\n\nThe year in which the house was built and the year when it was remodelled are not really the most relevant parameters we look at when buying a house: instead, buyers usually care a lot more about the age of the house and the time since the last remodel. Let’s transform these features:\n\nameshousing_filt_tr <-\n  ameshousing_filt %>%\n  mutate(time_since_remodel = year_sold - year_remod_add, \n         house_age = year_sold - year_built) %>%\n  select(-year_remod_add, -year_built)\n\nqsave(ameshousing_filt_tr, \"../_models/ames_dataset_filt.qs\")\n\n\nNote Make sure to create a “models” folder in your project working directory! Before you can save your data as .Rds objects, you will actually need to create a folder for these files to go into. Do this by clicking on the “new folder” button in the files window in R studio. Rename your new folder to “models”.\n\n\n\n\n\n\n\nKey points\n\n\n\nExploratory Data Analysis (EDA) is an essential first step in ML.\n\n\nTierney, Nicholas J, and Dianne H Cook. 2018. “Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations.” arXiv Preprint arXiv:1809.02264."
  },
  {
    "objectID": "100_regression/step2.html",
    "href": "100_regression/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nFrom base R to tidymodels;\nSplit our data into training and test sets;\nPreprocess the training data;\nSpecify a linear regression model;\nTrain our model on the training data;\nTransform the test data and obtain predictions using our trained model.\n\n\n\n\n\n\n\n\n\nExercise:\n\n\n\nIn this case study, you will predict houses selling price from characteristics of these houses, like size and layout of the living space in the house. What kind of model will you build?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo predict a continuous, numeric quantity like selling price, use regression models.\n\n\n\nLoad in the packages we’ll be using for modelling:\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(vip) \nlibrary(qs)\ntheme_set(theme_minimal())\n\n\names_data <- qread(\"../_models/ames_dataset_filt.qs\")\n\n\n\nIn a linear model, we assume that there is a linear relationship between the input variable(s) and the output variable. This means that as the input variable(s) increase or decrease, the output variable changes in a straight line.\nImagine you have a scatter plot with your data points all over it. A linear model is like drawing a straight line through the scatter plot that best fits all the points. The slope and intercept of this line are chosen in such a way that the distance between the line and all the points is minimized. This line is then used to predict the output for new input values.\n\n\n\nExample of a linear model\n\n\nThe straight red dotted line represents the linear model equation \\(y=mx+c\\), where \\(c\\) is the y-intercept of the regression line, \\(m\\) is the slope of the regression line, and \\(y\\) is the expected value for y for the given \\(x\\) value.\n\n#fit a linear model\names_lm <- lm(sale_price ~ gr_liv_area, data = ames_data)\n\n#Print the summary of the model\nsummary(ames_lm)\n\n\nCall:\nlm(formula = sale_price ~ gr_liv_area, data = ames_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94258 -0.06622  0.01359  0.07298  0.39246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.835e+00  7.406e-03  652.80   <2e-16 ***\ngr_liv_area 2.579e-04  4.714e-06   54.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.124 on 2923 degrees of freedom\nMultiple R-squared:  0.506, Adjusted R-squared:  0.5058 \nF-statistic:  2994 on 1 and 2923 DF,  p-value: < 2.2e-16\n\n\nR-squared value explains the variability of y with respect to x:\n\nvaries between 0 to 1 (0-100%);\nR-squared values closer to 0 mean the regression relationship is very low;\nR-squared values closer to 1 mean the regression relationship is very strong.\n\nLet’s plot our linear regression model:\n\names_data %>% \n  ggplot(aes(x=gr_liv_area,y=sale_price)) +\n  geom_point(alpha=0.25) +\n  geom_abline(\n    slope = coef(ames_lm)[[\"gr_liv_area\"]],\n    intercept = coef(ames_lm)[[\"(Intercept)\"]],\n    color = \"red\"\n  ) +\n  labs(\n    x = \"Gross Living Area\",\n    y = \"Sale Price\"\n  )\n\n\n\n\n\n\n\nWhen you type library(tidymodels), you load a collection of packages for modeling and machine learning using tidyverse principles. Some benefits on using Tidymodels include:\n\nconsistent syntax across all of its packages, making it easier to learn and use;\ncode is more readable and easier to maintain;\nbuilt around the tidy data principles, which emphasizes the importance of organizing data in a consistent and structured way;\nmodular design, each package serving a specific purpose;\nactive community of developers and users who are available to answer questions and provide support;\nintegration with other tidyverse packages like dplyr, ggplot2, and purrr, allowing for a more streamlined workflow when working with data\n\nThe first thing we are going to practice is splitting the data into a training set and a testing set. The tidymodels package rsample has functions that help you specify training and testing sets:\n\nset.seed(42) #so we all get the same results\names_split <- ames_data %>%\n    initial_split(prop = 0.8,\n                  strata = sale_price) #stratification\n\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\nqsave(ames_train, \"../_models/ames_train.qs\")\nqsave(ames_test, \"../_models/ames_test.qs\")\n\nStratified sampling would split within each quartile. Splitting with stratification involves dividing the data into subsets based on the target/outcome variable’s distribution, such that the proportion of each class in the target variable is maintained in each subset. This ensures that the training and testing sets have a similar distribution of the target variable, which can lead to more reliable model performance estimates. \nThe code here takes an input data set and puts 80% of it into a training dataset and 20% of it into a testing dataset; it chooses the individual cases so that both sets are balanced in selling price.\nLet’s check if the distribution of the selling price is the same in the testing and training datasets:\n\names_train %>% \n  ggplot(aes(x = log(sale_price),  col = \"red\", fill = NULL)) + \n  geom_density() + theme_minimal() +\n  geom_line(data = ames_test,\n            stat = \"density\",\n            col = \"blue\") + theme(legend.position=\"none\")\n\n\n\n\n\n\n\nWe might want to modify our predictors columns for a few reasons:\n\nThe model requires them in a different format;\nThe model needs certain data qualities;\nThe outcome is better predicted when one or more columns are transformed in some way (a.k.a “feature engineering”).\n\nIn tidymodels, you can use the recipes package, an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.\nSome of these steps can include:\n\nScaling and centering numeric predictors;\nRemoving skewness from numeric variables;\nOne-hot and dummy variable encoding for categorical variables;\nRemoving correlated predictors and zero variance variables;\nImputing missing data.\n\nStatistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.\nThe resulting processed output can be used as inputs for statistical or machine learning models.\n\names_rec <-\n  recipe(sale_price ~ ., data = ames_train) %>% #assigns columns to roles of “outcome” or “predictor” using the formula\n  step_other(all_nominal(), threshold = 0.01) %>% #useful when you have some factor levels with very few observations,   all_nominal selects both characters and factors, pools infrequently occurring values (frequency less than 0.01) into an \"other\" category\n  step_nzv(all_predictors()) %>% #remove predictors that are highly sparse and unbalanced\n  step_normalize(all_numeric_predictors()) %>% #normalize the data to a standard range by dividing each observation by the standard deviation of the feature\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) #create numeric representations of categorical data\names_rec\n\nqsave(ames_rec, \"../_models/ames_rec.qs\")\n\nWhen calling recipe(..., ames_train), the training set is used to determine the data types of each column so that selectors such as all_numeric_predictors() can be used.\nNote that each successive step() function adds a preprocessing step to our recipe object in the order that they are provided. The preprocessing recipe ames_rec has been defined but no values have been estimated.\n\n\n\n\n\n\nThe prep() function takes a recipe and computes everything so that the preprocessing steps can be executed. Note that this is done with the training data.\n\n\n\names_prep <- prep(ames_rec)\n\names_prep\n\nThe bake() and juice() functions both return data, not a preprocessing recipe object.\n\n\nThe bake() function takes a prepped recipe (one that has had all quantities estimated from training data) and applies it to new_data. That new_data could be the training data again or it could be the testing data (with the TRAINING parameters).\n\n\n\names_test_baked <- bake(ames_prep, new_data = ames_test)\n\n\n\nThe juice() function is a nice little shortcut. When we juice() the recipe, we squeeze that training data back out, transformed in the ways we specified.\n\n\nLet’s compare the bake() and juice() outputs:\n\nbake(ames_prep, new_data = ames_train)\n\n# A tibble: 2,339 × 224\n   lot_frontage lot_area mas_vnr_area bsmt_fin_sf_1 bsmt_fin_sf_2 bsmt_unf_sf\n          <dbl>    <dbl>        <dbl>         <dbl>         <dbl>       <dbl>\n 1       0.374   -0.212        -0.569        -1.42          0.162      -1.27 \n 2       0.374    0.0383       -0.569        -1.42         -0.298      -0.288\n 3      -0.137   -0.733        -0.569        -1.42         -0.298       0.341\n 4      -1.01    -0.943        -0.569         1.27         -0.298       0.630\n 5      -0.0770  -0.273        -0.569         1.27         -0.298       0.816\n 6      -0.227   -0.359        -0.569        -1.42          0.416      -1.27 \n 7       0.374   -0.0453       -0.569         1.27         -0.298       0.584\n 8       0.314   -0.149        -0.569         0.372        -0.298      -1.27 \n 9      -1.73    -0.0430       -0.391        -0.970        -0.298      -0.288\n10      -1.73    -0.392        -0.569        -1.42         -0.298      -0.404\n# ℹ 2,329 more rows\n# ℹ 218 more variables: total_bsmt_sf <dbl>, first_flr_sf <dbl>,\n#   second_flr_sf <dbl>, gr_liv_area <dbl>, bsmt_full_bath <dbl>,\n#   bsmt_half_bath <dbl>, full_bath <dbl>, half_bath <dbl>,\n#   bedroom_abv_gr <dbl>, tot_rms_abv_grd <dbl>, fireplaces <dbl>,\n#   garage_cars <dbl>, garage_area <dbl>, wood_deck_sf <dbl>,\n#   open_porch_sf <dbl>, mo_sold <dbl>, year_sold <dbl>, longitude <dbl>, …\n\njuice(ames_prep) \n\n# A tibble: 2,339 × 224\n   lot_frontage lot_area mas_vnr_area bsmt_fin_sf_1 bsmt_fin_sf_2 bsmt_unf_sf\n          <dbl>    <dbl>        <dbl>         <dbl>         <dbl>       <dbl>\n 1       0.374   -0.212        -0.569        -1.42          0.162      -1.27 \n 2       0.374    0.0383       -0.569        -1.42         -0.298      -0.288\n 3      -0.137   -0.733        -0.569        -1.42         -0.298       0.341\n 4      -1.01    -0.943        -0.569         1.27         -0.298       0.630\n 5      -0.0770  -0.273        -0.569         1.27         -0.298       0.816\n 6      -0.227   -0.359        -0.569        -1.42          0.416      -1.27 \n 7       0.374   -0.0453       -0.569         1.27         -0.298       0.584\n 8       0.314   -0.149        -0.569         0.372        -0.298      -1.27 \n 9      -1.73    -0.0430       -0.391        -0.970        -0.298      -0.288\n10      -1.73    -0.392        -0.569        -1.42         -0.298      -0.404\n# ℹ 2,329 more rows\n# ℹ 218 more variables: total_bsmt_sf <dbl>, first_flr_sf <dbl>,\n#   second_flr_sf <dbl>, gr_liv_area <dbl>, bsmt_full_bath <dbl>,\n#   bsmt_half_bath <dbl>, full_bath <dbl>, half_bath <dbl>,\n#   bedroom_abv_gr <dbl>, tot_rms_abv_grd <dbl>, fireplaces <dbl>,\n#   garage_cars <dbl>, garage_area <dbl>, wood_deck_sf <dbl>,\n#   open_porch_sf <dbl>, mo_sold <dbl>, year_sold <dbl>, longitude <dbl>, …\n\n\nNote that the juice() output is the same as bake(ames_rep, new_data = ames_train) and is just a shortcut that we are going to use later.\n\n\n\n\n\n\nChallenge 5\n\n\n\nWithin the recipe, does it make sense to apply the preprocessing steps to the test set?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, it doesn’t. You want the test set to look like new data that your model will see in the future. All preprocessing and feature engineering steps use only the training data. Otherwise, information leakage can negatively impact the model’s performance when used with new data. \nIt is helpful to know how to prep() and bake() because often you want to be able to dig into the internals or troubleshoot problems with recipe preprocessing.\n\n\n\n\n\n\nIn tidymodels, you specify models using three concepts:\n\ntype differentiates models such as logistic regression, linear regression, and so forth;\nmode includes common options like regression and classification, some model types support either of these while some only have one mode;\nengine is the computational tool which will be used to fit the model.\n\nWe will specify the model using the parsnip package. Many functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values.\n\n#a linear regression model specification\names_model <- linear_reg() %>% #pick a model\n  set_engine(\"lm\")           #set the engine\n                             #set_mode(\"regression\") we don't need this as the model linear_reg() only does regression\n\n#view model properties\names_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\n\nNow we are ready to train our model object on the training data. We can do this using the fit() function from the parsnip package. The fit() function takes the following arguments:\n\na parnsip model object specification;\na model formula\na data frame with the training data\n\nThe code below trains our linear regression model on the prepped training data. In our formula, we have specified that sale_price is the response variable and included all the rest as our predictor variables.\n\names_fit <- ames_model %>%\n  fit(sale_price ~ .,\n      data=juice(ames_prep))\n\n# View lm_fit properties\names_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = sale_price ~ ., data = data)\n\nCoefficients:\n                                           (Intercept)  \n                                             4.225e+00  \n                                          lot_frontage  \n                                             1.510e-03  \n                                              lot_area  \n                                             5.634e-03  \n                                          mas_vnr_area  \n                                             3.178e-03  \n                                         bsmt_fin_sf_1  \n                                             2.910e-02  \n                                         bsmt_fin_sf_2  \n                                            -2.438e-03  \n                                           bsmt_unf_sf  \n                                            -9.865e-03  \n                                         total_bsmt_sf  \n                                             3.052e-02  \n                                          first_flr_sf  \n                                             4.657e-03  \n                                         second_flr_sf  \n                                             9.635e-03  \n                                           gr_liv_area  \n                                             4.895e-02  \n                                        bsmt_full_bath  \n                                             4.946e-03  \n                                        bsmt_half_bath  \n                                             1.160e-03  \n                                             full_bath  \n                                             7.528e-03  \n                                             half_bath  \n                                             6.568e-03  \n                                        bedroom_abv_gr  \n                                            -3.780e-03  \n                                       tot_rms_abv_grd  \n                                            -4.477e-05  \n                                            fireplaces  \n                                             5.691e-03  \n                                           garage_cars  \n                                             8.626e-03  \n                                           garage_area  \n                                             6.286e-03  \n                                          wood_deck_sf  \n                                             1.943e-03  \n                                         open_porch_sf  \n                                             1.901e-03  \n                                               mo_sold  \n                                            -3.097e-04  \n                                             year_sold  \n                                            -2.016e-03  \n                                             longitude  \n                                            -8.325e-03  \n                                              latitude  \n                                            -4.980e-03  \n                                    time_since_remodel  \n                                            -5.556e-03  \n                                             house_age  \n                                            -1.929e-02  \n      ms_sub_class_One_Story_1946_and_Newer_All_Styles  \n                                             6.196e-03  \n                 ms_sub_class_One_Story_1945_and_Older  \n                                            -2.177e-02  \n     ms_sub_class_One_and_Half_Story_Finished_All_Ages  \n                                             2.813e-02  \n                 ms_sub_class_Two_Story_1946_and_Newer  \n                                            -1.093e-02  \n                 ms_sub_class_Two_Story_1945_and_Older  \n                                             1.847e-02  \n                      ms_sub_class_Split_or_Multilevel  \n                                            -2.027e-02  \n                              ms_sub_class_Split_Foyer  \n                                             3.816e-03  \n               ms_sub_class_Duplex_All_Styles_and_Ages  \n                                            -2.177e-03  \n             ms_sub_class_One_Story_PUD_1946_and_Newer  \n                                             2.046e-02  \n             ms_sub_class_Two_Story_PUD_1946_and_Newer  \n                                            -2.533e-02  \nms_sub_class_Two_Family_conversion_All_Styles_and_Ages  \n                                             2.991e-03  \n                                    ms_sub_class_other  \n                                                    NA  \n                ms_zoning_Floating_Village_Residential  \n                                             3.984e-02  \n                     ms_zoning_Residential_Low_Density  \n                                             3.224e-02  \n                  ms_zoning_Residential_Medium_Density  \n                                             2.443e-02  \n                                       ms_zoning_other  \n                                                    NA  \n                                     lot_shape_Regular  \n                                             4.462e-03  \n                          lot_shape_Slightly_Irregular  \n                                             4.294e-03  \n                        lot_shape_Moderately_Irregular  \n                                             1.805e-02  \n                                       lot_shape_other  \n                                                    NA  \n                                     lot_config_Corner  \n                                             5.998e-03  \n                                    lot_config_CulDSac  \n                                             1.298e-02  \n                                        lot_config_FR2  \n                                            -5.176e-03  \n                                     lot_config_Inside  \n                                             3.934e-03  \n                                      lot_config_other  \n                                                    NA  \n                               neighborhood_North_Ames  \n                                            -1.547e-02  \n                            neighborhood_College_Creek  \n                                            -3.736e-02  \n                                 neighborhood_Old_Town  \n                                            -3.894e-02  \n                                  neighborhood_Edwards  \n                                            -4.494e-02  \n                                 neighborhood_Somerset  \n                                             1.495e-02  \n                       neighborhood_Northridge_Heights  \n                                             1.344e-02  \n                                  neighborhood_Gilbert  \n                                            -8.690e-03  \n                                   neighborhood_Sawyer  \n                                            -2.252e-02  \n                           neighborhood_Northwest_Ames  \n                                            -1.568e-02  \n                              neighborhood_Sawyer_West  \n                                            -3.405e-02  \n                                 neighborhood_Mitchell  \n                                            -2.449e-02  \n                                neighborhood_Brookside  \n                                            -1.037e-02  \n                                 neighborhood_Crawford  \n                                             1.911e-02  \n                   neighborhood_Iowa_DOT_and_Rail_Road  \n                                            -4.515e-02  \n                               neighborhood_Timberland  \n                                            -2.225e-02  \n                               neighborhood_Northridge  \n                                             1.195e-02  \n                              neighborhood_Stone_Brook  \n                                             3.834e-02  \n  neighborhood_South_and_West_of_Iowa_State_University  \n                                            -3.175e-02  \n                              neighborhood_Clear_Creek  \n                                            -1.395e-02  \n                           neighborhood_Meadow_Village  \n                                            -6.325e-02  \n                                    neighborhood_other  \n                                                    NA  \n                                    condition_1_Artery  \n                                            -1.792e-02  \n                                     condition_1_Feedr  \n                                            -1.227e-02  \n                                      condition_1_Norm  \n                                             6.575e-03  \n                                      condition_1_PosN  \n                                             1.180e-02  \n                                      condition_1_RRAn  \n                                            -1.178e-02  \n                                     condition_1_other  \n                                                    NA  \n                                      bldg_type_OneFam  \n                                             3.635e-02  \n                                    bldg_type_TwoFmCon  \n                                             2.470e-02  \n                                      bldg_type_Duplex  \n                                                    NA  \n                                       bldg_type_Twnhs  \n                                            -1.657e-02  \n                                      bldg_type_TwnhsE  \n                                                    NA  \n                          house_style_One_and_Half_Fin  \n                                            -3.279e-02  \n                                 house_style_One_Story  \n                                            -9.496e-03  \n                                    house_style_SFoyer  \n                                             5.423e-04  \n                                      house_style_SLvl  \n                                             1.420e-02  \n                                 house_style_Two_Story  \n                                            -8.472e-03  \n                                     house_style_other  \n                                                    NA  \n                                     overall_qual_Fair  \n                                            -9.709e-03  \n                            overall_qual_Below_Average  \n                                            -7.711e-03  \n                                  overall_qual_Average  \n                                             1.465e-02  \n                            overall_qual_Above_Average  \n                                             2.453e-02  \n                                     overall_qual_Good  \n                                             3.288e-02  \n                                overall_qual_Very_Good  \n                                             5.065e-02  \n                                overall_qual_Excellent  \n                                             5.328e-02  \n                                    overall_qual_other  \n                                                    NA  \n                                     overall_cond_Fair  \n                                             8.424e-02  \n                            overall_cond_Below_Average  \n                                             1.386e-01  \n                                  overall_cond_Average  \n                                             1.647e-01  \n                            overall_cond_Above_Average  \n                                             1.786e-01  \n                                     overall_cond_Good  \n                                             1.976e-01  \n                                overall_cond_Very_Good  \n                                             2.004e-01  \n                                overall_cond_Excellent  \n                                             2.172e-01  \n                                    overall_cond_other  \n                                                    NA  \n                                      roof_style_Gable  \n                                             3.064e-03  \n                                        roof_style_Hip  \n                                             1.400e-03  \n                                      roof_style_other  \n                                                    NA  \n                                  exterior_1st_AsbShng  \n                                            -2.269e-02  \n                                  exterior_1st_BrkFace  \n                                             1.762e-02  \n                                  exterior_1st_CemntBd  \n                                            -6.648e-02  \n                                  exterior_1st_HdBoard  \n                                            -2.029e-02  \n                                  exterior_1st_MetalSd  \n                                            -9.188e-03  \n                                  exterior_1st_Plywood  \n                                            -1.778e-02  \n                                   exterior_1st_Stucco  \n                                            -2.030e-02  \n                                  exterior_1st_VinylSd  \n                                            -3.227e-02  \n                                  exterior_1st_Wd.Sdng  \n                                            -1.530e-02  \n                                  exterior_1st_WdShing  \n                                            -2.727e-02  \n                                    exterior_1st_other  \n                                                    NA  \n                                  exterior_2nd_AsbShng  \n                                            -2.730e-02  \n                                  exterior_2nd_BrkFace  \n                                            -2.295e-02  \n                                  exterior_2nd_CmentBd  \n                                             4.882e-02  \n                                  exterior_2nd_HdBoard  \n                                            -5.905e-03  \n                                  exterior_2nd_MetalSd  \n                                            -5.823e-03  \n                                  exterior_2nd_Plywood  \n                                            -7.550e-03  \n                                   exterior_2nd_Stucco  \n                                             7.010e-03  \n                                  exterior_2nd_VinylSd  \n                                             9.093e-03  \n                                  exterior_2nd_Wd.Sdng  \n                                            -3.418e-03  \n                                  exterior_2nd_Wd.Shng  \n                                             3.114e-03  \n                                    exterior_2nd_other  \n                                                    NA  \n                                  mas_vnr_type_BrkFace  \n                                             1.391e-02  \n                                     mas_vnr_type_None  \n                                             1.470e-02  \n                                    mas_vnr_type_Stone  \n                                             2.233e-02  \n                                    mas_vnr_type_other  \n                                                    NA  \n                                  exter_qual_Excellent  \n                                             4.355e-02  \n                                       exter_qual_Fair  \n                                            -1.018e-02  \n                                       exter_qual_Good  \n                                             7.081e-03  \n                                    exter_qual_Typical  \n                                                    NA  \n                                       exter_cond_Fair  \n                                            -5.441e-02  \n                                       exter_cond_Good  \n                                            -2.790e-02  \n                                    exter_cond_Typical  \n                                            -2.038e-02  \n                                      exter_cond_other  \n                                                    NA  \n                                     foundation_BrkTil  \n                                            -2.178e-02  \n                                     foundation_CBlock  \n                                            -1.965e-02  \n                                      foundation_PConc  \n                                            -1.085e-02  \n                                       foundation_Slab  \n                                             6.116e-05  \n                                      foundation_other  \n                                                    NA  \n                                   bsmt_qual_Excellent  \n                                             3.459e-02  \n                                        bsmt_qual_Fair  \n                                             6.404e-03  \n                                        bsmt_qual_Good  \n                                             2.045e-02  \n                                 bsmt_qual_No_Basement  \n                                             2.428e-02  \n                                     bsmt_qual_Typical  \n                                             1.874e-02  \n                                       bsmt_qual_other  \n                                                    NA  \n                                      bsmt_exposure_Av  \n                                             3.624e-03  \n                                      bsmt_exposure_Gd  \n                                             2.669e-02  \n                                      bsmt_exposure_Mn  \n                                             5.688e-05  \n                                      bsmt_exposure_No  \n                                            -4.150e-03  \n                             bsmt_exposure_No_Basement  \n                                                    NA  \n                                   bsmt_fin_type_1_ALQ  \n                                             8.386e-02  \n                                   bsmt_fin_type_1_BLQ  \n                                             6.682e-02  \n                                   bsmt_fin_type_1_GLQ  \n                                             5.941e-02  \n                                   bsmt_fin_type_1_LwQ  \n                                             3.318e-02  \n                           bsmt_fin_type_1_No_Basement  \n                                                    NA  \n                                   bsmt_fin_type_1_Rec  \n                                             7.956e-03  \n                                   bsmt_fin_type_1_Unf  \n                                                    NA  \n                                  heating_qc_Excellent  \n                                             7.722e-01  \n                                       heating_qc_Fair  \n                                             7.472e-01  \n                                       heating_qc_Good  \n                                             7.654e-01  \n                                    heating_qc_Typical  \n                                             7.598e-01  \n                                      heating_qc_other  \n                                                    NA  \n                                         central_air_N  \n                                            -2.408e-02  \n                                         central_air_Y  \n                                                    NA  \n                                      electrical_FuseA  \n                                            -1.545e-02  \n                                      electrical_FuseF  \n                                            -1.968e-02  \n                                      electrical_SBrkr  \n                                            -1.951e-02  \n                                      electrical_other  \n                                                    NA  \n                                kitchen_qual_Excellent  \n                                             3.178e-02  \n                                     kitchen_qual_Fair  \n                                            -3.273e-03  \n                                     kitchen_qual_Good  \n                                             5.608e-03  \n                                  kitchen_qual_Typical  \n                                                    NA  \n                                    kitchen_qual_other  \n                                                    NA  \n                                fireplace_qu_Excellent  \n                                            -1.090e-02  \n                                     fireplace_qu_Fair  \n                                            -5.742e-03  \n                                     fireplace_qu_Good  \n                                             3.799e-03  \n                             fireplace_qu_No_Fireplace  \n                                            -2.870e-03  \n                                     fireplace_qu_Poor  \n                                            -8.559e-03  \n                                  fireplace_qu_Typical  \n                                                    NA  \n                                    garage_type_Attchd  \n                                             2.717e-02  \n                                   garage_type_Basment  \n                                             1.841e-02  \n                                   garage_type_BuiltIn  \n                                             2.514e-02  \n                                    garage_type_Detchd  \n                                             2.309e-02  \n                                 garage_type_No_Garage  \n                                             4.121e-04  \n                                     garage_type_other  \n                                                    NA  \n                                     garage_finish_Fin  \n                                             9.351e-04  \n                               garage_finish_No_Garage  \n                                            -5.710e-03  \n                                     garage_finish_RFn  \n                                            -2.171e-03  \n                                     garage_finish_Unf  \n                                                    NA  \n                                      garage_qual_Fair  \n                                            -2.731e-02  \n                                 garage_qual_No_Garage  \n                                                    NA  \n                                   garage_qual_Typical  \n                                            -1.877e-02  \n                                     garage_qual_other  \n                                                    NA  \n                                      garage_cond_Fair  \n                                            -2.552e-02  \n                                 garage_cond_No_Garage  \n                                                    NA  \n                                   garage_cond_Typical  \n                                            -1.678e-03  \n                                     garage_cond_other  \n                                                    NA  \n                               paved_drive_Dirt_Gravel  \n                                            -4.599e-03  \n                          paved_drive_Partial_Pavement  \n                                            -7.736e-03  \n                                     paved_drive_Paved  \n                                                    NA  \n                                    fence_Good_Privacy  \n                                            -4.646e-03  \n                                       fence_Good_Wood  \n                                            -1.104e-02  \n                                 fence_Minimum_Privacy  \n                                            -1.837e-03  \n                                        fence_No_Fence  \n                                            -3.494e-03  \n                                           fence_other  \n                                                    NA  \n                                         sale_type_COD  \n                                            -1.792e-02  \n                                         sale_type_New  \n                                             1.829e-02  \n                                         sale_type_WD.  \n                                            -1.673e-02  \n                                       sale_type_other  \n                                                    NA  \n                                sale_condition_Abnorml  \n                                            -4.424e-02  \n                                 sale_condition_Family  \n                                            -2.159e-02  \n                                 sale_condition_Normal  \n                                            -1.563e-03  \n                                sale_condition_Partial  \n                                            -2.112e-02  \n                                  sale_condition_other  \n                                                    NA  \n\n\nTo obtain the detailed results from our trained linear regression model in a data frame, we can use the tidy() and glance() functions directly on our trained parsnip model, ames_fit.\n\nThe tidy() function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values;\nThe glance() function returns performance metrics obtained on the training data;\nWe can also use the vip() function to plot the variable importance for each predictor in our model. The importance value is determined based on the F-statistics and estimate coefficents in our trained model object.\n\n\n# Data frame of estimated coefficients\ntidy(ames_fit)\n\n# A tibble: 224 × 5\n   term          estimate std.error statistic   p.value\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)    4.22      0.0965     43.8   8.49e-300\n 2 lot_frontage   0.00151   0.00121     1.24  2.14e-  1\n 3 lot_area       0.00563   0.00129     4.35  1.42e-  5\n 4 mas_vnr_area   0.00318   0.00168     1.89  5.90e-  2\n 5 bsmt_fin_sf_1  0.0291    0.0248      1.17  2.41e-  1\n 6 bsmt_fin_sf_2 -0.00244   0.00119    -2.05  4.03e-  2\n 7 bsmt_unf_sf   -0.00986   0.00209    -4.71  2.61e-  6\n 8 total_bsmt_sf  0.0305    0.00326     9.36  2.01e- 20\n 9 first_flr_sf   0.00466   0.00910     0.512 6.09e-  1\n10 second_flr_sf  0.00963   0.0100      0.960 3.37e-  1\n# ℹ 214 more rows\n\n# Performance metrics on training data\nglance(ames_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared  sigma statistic p.value    df logLik    AIC    BIC\n      <dbl>         <dbl>  <dbl>     <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n1     0.934         0.928 0.0479      164.       0   185  3884. -7395. -6318.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n# Plot variable importance\nvip(ames_fit)\n\n\n\n\n\n\n\nTo assess the accuracy of our trained linear regression model, we must use it to make predictions on new data. This is done with the predict() function from parnsip. This function takes two important arguments:\n\na trained parnsip model object;\nnew_data for which to generate predictions.\n\nLet’s check how the model performs on our test dataset. The code below uses the predict() function to generate a data frame with a single column, .pred, which contains the predicted Sale Price values on the ames_test data.\n\npredict(ames_fit, new_data = ames_test_baked)\n\n# A tibble: 586 × 1\n   .pred\n   <dbl>\n 1  5.03\n 2  5.19\n 3  5.39\n 4  5.13\n 5  5.23\n 6  4.99\n 7  4.99\n 8  4.98\n 9  5.14\n10  5.77\n# ℹ 576 more rows\n\n\nGenerally it’s best to combine the new data set and the predictions into a single data frame. We create a data frame with the predictions on the training data and then use bind_cols() to add the baked test data to the results.\n\names_test_results <- predict(ames_fit, new_data = ames_test_baked) %>% \n  bind_cols(ames_test_baked)\n\n# View results\names_test_results\n\n# A tibble: 586 × 225\n   .pred lot_frontage lot_area mas_vnr_area bsmt_fin_sf_1 bsmt_fin_sf_2\n   <dbl>        <dbl>    <dbl>        <dbl>         <dbl>         <dbl>\n 1  5.03        0.675    0.172      -0.569          0.819         0.552\n 2  5.19        0.705    0.488       0.0527        -1.42         -0.298\n 3  5.39        0.916    0.145       1.45          -0.523        -0.298\n 4  5.13        0.224   -0.207      -0.569         -0.970        -0.298\n 5  5.23       -0.949   -0.516      -0.569         -0.523        -0.298\n 6  4.99       -1.10    -1.01        2.33           0.819        -0.298\n 7  4.99       -1.10    -1.01        2.26           0.819        -0.298\n 8  4.98       -1.10    -1.01        1.62           1.27         -0.298\n 9  5.14       -1.01    -0.943      -0.569         -1.42         -0.298\n10  5.77        1.58     0.492       5.74          -0.523        -0.298\n# ℹ 576 more rows\n# ℹ 219 more variables: bsmt_unf_sf <dbl>, total_bsmt_sf <dbl>,\n#   first_flr_sf <dbl>, second_flr_sf <dbl>, gr_liv_area <dbl>,\n#   bsmt_full_bath <dbl>, bsmt_half_bath <dbl>, full_bath <dbl>,\n#   half_bath <dbl>, bedroom_abv_gr <dbl>, tot_rms_abv_grd <dbl>,\n#   fireplaces <dbl>, garage_cars <dbl>, garage_area <dbl>, wood_deck_sf <dbl>,\n#   open_porch_sf <dbl>, mo_sold <dbl>, year_sold <dbl>, longitude <dbl>, …\n\n\nNow we have the model results and the test data in a single data frame.\n\n\n\n\n\nRoot Mean Square Error (RMSE): difference between the predicted and observed values (loss of function);\nR-squared (rsq): squared correlation between the predicted and observed values.\n\n\nTo obtain the rmse and rsq values on our results, we can use the rmse() and rsq() functions. Both functions take the following arguments:\n\na data frame with columns that have the true values and predictions;\nthe column with the true response values;\nthe column with predicted values.\n\nIn the examples below we pass our ames_test_results to these functions to obtain these values for our test set. Results are always returned as a data frame with the following columns: .metric, .estimator, and .estimate.\n\n#RMSE on test set\ntest_rmse <- rmse(ames_test_results, \n     truth = sale_price,\n     estimate = .pred)\n\ntest_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0656\n\n#rsq on test set\ntest_rsq<- rsq(ames_test_results,\n    truth = sale_price,\n    estimate = .pred)\n\ntest_rsq\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.858\n\n\nLet’s visualise the situation with an R2 plot:\n\names_test_results %>%\n  ggplot(aes(sale_price, .pred)) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linewidth = 0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.15, color = \"blue\") +\n  labs(\n    x = \"Actual Selling Price\",\n    y = \"Predicted Selling Price\",\n    color = \"Test/Training data\"\n  )\n\n\n\n\nThis is a plot that can be used for any regression model. It plots the actual values (Sale Prices) versus the model predictions (.pred) as a scatter plot. It also plot the line y = x through the origin. This line is a visually representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit. The reason this plot is called an R2 plot, is because the R2 is the squared correlation between the true and predicted values, which are plotted as paired in the plot.\n\n\n\nIn the previous section, we trained a linear regression model to the housing data step-by-step. In this section, we will go over how to combine all of the modeling steps into a single workflow.\nThe workflow package was designed to capture the entire modeling process and combine models and recipes into a single object. To create a workflow, we start with workflow() to create an empty workflow and then add out model and recipe with add_model() and add_recipe().\n\names_wf <- workflow() %>%\n  add_model(ames_model) %>% \n  add_recipe(ames_rec)\n\names_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_other()\n• step_nzv()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nWe can now train our model using fit(). Here, the training data ames_train are used for all estimation operations including the recipe that is part of the workflow:\n\names_fit_wf <- ames_wf %>%\n  fit(ames_train)\n\nNow let’s check how the model performs on our test dataset with predict():\n\names_results_wf <- predict(ames_fit_wf, new_data = ames_test) %>%\n  bind_cols(ames_test)\n\nNote no model or preprocessor parameters like those from recipes are re-estimated using the values in new_data. The new values at prediction time are standardized using the values from training when predict() is invoked.\nCollect RMSE and rsq metrics:\n\ntest_rmse_wf <- rmse(ames_results_wf, \n     truth = sale_price,\n     estimate = .pred)\n\ntest_rmse_wf\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard      0.0656\n\ntest_rsq_wf <- rsq(ames_results_wf,\n    truth = sale_price,\n    estimate = .pred)\n\ntest_rsq_wf\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.858\n\n\n\n\n\nYou just trained your model one time on the whole training set and then evaluated them on the testing set. Statisticians have come up with a slew of approaches to evaluate models in better ways than this; many important ones fall under the category of resampling.\nWe can resample the training set to produce an estimate of how the model will perform.You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice). One of these resampling methods is cross-validation.\n\n\nIf we only split the data once into a training and testing set, there is a risk that our model might be overfitting to the training data and perform poorly on new data. To overcome this, we can use a technique called cross-validation, which involves splitting the data into multiple subsets, or “folds”, and training and testing the model on each fold.\nIn k-fold cross-validation, we split the data into k equally sized folds. We then train the model on k-1 folds and test it on the remaining fold, repeating this process k times, so that each fold is used as the testing set once. We then average the performance of the model across all k folds to get an estimate of its generalization performance.\nBy using cross-validation, we can get a more accurate estimate of how well our model will perform on new, unseen data, and we can avoid overfitting to the training data.\n\n\n\n\n\n\nChallenge 6\n\n\n\nWhen you implement 10-fold cross-validation repeated 5 times, you:\n\nrandomly divide your training data into 50 subsets and train on 49 at a time (assessing on the other subset), iterating through all 50 subsets for assessment.\nrandomly divide your training data into 10 subsets and train on 9 at a time (assessing on the other subset), iterating through all 10 subsets for assessment. Then you repeat that process 5 times.\nrandomly divide your training data into 5 subsets and train on 4 at a time (assessing on the other subset), iterating through all 5 subsets. Then you repeat that process 10 times.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSimulations and practical experience show that 10-fold cross-validation repeated 5 times is a great resampling approach for many situations. This approach involves randomly dividing your training data into 10 folds, or subsets or groups, and training on only 9 while using the other fold for assessment. You iterate through all 10 folds being used for assessment; this is one round of cross-validation. You can then repeat the whole process multiple, perhaps 5, times.\n\n\n\n\nset.seed(9)\n\names_folds <- vfold_cv(ames_train, v=10, repeats = 5, strata = sale_price)\n\nglimpse(ames_folds)\n\nRows: 50\nColumns: 3\n$ splits <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2103 x …\n$ id     <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1…\n$ id2    <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06\", \"Fo…\n\n\nOnce we have created a set of resamples, we can use the function fit_resamples() to:\n\ntrain and evaluate the model on each fold;\nget the performance metrics for each fold;\nget the average performance across all the folds.\n\n\nset.seed(234)\names_res <- ames_wf %>%\n  fit_resamples(\n    ames_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nglimpse(ames_res)\n\nRows: 50\nColumns: 6\n$ splits       <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2…\n$ id           <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"R…\n$ id2          <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     <list> [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>],…\n$ .notes       <list> [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>],…\n$ .predictions <list> [<tbl_df[236 x 4]>], [<tbl_df[236 x 4]>], [<tbl_df[236 x…\n\nqsave(ames_res, \"../_models/ames_res.qs\")\n\n\nLinear regression detects some redundancies in the predictor set. We can ignore the warnings since lm() can deal with it.\n\nThe column .metric contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n# access the performance metrics for each fold, the average performance metric, and other information such as the predictions for each fold \names_res %>% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0540    50 0.00129 Preprocessor1_Model1\n2 rsq     standard   0.909     50 0.00368 Preprocessor1_Model1\n\n\nWe can see that the regression relationship is very strong: 90.8% of the variability in the selling price can be explained by the predictors and, on average, each element in the predicted selling price differs from the actual selling price by 0.05.\nWe can reliably measure performance using only the training data.\nIf we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance.\n\names_res %>%\n  collect_predictions() %>%\n  ggplot(aes(.pred, sale_price, color = id)) + \n  geom_abline(intercept = 0, slope = 1, color = 'black', linewidth=0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.15) +\n   labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')\n\n\n\n\n\n\n\n\nLet’s use the last_fit() function to evaluate once on the testing set:\n\n#Final fit on test dataset\names_final <- ames_wf %>%\n  last_fit(ames_split)\n\n# Obtain performance metrics on test data\ncollect_metrics(ames_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0656 Preprocessor1_Model1\n2 rsq     standard      0.858  Preprocessor1_Model1\n\n\nThe R2 and RMSE metrics are similar for both the training and testing datasets in our linear regression model. This is a good sign that the model is not over-fitting and can be used for making predictions on new data.\nWe can save the test set predictions by using the collect_predictions() function. This function returns a data frame which will have the response variables values from the test set and a column named .pred with the model predictions.\n\n# Obtain test set predictions data frame\names_results_final <- ames_final %>% \n                 collect_predictions()\n# View results\names_results_final\n\n# A tibble: 586 × 5\n   id               .pred  .row sale_price .config             \n   <chr>            <dbl> <int>      <dbl> <chr>               \n 1 train/test split  5.03     2       5.02 Preprocessor1_Model1\n 2 train/test split  5.19     3       5.24 Preprocessor1_Model1\n 3 train/test split  5.39    18       5.60 Preprocessor1_Model1\n 4 train/test split  5.13    26       5.15 Preprocessor1_Model1\n 5 train/test split  5.23    29       5.26 Preprocessor1_Model1\n 6 train/test split  4.99    30       4.98 Preprocessor1_Model1\n 7 train/test split  4.99    31       5.02 Preprocessor1_Model1\n 8 train/test split  4.98    32       4.94 Preprocessor1_Model1\n 9 train/test split  5.14    34       5.18 Preprocessor1_Model1\n10 train/test split  5.77    47       5.70 Preprocessor1_Model1\n# ℹ 576 more rows\n\n\nFinally, let’s use this data frame to make an R2 plot to visualize our model performance on the test data set:\n\nggplot(data = ames_results_final,\n       mapping = aes(x = .pred, y = sale_price)) +\n  geom_point(color = '#006EA1', alpha = 0.25) +\n  geom_abline(intercept = 0, slope = 1, color = 'black', linewidth=0.5, linetype=\"dotted\") +\n  labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')\n\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nThe workflows package enables a handy type of object that can bundle pre-processing and models together;\nYou don’t have to keep track of separate objects in your workspace;\nThe recipe prepping and model fitting can be executed using a single call to fit() instead of prep()-juice()-fit();\nThe recipe baking and model predictions are handled with a single call to predict() instead of bake()-predict();\nWorkflows are be able to evaluate different recipes and models at once (as we will see in Day 2 of this workshop);\nvfold_cv() creates folds for cross-validation;\nfit_resamples() fits models to resamples;\ncollect_metrics() obtains performance metrics from the results.\n\n\n\n\nAdapted from “Linear Regression and tidymodels”, available here.\nMax Kuhn and Julia Silge, “Tidy Modeling with R”, Version 1.0.0(2022-12-20)."
  },
  {
    "objectID": "200_classification/step1.html",
    "href": "200_classification/step1.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nUse tidyverse functions for exploratory data analysis;\nIntroduce and explore the Pima Indians Diabetes dataset;\nImpute missing data.\n\n\n\n\n\n\nToday, we are going to be working with Pima Indian Women’s diabetes dataset which contains information on 768 Pima Indian women’s diabetes status, as well as many predictive features:\n\npregnant - Number of times pregnant\nglucose - Plasma glucose concentration a 2 hours in an oral glucose tolerance test\npressure - Diastolic blood pressure (mm Hg)\ntriceps - Triceps skin fold thickness (mm) - a measure correlated with body fat\ninsulin - 2-Hour serum insulin (mu U/ml)\nmass - Body mass index (weight in kg/(height in m)^2)\nage - Age (years)\ndiabetes - diabetes status (pos - diabetic; neg - non-diabetic)\npedigree - diabetes pedigree function\n\nThe diabetes pedigree function was developed by Smith 1988 to provide a synthesis ofthe diabetes mellitus history in relatives and the genetic relationship of those relatives to the subject. It uses information from parents, grandparents, siblings, aunts and uncles, and first cousin to provide a measure of the expected genetic influence of affected and unaffected relatives on the subject’s eventual diabetes risk.\nThe Pima Indians are a group of Native Americans living in an area consisting of what is now central and southern Arizona. The short name, “Pima” is believed to have come from a phrase meaning “I don’t know,” which they used repeatedly in their initial meetings with Spanish colonists. Thanks Wikipedia!\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggcorrplot)\nlibrary(GGally)\nlibrary(qs)\nlibrary(mlbench)\nlibrary(skimr)\ntheme_set(theme_minimal())\n\nLoad data:\n\n# load the Pima Indians dataset from the mlbench dataset\ndata(PimaIndiansDiabetes)\n# rename dataset to have shorter name because lazy\ndiabetes_data <- PimaIndiansDiabetes\n# look at the variable names\nnames(diabetes_data)\n\n[1] \"pregnant\" \"glucose\"  \"pressure\" \"triceps\"  \"insulin\"  \"mass\"     \"pedigree\"\n[8] \"age\"      \"diabetes\"\n\n# look at the data\nglimpse(diabetes_data)\n\nRows: 768\nColumns: 9\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, 1…\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0,…\n$ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, 0…\n$ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 230…\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37…\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.158…\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n\n\nLook for missing data:\n\nanyNA(diabetes_data)\n\n[1] FALSE\n\n\nIt seems like there is no missing data.\nGet a summary of the data frame:\n\nsummary(diabetes_data)\n\n    pregnant         glucose         pressure         triceps     \n Min.   : 0.000   Min.   :  0.0   Min.   :  0.00   Min.   : 0.00  \n 1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 62.00   1st Qu.: 0.00  \n Median : 3.000   Median :117.0   Median : 72.00   Median :23.00  \n Mean   : 3.845   Mean   :120.9   Mean   : 69.11   Mean   :20.54  \n 3rd Qu.: 6.000   3rd Qu.:140.2   3rd Qu.: 80.00   3rd Qu.:32.00  \n Max.   :17.000   Max.   :199.0   Max.   :122.00   Max.   :99.00  \n    insulin           mass          pedigree           age        diabetes \n Min.   :  0.0   Min.   : 0.00   Min.   :0.0780   Min.   :21.00   neg:500  \n 1st Qu.:  0.0   1st Qu.:27.30   1st Qu.:0.2437   1st Qu.:24.00   pos:268  \n Median : 30.5   Median :32.00   Median :0.3725   Median :29.00            \n Mean   : 79.8   Mean   :31.99   Mean   :0.4719   Mean   :33.24            \n 3rd Qu.:127.2   3rd Qu.:36.60   3rd Qu.:0.6262   3rd Qu.:41.00            \n Max.   :846.0   Max.   :67.10   Max.   :2.4200   Max.   :81.00            \n\n\n\n\n\n\n\n\nExercise:\n\n\n\nLook at the output of summary above and the table that explains what each of the variables are. Do the values make sense for all of: - (a) Pregnancies and Glucose - (b) Blood pressure and Skin thickness - (c) Insulin and DiabetesPedigreeFunction, and - (d) BMI and Age\nIf not, how do you think we should deal with them? Can you hypothesise what the consequences of this approach would be?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Possibly missing: \n\ncolSums(diabetes_data == 0)\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n     111        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n#Not missing:\n\ncolSums(diabetes_data != 0)\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n     657      763      733      541      394      757      768      768 \ndiabetes \n     768 \n\n\nIt is clear that the values of several variables are zero when it is impossible for them to be so (i.e. this value could not be zero if it was measured). Hence, we are dealing with “hidden” missing data, and should recode it as NA.\nThe following variables have zero “values” that are actually likely to be missing:\n\nGlucose (a)\nBloodPressure (b)\nSkinThickness (b)\nInsulin (c)\nBMI (d)\n\n\n\n\n\n\n\n\nggplot(diabetes_data, aes(x = pregnant, fill = diabetes)) + geom_bar(position = \"dodge\")\n\n\n\n\n\nggplot(\n  diabetes_data,\n  aes(\n    x = pressure,\n    y = glucose,\n    color = diabetes\n  )\n) + geom_point(alpha = 0.5)\n\n\n\n\nIf we wanted to look at all possible scatterplot pairs we would do something like:\n\n# make a pair plot\nggpairs(data = diabetes_data, \n        mapping = aes(color = diabetes),\n        upper = list(combo = \"box\"))\n\n\n\n\nBut it’s easier to look at a correlation plot:\n\n# get a correlation matrix of the variables in the diabetes dataset:\ndiabetes_corr <- diabetes_data %>%\n  # recode outcome to be numeric (subtract 1 to return it to zero/one)\n  mutate(diabetes = as.integer(diabetes) - 1) %>%\n  cor()\n\nggcorrplot(diabetes_corr, type = \"lower\", lab = TRUE )\n\n\n\n\nLet’s create a new dataframe d_na, which has the missing values recoded as NA:\n\nd_na <- diabetes_data %>%\n  mutate(glucose = na_if(glucose, 0)) %>%\n  mutate(triceps = na_if(triceps, 0)) %>%\n  mutate(insulin = na_if(insulin, 0)) %>%\n  mutate(mass = na_if(mass, 0)) %>%\n  mutate(pressure = na_if(pressure, 0))\n\n# approximately half of the dataset is complete, whereas half is missing data\ntable(complete.cases(d_na))\n\n\nFALSE  TRUE \n  376   392 \n\nnaniar::gg_miss_var(d_na)\n\n\n\nvisdat::vis_dat(d_na)\n\n\n\n\nLet’s compare the correlation plot from before with another one now that we’ve correctly labelled the missing data:\n\ndiabetes_corr_na <-\n  d_na %>%\n  # recode outcome to be numeric (subtract 1 to return it to zero/one)\n  mutate(diabetes = as.integer(diabetes) - 1) %>%\n  # use pairwise complete observations for the two variables\n  cor(use = \"pairwise.complete.obs\")\n\nggcorrplot(diabetes_corr_na,  type = \"lower\",lab = TRUE)\n\n\n\n\nNotice that the correlation between some variables (eg. pregnant - insulin) changes quite substantially. (Negative before to Positive now).\n\n\n\n\nWe’re going to split our data into 70% training and 30% testing sets.\n\nset.seed(42) # so we all get the same results\n\ndiabetes_split <- initial_split(d_na , prop = 0.7, strata = \"diabetes\" )\nd_na_train <- training(diabetes_split)\nd_na_test <- testing(diabetes_split)\n\nqsave(d_na_train, \"../_models/d_na_train.qs\")\nqsave(d_na_test, \"../_models/d_na_test.qs\")\nqsave(diabetes_split, \"../_models/diabetes_split.qs\")\n\n\n\nLook how many examples we have in the training and testing sets.\n\ndim(d_na_train)\n\n[1] 537   9\n\ndim(d_na_test)\n\n[1] 231   9\n\n\nPlot histograms of outputs to check we stratified appropriately\n\ntogether <- bind_rows(train = d_na_train,\n                      test = d_na_test,\n                      .id = \"test_train\" ) \n\ntogether %>%\n  ggplot(aes(x = diabetes))+\n  geom_bar()+\n  facet_grid(test_train~., scales = \"free\")\n\n\n\ntogether %>%\n  {ggduo(., \n         setdiff( names(.), c(\"test_train\", \"diabetes\") ), \n         # column names not including test_train or the outcome\n         \"test_train\")} # faceted by test_train split\n\n\n\n\nAt some point we’re going to want to do some parameter tuning (explained later), and to do that we’re going to want to use cross-validation. So we can create a cross-validated version of the training set in preparation for that moment:\n\ndiabetes_folds <- vfold_cv(d_na_train, v=10, repeats = 5, strata = diabetes)\n\nqsave(diabetes_folds, \"../_models/diabetes_folds.qs\")\n\n\n\n\nImputation is often used to handle missing data because many statistical methods and machine learning algorithms require complete data. When we do imputation, we aren’t adding new information to our dataset, but we are using the patterns in our dataset so that we don’t have to throw away the data that have some variables missing. We can impute the missing data using a recipe:\n\n# set seed to be 42 so everyone gets the same results\nset.seed(42)\n\ndiabetes_rec <- recipe(diabetes ~ ., data = d_na_train) %>%\n                step_impute_median(all_predictors()) %>%\n                # all our predictors are numeric so standardize them\n                step_normalize(all_numeric_predictors())\n  \n\ndiabetes_rec  \n\nqsave(diabetes_rec, \"../_models/diabetes_rec.qs\")\n\n\n\n\n\n\n\nKey Points\n\n\n\n\nClassification attempts to predict the class to which a particular observation belongs;\nThere are many different metrics for assessing performance for a classification problem;\nWhich metric you choose and optimise for should be considered carefully, and will be different depending on the problem;\nExporatory data analysis is a time consuming but critical process that needs to be carried out prior to any modeling."
  },
  {
    "objectID": "200_classification/step2.html",
    "href": "200_classification/step2.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Learning objective:\n\n\n\n\nBuild a ML model for predicting whether a person has diabetes or not;"
  },
  {
    "objectID": "200_classification/step2.html#what-is-a-classifier",
    "href": "200_classification/step2.html#what-is-a-classifier",
    "title": "Sydney Informatics Hub",
    "section": "What is a classifier?",
    "text": "What is a classifier?\nA classifier is some kind of rule / black box / widget that you can feed a new example and it will spit out whether or not it is part of a given class. E.g. below, we are classifying the animals to be either cat or not cat.\n\n\n\nA classifier for cats and not cats.\n\n\nYou can have classifiers for anything you can have a yes/no answer to, e.g.\n\nIs this a cat? 🐱\nDo these test results indicate cancer? 🚑\nIs this email spam or not spam? 📧\n\nYou can also have classifiers that categorise things into multiple (more than two) categories e.g.\n\nWhich animal is this, out of the 12 animals I have trained my model on? 🐱\nDo these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? 🚑\nIs this email important, not important but not spam, or spam? 📧\n\nIt is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it’s better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off."
  },
  {
    "objectID": "200_classification/step2.html#model-evaluation-classification",
    "href": "200_classification/step2.html#model-evaluation-classification",
    "title": "Sydney Informatics Hub",
    "section": "Model evaluation (classification)",
    "text": "Model evaluation (classification)\nFor now, let’s imagine we have a classifier already. How can we test it to see how good it is? A good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.\n\n\n\nDemonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.\n\n\n\nConfusion Matrix\nWhen applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares “the truth” to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative. However, if we label something which is not a cat as a cat, this is called a false positive; and if we correctly label something which is not a cat, as not a cat, then this is a true negative. In our case, the confusion matrix will look like this:\n\ntrue positive (TP) : Diabetic correctly identified as diabetic\ntrue negative (TN) : Healthy correctly identified as healthy\nfalse positive (FP) : Healthy incorrectly identified as diabetic\nfalse negative (FN) : Diabetic incorrectly identified as healthy\n\n\n\nSome common classification metrics\nDon’t worry if you forget some of these - there are so many different words used to describe different ways to divide up the confusion matrix, it can get very confusing. I swear each time I just look up wikipedia again to figure out which part of the confusion matrix to look at. There are even more there that we won’t even bother talking about here.\n\nAccuracy:\nHow often does the classifier label examples correctly?\n\\[\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correctly labelled examples}}{\\text{All examples}}\\]\n\n\nPrecision:\nWhat fraction of things labelled as a cat were actually cats?\n\\[\\frac{TP}{TP+FP} = \\frac{\\text{Correctly labelled cats}}{\\text{All things labelled as cats}}\\]\n\n\nSensitivity / Recall:\nHow often does the classifier label a cat as a cat?\n\\[\\frac{TP}{TP+FN} = \\frac{\\text{Correctly labelled cats}}{\\text{All true cats}}\\]\n\n\nSpecificity:\nHow often does it label a not-cat as a not-cat?\n\\[\\frac{TN}{TN+FP} = \\frac{\\text{Correctly labelled not-cats}}{\\text{All true not-cats}}\\]\n\n\nF1-score:\nThis is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;\n\\[\\frac{1}{F_1} = \\frac{1}{2}\\left(\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Sensitivity}}\\right)\\]\n\n\n\nAUC: Area under the curve\nA good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.\nTo capture this balance, we often use a Reciever Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.\n\n\n\nA Reciever Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.\n\n\n\nFor additional discussion of classification error metrics, see Tharwat 2018, for example.\n\n\n\n\n\n\n\nChallenge 7\n\n\n\n\nIn the case of patients with a rare disease, what can be the problem of using accuracy to evaluate the performance of a machine learning model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAccuracy is calculated as the (TP + TN)/(total) number of cases in the dataset. If you have very few positive cases, such as when working with a rare disease, the numerator of this fraction will be dominated by the true negatives you accurately predict in your dataset - so not very informative when assessing whether your classifier predicts the disease well at all!\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(tune)\nlibrary(vip)\nlibrary(ParallelLogger)\nlibrary(doParallel)\nlibrary(workflowsets)\nlibrary(qs)\nlibrary(ranger)\nlibrary(glmnet)\ntheme_set(theme_minimal())\n\ndiabetes_rec <- qread(\"../_models/diabetes_rec.qs\")\ndiabetes_folds <- qread(\"../_models/diabetes_folds.qs\")\nd_na_train <- qread(\"../_models/d_na_train.qs\")\nd_na_test <- qread(\"../_models/d_na_test.qs\")\ndiabetes_split <- qread(\"../_models/diabetes_split.qs\")"
  },
  {
    "objectID": "200_classification/step2.html#some-classification-models",
    "href": "200_classification/step2.html#some-classification-models",
    "title": "Sydney Informatics Hub",
    "section": "Some classification models",
    "text": "Some classification models\n\nTree-based models\nA tree-based model is a type of algorithm that creates a tree-like structure to make predictions about a certain outcome, such as whether a customer will buy a product or not. The tree structure consists of nodes that represent different features, and the algorithm uses these features to split the data into smaller and smaller subsets. Each subset is then assigned a label based on the majority of its observations, and this process continues until the algorithm reaches a stopping criterion or has created a fully-grown tree. Once the tree is created, it can be used to make predictions by following the path through the tree that corresponds to a given set of input features. Tree-based models are simple and intuitive to understand, and can be used for both classification and regression tasks.\n\nDecision trees are a simple type of tree-based model that use a hierarchical structure of nodes to make predictions about a certain outcome. The process continues until a stopping criterion is met, such as a maximum tree depth or a minimum number of observations per leaf node, and it can predict the outcome. A single decision tree may not be accurate enough for many real-world problems; \nRandom forest overcomes this limitation by building many decision trees, each using a randomly selected subset of the data and features, and then combining their predictions to make a final prediction.\n\n\n\nLogistic regression\nLogistic regression is a type of regression where the range of mapping is confined to [0,1], unlike simple linear regression models where the domain and range could take any real value. Logistic regression is a type of algorithm that is used to predict a binary outcome, such as whether a patient is likely to develop diabetes or no. It works by creating a mathematical function that predicts the probability of an observation belonging to a certain class (e.g., diabetes or not diabetes). The function takes into account one or more input variables, such as the patients’s age, gender, or body mass index. The output of the function is a value between 0 and 1, which represents the probability of the observation belonging to the positive class (e.g., developing diabetes). To make a prediction, the algorithm compares the predicted probability to a threshold value (e.g., 0.5), and assigns the observation to the positive class if the probability is greater than the threshold, and to the negative class otherwise. The scatter plot of this data looks something like this:  We see that the data points are in the two extreme clusters. For our prediction modeling, a naive regression line in this scenario will give a nonsense fit (red line on the right plot) and what we actually require to fit is a line (blue on the right plot) to explain (or to correctly separate) a maximum number of data points. Logistic regression is a scheme to search this most optimum blue line.\nRegularization is a technique that can be used to prevent overfitting of the model. A regularized logistic regression model, is a logistic classifier that has been modified to include a regularization term. This is done by adding a penalty to the model that discourages it from giving too much importance to any variable.\nThere are several regularized regression models, defined with the mixture parameter:\n\nRidge regularization encourages the model to have small coefficient values (mixture = 0);\nLasso regularization encourages the model to set some of the coefficients to zero, which performs feature selection. This can help improve the model’s interpretability and reduce the impact of irrelevant features on the model’s performance (mixture = 1);\nElastic Net regularization combines Ridge and Lasso regularization by adding a penalty term that is a weighted average of both penalties. This approach can provide the benefits of both Ridge and Lasso regularization, such as feature selection and coefficient shrinkage (mixture between 0 and 1)."
  },
  {
    "objectID": "200_classification/step2.html#tune-model-hyperparameters",
    "href": "200_classification/step2.html#tune-model-hyperparameters",
    "title": "Sydney Informatics Hub",
    "section": "Tune model hyperparameters",
    "text": "Tune model hyperparameters\nSome model parameters cannot be learned directly from a dataset during model training; these kinds of parameters are called hyperparameters. Some examples of hyperparameters include the number of randomly selected variables to be considered at each split in a tree-based model (called mtry in tidymodels).\nInstead of learning these kinds of hyperparameters during model training, we can estimate the best values for these parameters by training many models on a resampled data set (like the cross-validation folds we have previously created) and measuring how well all these models perform. This process is called tuning.\n\n\n\n\n\n\nChallenge 8:\n\n\n\nAre these tuning hyperparameters?\n\nThe random seed;\nRegularization strength in a linear regression model;\nThreshold for minimum number of samples required to split an internal node in a decision tree.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n2 and 3 are parameters that directly affect the performance of a machine learning model during the training process.\n\n\n\nYou can identify which parameters to tune() in a model specification.\nWe can specify a random forest classifier with the following hyperparameters:\n\nmtry: the number of predictors that will be randomly sampled at each split when creating the tree models;\ntrees: the number of decision trees to fit and ultimately average;\nmin_n: The minimum number of data points in a node that are required for the node to be split further.\n\nTo specify a random forest model with tidymodels, we need the rand_forest() function. The hyperparameters of the model are arguments within the rand_forest() function and may be set to specific values. However, if tuning is required, then each of these parameters must be set to tune().\nWe will be using the ranger engine. This engine has an optional importance argument which can be used to track variable importance measures. In order to make a variable importance plot with vip(), we must add importance = 'impurity' inside our set_engine() function:\n\nrf_model_diabetes <- \n  # specify that the model is a random forest and which hyperparameters need to be tuned\n  rand_forest(mtry = tune(),\n              trees = tune(),\n              min_n = tune()) %>%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %>% #get variable importance scores\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\nrlr_model_diabetes <- \n  logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\")\n\n\nNote Nothing about this model specification is specific to the diabetes dataset.\n\n\nFind which parameters will give the model its best accuracy\n\n\nTry different values and measure their performance;\nFind good values for these parameters;\nOnce the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n\n\nYou have a couple of options for how to choose which possible values for the tuning parameters to try. One of these options is creating a random grid of values. Random grid search is implemented with the grid_random() function in tidymodels, taking a sequence of hyperparameter names to create the grid. It also has a size parameter that specifies the number of random combinations to create.\nThe mtry() hyperparameter requires a pre-set range of values to test since it cannot exceed the number of columns in our data. When we add this to grid_random() we can pass mtry() into the range_set() function and set a range for the hyperparameter with a numeric vector.\nIn the code below, we set the range from 3 to 6. This is because we have 9 columns in diabetes_data and we would like to test mtry() values somewhere in the middle between 1 and 9, trying to avoid values close to the ends.\nWhen using grid_random(), it is suggested to use set.seed() for reproducibility.\nWe can then use the function tune_grid() to tune either a workflow or a model specification with a set of resampled data, such as the cross-validation we created. Grid search, combined with resampling, requires fitting a lot of models! These models don’t depend on one another and can be run in parallel.\n\nset.seed(314)\n\nrf_grid <- grid_random(mtry() %>% range_set(c(3, 6)),\n                       trees(),\n                       min_n(),\n                       size = 10)\n\n#View grid\nrf_grid\n\n# A tibble: 10 × 3\n    mtry trees min_n\n   <int> <int> <int>\n 1     4  1644    34\n 2     6  1440    20\n 3     6  1549    31\n 4     5  1734    39\n 5     6   332    11\n 6     5  1064     3\n 7     5   218    37\n 8     4  1304    24\n 9     4   477    32\n10     5  1621     6\n\n#Tune random forest model \nrf_tune_model <- tune_grid(\n  rf_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rf_grid)\n\nrf_tune_model\n\n# Tuning results\n# 10-fold cross-validation repeated 5 times using stratification \n# A tibble: 50 × 5\n   splits           id      id2    .metrics          .notes          \n   <list>           <chr>   <chr>  <list>            <list>          \n 1 <split [483/54]> Repeat1 Fold01 <tibble [20 × 7]> <tibble [0 × 3]>\n 2 <split [483/54]> Repeat1 Fold02 <tibble [20 × 7]> <tibble [0 × 3]>\n 3 <split [483/54]> Repeat1 Fold03 <tibble [20 × 7]> <tibble [0 × 3]>\n 4 <split [483/54]> Repeat1 Fold04 <tibble [20 × 7]> <tibble [0 × 3]>\n 5 <split [483/54]> Repeat1 Fold05 <tibble [20 × 7]> <tibble [0 × 3]>\n 6 <split [483/54]> Repeat1 Fold06 <tibble [20 × 7]> <tibble [0 × 3]>\n 7 <split [483/54]> Repeat1 Fold07 <tibble [20 × 7]> <tibble [0 × 3]>\n 8 <split [484/53]> Repeat1 Fold08 <tibble [20 × 7]> <tibble [0 × 3]>\n 9 <split [484/53]> Repeat1 Fold09 <tibble [20 × 7]> <tibble [0 × 3]>\n10 <split [484/53]> Repeat1 Fold10 <tibble [20 × 7]> <tibble [0 × 3]>\n# ℹ 40 more rows\n\n\nUse collect_metrics to extract the metrics calculated from the cross-validation performance across the different values of the parameters:\n\n#collect metrics\nrf_tune_model %>%\n  collect_metrics()\n\n# A tibble: 20 × 9\n    mtry trees min_n .metric  .estimator  mean     n std_err .config            \n   <int> <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \n 1     4  1644    34 accuracy binary     0.764    50 0.00646 Preprocessor1_Mode…\n 2     4  1644    34 roc_auc  binary     0.848    50 0.00641 Preprocessor1_Mode…\n 3     6  1440    20 accuracy binary     0.763    50 0.00629 Preprocessor1_Mode…\n 4     6  1440    20 roc_auc  binary     0.842    50 0.00650 Preprocessor1_Mode…\n 5     6  1549    31 accuracy binary     0.759    50 0.00626 Preprocessor1_Mode…\n 6     6  1549    31 roc_auc  binary     0.843    50 0.00633 Preprocessor1_Mode…\n 7     5  1734    39 accuracy binary     0.758    50 0.00651 Preprocessor1_Mode…\n 8     5  1734    39 roc_auc  binary     0.846    50 0.00646 Preprocessor1_Mode…\n 9     6   332    11 accuracy binary     0.770    50 0.00587 Preprocessor1_Mode…\n10     6   332    11 roc_auc  binary     0.841    50 0.00665 Preprocessor1_Mode…\n11     5  1064     3 accuracy binary     0.770    50 0.00633 Preprocessor1_Mode…\n12     5  1064     3 roc_auc  binary     0.841    50 0.00644 Preprocessor1_Mode…\n13     5   218    37 accuracy binary     0.761    50 0.00678 Preprocessor1_Mode…\n14     5   218    37 roc_auc  binary     0.844    50 0.00651 Preprocessor1_Mode…\n15     4  1304    24 accuracy binary     0.764    50 0.00633 Preprocessor1_Mode…\n16     4  1304    24 roc_auc  binary     0.846    50 0.00638 Preprocessor1_Mode…\n17     4   477    32 accuracy binary     0.760    50 0.00644 Preprocessor1_Mode…\n18     4   477    32 roc_auc  binary     0.846    50 0.00632 Preprocessor1_Mode…\n19     5  1621     6 accuracy binary     0.767    50 0.00604 Preprocessor1_Mode…\n20     5  1621     6 roc_auc  binary     0.841    50 0.00645 Preprocessor1_Mode…\n\n#see which model performed the best, in terms of some given metric\nrf_tune_model %>%\n  show_best(\"roc_auc\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4  1644    34 roc_auc binary     0.848    50 0.00641 Preprocessor1_Model01\n2     4   477    32 roc_auc binary     0.846    50 0.00632 Preprocessor1_Model09\n3     4  1304    24 roc_auc binary     0.846    50 0.00638 Preprocessor1_Model08\n4     5  1734    39 roc_auc binary     0.846    50 0.00646 Preprocessor1_Model04\n5     5   218    37 roc_auc binary     0.844    50 0.00651 Preprocessor1_Model07\n\n\n\n\n\n\n\n\nChallenge 9\n\n\n\nUse tune_grid and collect_metrics to tune a workflow. Hints:\nUse workflow() to define the workflow:\n\n#set the workflow\n\n#add the recipe\n\n#add the model\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#set the workflow\nrf_workflow <- workflow() %>%\n#add the recipe\nadd_recipe(diabetes_rec) %>%\n#add the model\n  add_model(rf_model_diabetes)\n\n#tune the workflow\nset.seed(314)\n\nrf_tune_wf <- rf_workflow %>%\n  tune_grid(resamples = diabetes_folds,\n            grid = rf_grid)\n\nrf_tune_wf %>%\n  collect_metrics()\n\n# A tibble: 20 × 9\n    mtry trees min_n .metric  .estimator  mean     n std_err .config            \n   <int> <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \n 1     4  1644    34 accuracy binary     0.762    50 0.00589 Preprocessor1_Mode…\n 2     4  1644    34 roc_auc  binary     0.847    50 0.00627 Preprocessor1_Mode…\n 3     6  1440    20 accuracy binary     0.759    50 0.00641 Preprocessor1_Mode…\n 4     6  1440    20 roc_auc  binary     0.842    50 0.00644 Preprocessor1_Mode…\n 5     6  1549    31 accuracy binary     0.762    50 0.00644 Preprocessor1_Mode…\n 6     6  1549    31 roc_auc  binary     0.843    50 0.00661 Preprocessor1_Mode…\n 7     5  1734    39 accuracy binary     0.761    50 0.00659 Preprocessor1_Mode…\n 8     5  1734    39 roc_auc  binary     0.846    50 0.00636 Preprocessor1_Mode…\n 9     6   332    11 accuracy binary     0.764    50 0.00595 Preprocessor1_Mode…\n10     6   332    11 roc_auc  binary     0.840    50 0.00656 Preprocessor1_Mode…\n11     5  1064     3 accuracy binary     0.766    50 0.00616 Preprocessor1_Mode…\n12     5  1064     3 roc_auc  binary     0.841    50 0.00654 Preprocessor1_Mode…\n13     5   218    37 accuracy binary     0.759    50 0.00690 Preprocessor1_Mode…\n14     5   218    37 roc_auc  binary     0.845    50 0.00674 Preprocessor1_Mode…\n15     4  1304    24 accuracy binary     0.762    50 0.00655 Preprocessor1_Mode…\n16     4  1304    24 roc_auc  binary     0.846    50 0.00634 Preprocessor1_Mode…\n17     4   477    32 accuracy binary     0.764    50 0.00687 Preprocessor1_Mode…\n18     4   477    32 roc_auc  binary     0.846    50 0.00637 Preprocessor1_Mode…\n19     5  1621     6 accuracy binary     0.764    50 0.00615 Preprocessor1_Mode…\n20     5  1621     6 roc_auc  binary     0.841    50 0.00639 Preprocessor1_Mode…\n\nrf_tune_wf %>%\n  show_best(\"roc_auc\")\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4  1644    34 roc_auc binary     0.847    50 0.00627 Preprocessor1_Model01\n2     4   477    32 roc_auc binary     0.846    50 0.00637 Preprocessor1_Model09\n3     4  1304    24 roc_auc binary     0.846    50 0.00634 Preprocessor1_Model08\n4     5  1734    39 roc_auc binary     0.846    50 0.00636 Preprocessor1_Model04\n5     5   218    37 roc_auc binary     0.845    50 0.00674 Preprocessor1_Model07\n\n\n\n\n\nLet’s visualise our results:\n\nautoplot(rf_tune_model)\n\n\n\nautoplot(rf_tune_wf)\n\n\n\n\nWe can also specify the values of the parameters to tune with an tuning grid, entered as a data frame. It contains all the combinations of parameters to be tested. For regularized logistic regression, we test eleven values of mixture:\n\n#set the grid\nrlr_grid <- data.frame(mixture = seq(0, 1, 0.1),\n                       penalty = seq(0, 1, 0.1))\nrlr_grid\n\n   mixture penalty\n1      0.0     0.0\n2      0.1     0.1\n3      0.2     0.2\n4      0.3     0.3\n5      0.4     0.4\n6      0.5     0.5\n7      0.6     0.6\n8      0.7     0.7\n9      0.8     0.8\n10     0.9     0.9\n11     1.0     1.0\n\nset.seed(435)\n\n##use tune_grid() for hyperparameters tuning, doing cross validation for each row of the tuning grid\nrlr_tune_model <- tune_grid(\n  rlr_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rlr_grid)\n\nrlr_tune_model %>%\n  collect_metrics()\n\n# A tibble: 22 × 8\n   penalty mixture .metric  .estimator  mean     n  std_err .config             \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>               \n 1     0       0   accuracy binary     0.747    50 0.00649  Preprocessor1_Model…\n 2     0       0   roc_auc  binary     0.838    50 0.00623  Preprocessor1_Model…\n 3     0.1     0.1 accuracy binary     0.756    50 0.00596  Preprocessor1_Model…\n 4     0.1     0.1 roc_auc  binary     0.839    50 0.00620  Preprocessor1_Model…\n 5     0.2     0.2 accuracy binary     0.751    50 0.00602  Preprocessor1_Model…\n 6     0.2     0.2 roc_auc  binary     0.837    50 0.00603  Preprocessor1_Model…\n 7     0.3     0.3 accuracy binary     0.681    50 0.00402  Preprocessor1_Model…\n 8     0.3     0.3 roc_auc  binary     0.805    50 0.00690  Preprocessor1_Model…\n 9     0.4     0.4 accuracy binary     0.652    50 0.000801 Preprocessor1_Model…\n10     0.4     0.4 roc_auc  binary     0.783    50 0.00742  Preprocessor1_Model…\n# ℹ 12 more rows\n\nrlr_tune_model %>%\n  show_best(\"roc_auc\")\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config              \n    <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.1     0.1 roc_auc binary     0.839    50 0.00620 Preprocessor1_Model02\n2     0       0   roc_auc binary     0.838    50 0.00623 Preprocessor1_Model01\n3     0.2     0.2 roc_auc binary     0.837    50 0.00603 Preprocessor1_Model03\n4     0.3     0.3 roc_auc binary     0.805    50 0.00690 Preprocessor1_Model04\n5     0.4     0.4 roc_auc binary     0.783    50 0.00742 Preprocessor1_Model05\n\n\n\n\nThe workflowsets package\nTidymodels allows us to perform all of the above steps in a much faster way with the workflowsets package:\n\ndiabetes_wf_set <- workflow_set(list(diabetes_rec),  #list of recipes\n             list(rf_model_diabetes, rlr_model_diabetes), #list of models\n             cross = TRUE) #all combinations of the preprocessors and models are used to create the workflows\n  \ndiabetes_wf_set$option\n\n[[1]]\nan empty container for options\n\n[[2]]\nan empty container for options\n\ndiabetes_wf_set <- diabetes_wf_set %>%\n  option_add(grid=rf_grid, id=\"recipe_rand_forest\") %>%\n  option_add(grid=rlr_grid, id=\"recipe_logistic_reg\")\n\ndiabetes_wf_set$option\n\n[[1]]\na list of options with names:  'grid'\n\n[[2]]\na list of options with names:  'grid'\n\ndiabetes_wf_set <- diabetes_wf_set %>%\n  workflow_map(\"tune_grid\", # the first argument is a function name from the tune package (tune_grid(), fit_resamples()..)\n               resamples = diabetes_folds,\n               verbose = TRUE) \n\n\ndiabetes_wf_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id            info             option    result   \n  <chr>               <list>           <list>    <list>   \n1 recipe_rand_forest  <tibble [1 × 4]> <opts[2]> <tune[+]>\n2 recipe_logistic_reg <tibble [1 × 4]> <opts[2]> <tune[+]>\n\n\nThe results column contains the results of each call to tune_grid() for the workflows. From these results, we can get quick assessments of how well these models classified the data:\n\n#To get the rankings of the models (and their tuning parameter sub-models) as a data frame:\nrank_results(diabetes_wf_set, rank_metric = \"roc_auc\")\n\n# A tibble: 42 × 9\n   wflow_id         .config .metric  mean std_err     n preprocessor model  rank\n   <chr>            <chr>   <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n 1 recipe_rand_for… Prepro… accura… 0.763 0.00658    50 recipe       rand…     1\n 2 recipe_rand_for… Prepro… roc_auc 0.847 0.00637    50 recipe       rand…     1\n 3 recipe_rand_for… Prepro… accura… 0.761 0.00638    50 recipe       rand…     2\n 4 recipe_rand_for… Prepro… roc_auc 0.846 0.00643    50 recipe       rand…     2\n 5 recipe_rand_for… Prepro… accura… 0.762 0.00647    50 recipe       rand…     3\n 6 recipe_rand_for… Prepro… roc_auc 0.846 0.00639    50 recipe       rand…     3\n 7 recipe_rand_for… Prepro… accura… 0.760 0.00687    50 recipe       rand…     4\n 8 recipe_rand_for… Prepro… roc_auc 0.846 0.00643    50 recipe       rand…     4\n 9 recipe_rand_for… Prepro… accura… 0.762 0.00696    50 recipe       rand…     5\n10 recipe_rand_for… Prepro… roc_auc 0.845 0.00640    50 recipe       rand…     5\n# ℹ 32 more rows\n\n#plot the results\nautoplot(diabetes_wf_set, metric = \"roc_auc\")\n\n\n\n\nThis shows the results for all tuning parameter combinations for each model. It looks like the random forest model did well. We can use the extract_workflow_set_result() function to extract the tuning results:\n\nbest_results <- diabetes_wf_set %>%\n  extract_workflow_set_result(\"recipe_rand_forest\") %>%\n  select_best(metric=\"roc_auc\")\n\nbest_results\n\n# A tibble: 1 × 4\n   mtry trees min_n .config              \n  <int> <int> <int> <chr>                \n1     4  1644    34 Preprocessor1_Model01\n\n\n\n\nUpdate and fit the workflow\nThe last step in hyperparameter tuning is to use finalize_workflow() to add our optimal model to our workflow object, and apply the last_fit() function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set:\n\nfinal_diabetes_fit <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results) %>%\n  last_fit(diabetes_split)\n\nfinal_diabetes_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [537/231]> train/test split <tibble> <tibble> <tibble>     <workflow>\n\n\nSince we supplied the train/test object when we fit the workflow, the metrics are evaluated on the test set. Now when we use the collect_metrics() function (the same we used when tuning our parameters) to extract the performance of the final model (since rf_fit_final now consists of a single final model) applied to the test set:\n\ntest_performance <- final_diabetes_fit %>% collect_metrics()\ntest_performance\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.762 Preprocessor1_Model1\n2 roc_auc  binary         0.816 Preprocessor1_Model1\n\n\nWe can plot the ROC curve to visualize test set performance of our random forest model, and generate a confusion matrix:\nNote In R, factor levels are ordered alphabetically by default, which means that “no” comes first before “yes” and is considered the level of interest or positive case. Use the argument event_level = \"second\" to alter this as needed.\n\n#ROC curve\n  collect_predictions(final_diabetes_fit) %>%\n  roc_curve(truth  = diabetes, event_level=\"second\", estimate = .pred_pos) %>%  #specify which level of truth to consider as the \"event\"\n                autoplot()\n\n\n\n#confusion matrix\nconf_matrix_rf <- final_diabetes_fit %>%\n  collect_predictions() %>%\n  conf_mat(truth = diabetes, estimate = .pred_class) \n\nconf_matrix_rf\n\n          Truth\nPrediction neg pos\n       neg 128  33\n       pos  22  48\n\nconf_matrix_rf %>%\n  autoplot()\n\n\n\n\n\n\nVariable importance\nIn order to visualize the variable importance scores of our random forest model, we will need to manually train our workflow object with the fit() function on the training data, then extract the trained model with the pull_workflow_fit() function, and next passing the trained model to the vip() function:\n\n#extract the final workflow\nfinal_workflow <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results)\n\n#fit on the training data\nwf_fit <- final_workflow %>%\n  fit(data = d_na_train)\n#extract the trained model\nwf_fit <- wf_fit %>% \n          pull_workflow_fit()\n#plot variable importance\nvip(wf_fit)\n\n\n\n\nThis returns a ggplot object with the variable importance scores from our model.\nWe see from the results below, that the glucose concentration, body mass index and age are the most important predictors of diabetes.\n\n\n\n\n\n\nKey Points\n\n\n\n\nA workflow is a combination of a model and preprocessors (e.g, a formula, recipe, etc.);\nIn order to try different combinations of these, the workflow_set() function creates an object that contains many workflows;\nThe workflow_map() executes the function from the tune package (e.g, tune_grid(), fit_resamples()) across all the workflows in the set.\n\n\n\n\nAdapted from “Decision Trees and Random Forests”, available here.\nAdapted from “Machine Learning with tidymodels” workshop, licensed CC Y-SA 4.0. Available here."
  },
  {
    "objectID": "Tidymodels_tips_tricks.html",
    "href": "Tidymodels_tips_tricks.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "The sale price data are already log-transformed in the ames data frame. Why not use step_log() in our recipe:\nstep_log(Sale_Price, base = 10)\nThis will cause a failure when the recipe is applied to new properties with an unknown sale price. Since price is what we are trying to predict, there probably won’t be a column in the data for this variable. In fact, to avoid information leakage, many tidymodels packages isolate the data being used when making any predictions. This means that the training set and any outcome columns are not available for use at prediction time.\nFor simple transformations of the outcome column(s), we strongly suggest that those operations be conducted outside of the recipe.\nHowever, there are other circumstances where this is not an adequate solution. When using a recipe, we thus need a mechanism to ensure that some operations are applied only to the data that are given to the model. Each step function has an option called skip that, when set to TRUE, will be ignored by the predict() function. In this way, you can isolate the steps that affect the modeling data without causing errors when applied to new samples. However, all steps are applied when using fit()."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand what Machine Learning (ML) is;\nUnderstand which modelling approaches to use for different kinds of data;\nUnderstand how tidymodels can help build and evaluate ML models."
  },
  {
    "objectID": "Introduction.html#what-is-machine-learning",
    "href": "Introduction.html#what-is-machine-learning",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "What is Machine Learning",
    "text": "What is Machine Learning\nMachine Learning (ML) is the field of study that gives computers the ability to learn from data without being explicitly programmed\n\nAn ML system is trained rather than explicitly programmed;\nPart of the broader field of Artificial Intelligence\nAlso known as predictive modelling or statistical learning."
  },
  {
    "objectID": "Introduction.html#learning-by-experience",
    "href": "Introduction.html#learning-by-experience",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Learning by experience",
    "text": "Learning by experience\nWe are never certain something will happen, but we usually know (or can estimate rather well) how likely it is to happen or, at least, what is most likely to happen, based on the experience we have acquired throughout our life.\nImagine you decide to play the violin. The first time you pick up a violin, you probably will struggle to make sounds that are pleasing, let alone to play anything. However, each successive time you practice your violin, you will get better and better. Eventually you will be able to produce good sounds, and with sufficient training you can play anything!\nIn machine learning, we use this same approach to train our models to solve problems."
  },
  {
    "objectID": "Introduction.html#where-can-our-models-get-experience",
    "href": "Introduction.html#where-can-our-models-get-experience",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Where can our models get experience?",
    "text": "Where can our models get experience?\nTo make make a model that performs well at solving a problem, we need to give it data that will be similar to what it will encounter when trying to solve the problem.\nSpecifically, we want a dataset with a target variable that we want to predict, and predictor variables that we will use to predict the target variable. Here’s two different examples:\n\n\n\n\n\n\n\n\nProblem\nTarget Variable\nPredictor Variable\n\n\n\n\nPredicting house prices\nhouse sale price records\nnumber of bedrooms\n\n\nClassifying penguin species\npenguin species\nbill length"
  },
  {
    "objectID": "Introduction.html#what-do-we-mean-by-learning",
    "href": "Introduction.html#what-do-we-mean-by-learning",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "What do we mean by learning?",
    "text": "What do we mean by learning?\nLearning is our means of attaining the ability to perform automatically a task.\n\nThe main goal of the ML process is to find an algorithm \\(f(x)\\) that most accurately predicts future values \\(y\\) (or outcome) based on a set of inputs \\(X\\) (or predictors), where each entry \\(x^{i}\\) is a different feature:\n\n\nFeatures of an image are usually the values of the pixels in the image;\nWe want to predict the price of an house on the basis of some characteristics (n° rooms, garden, position, floor, etc..) that are the features of each house predictors from which we learn."
  },
  {
    "objectID": "Introduction.html#supervised-ml",
    "href": "Introduction.html#supervised-ml",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Supervised ML",
    "text": "Supervised ML\nThe dataset contains a series of inputs, based on which we are trying to predict a predifined outcome, which we know for the original data in the dataset. The outcome can be numerical (in which case the problem is called regression) or categorical (classification).\n\nRegression:\nThink of regression as predicting numbers (or divide the ties by length). You are asking to predict a numerical value given some input. For example:\n\nGiven the house features we want to predict the price. It is a pure numerical and unbounded value (at least positive unbounded);\nTemperature of a city considering several factors (pulling, humidity, season,etc..);\nTo solve this task, the learning algorithm is asked to produce a function so that the model takes an example x as input and after some processing \\(f(x)\\) it returns a value y that can be any real value. \n\n\n\nClassification:\nThink of classification as predicting a category (or divide the socks by color). For example:\n\nGiven a sentence (maybe a tweet) the system should determines if it express a positive or negative or neutral feeling;\nGiven an image where it can be a dog or a cat, we want to determine with the system which one is present;\nTo solve this task, the learning algorithm is usually asked to produce a function \\(y= f(x)\\)\nSo the model takes an example x as input and after some processing \\(f(x)\\) it returns a value y that is one of the categories the example x should belong to. \n\n\n\nData splitting and spending\nFor machine learning, we typically split data into training and test sets:\n\nThe training set is used to estimate model parameters;\nThe test set is used to find an independent assessment of model performance.\n\nOnce we have built a ML model we might want to measure its performance, for example the accuracy in classification task (proportion of correct predicted examples) or the average error in a regression task. Our goal is to build a model that is able to really understand the task and that is able to generalize. For this reason we need to separate our dataset into training and testing sets.  The relative proportions of the training and testing set depend on the total of number of observations, and the variability observed in the data. The trade off to be considered is:\n\nif there is too much data in the training set, the assessment of the prediction error will be carried out on a very small test set, therefore we might find a model that fits the existing data very well, but generalizes very poorly;\nif there is too much data in the testing set, this means that we might not have enough data in the training set to accurately estimate the model parameters - so the model won’t be very accurate.\n\nSome commonly used cutoffs include:\n\n60% training / 40% testing\n70% training / 30% testing\n80% training / 20% testing"
  },
  {
    "objectID": "Introduction.html#exploratory-data-analysis-eda-with-the-tidyverse",
    "href": "Introduction.html#exploratory-data-analysis-eda-with-the-tidyverse",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Exploratory Data Analysis (EDA) with the Tidyverse",
    "text": "Exploratory Data Analysis (EDA) with the Tidyverse\nA hugely important part of any modeling approach is exploratory data analysis. In this course, we’ll be using tidyverse packages for getting to know your data, manipulating it, and visualizing it. The tidyverse is a collection of R packages designed for data science that share common APIs and an underlying philosophy. When you type library(tidyverse), what you’re doing is loading this collection of related packages for handling data using tidy data principles. These packages include ggplot2 for data visualization, and dplyr and tidyr for data manipulation and transformation. During this course, we’ll point out when we use functions from these different packages.\nVisit this page to learn more about tidyverse: https://www.tidyverse.org/learn/"
  },
  {
    "objectID": "Introduction.html#machine-learning-ml-with-tidymodels",
    "href": "Introduction.html#machine-learning-ml-with-tidymodels",
    "title": "Introduction to Machine Learning with R and tidymodels",
    "section": "Machine Learning (ML) with Tidymodels",
    "text": "Machine Learning (ML) with Tidymodels\nTidymodels is a collection of R packages that provides a modern and consistent approach to building and validating machine learning models. Compared to older packages such as caret, tidymodels offers several benefits:\n\nit uses a consistent and intuitive syntax across all of its packages, which makes it easier to learn and use compared to the varied and sometimes complex syntax of older packages;\nit is built on top of the tidyverse (dplyr, ggplot2), for a seamless data analysis workflow;\nit includes a number of packages (recipes, rsample) that provide tools for data preprocessing, making it easy to perform common data preprocessing tasks, such as feature engineering, and to integrate these tasks into the machine learning workflow;\nit includes packages (parsnip, tune) that provide a more flexible and modern approach to model tuning and selection. These packages allow for easy cross-validation, hyperparameter tuning, and model selection, and provide a more transparent and reproducible workflow;\nit is actively developed and has a growing community of users and contributors. This means that there are many resources available for learning and troubleshooting, and that the packages are likely to continue to evolve and improve over time.\n\nOverall, tidymodels offers a more modern and consistent approach to building and validating machine learning models, and provides a number of tools for data preprocessing, model tuning and selection, and workflow integration. These features make it a powerful and user-friendly tool."
  },
  {
    "objectID": "00_setup.html",
    "href": "00_setup.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Question\n\n\n\n\nWhat packages do I need to install in order to attend the SIH’s Machine Learning with R course?"
  },
  {
    "objectID": "00_setup.html#installation-instructions",
    "href": "00_setup.html#installation-instructions",
    "title": "Sydney Informatics Hub",
    "section": "Installation instructions",
    "text": "Installation instructions\n\nif (getRversion() < 3.6){\n  stop(\"You need R >= 3.6 to attend the workshop. Please update R!\")\n}\n\nlist_of_pkgs <- c(\n  \"AmesHousing\",\n  \"tidyverse\",\n  \"tidymodels\",\n  \"doParallel\",\n  \"vip\",\n  \"ggcorrplot\",\n  \"naniar\",\n  \"parallelly\",\n  \"GGally\",\n  \"mlbench\",\n  \"usemodels\",\n  \"qs\",\n  \"ranger\",\n  \"skimr\",\n  \"bestNormalize\",\n  \"plotly\"\n  )\n\nau_repo <- \"https://mirror.aarnet.edu.au/pub/CRAN/\"\n\n#install installr and rtools if windows\nif (.Platform$OS.type == \"windows\"){\n  install.packages(c(\"installr\"), repos = au_repo)\n  installr::install.Rtools(choose_version = F,\n                           check = T,\n                           GUI = T)\n  win_only <- c(\"installr\")\n} else {\n  win_only <- c()\n}\n\n# run the following line of code to install the packages you currently do not have\nnew_pkgs <- list_of_pkgs[!(list_of_pkgs %in% installed.packages()[,\"Package\"])]\nif(length(new_pkgs)) install.packages(new_pkgs,repos = au_repo)\n\n# install helper function to plot metrics\nlist_of_pkgs <- c(list_of_pkgs,\n                  win_only)\n\n#check everything is installed and write out an error message if it ain't.\nmissing_pkg <- list_of_pkgs[!(list_of_pkgs %in% installed.packages()[,\"Package\"])]\nif(length(missing_pkg)) {\n  print(\"=======================================================\")\n  print(\"Packages which didn't install properly:\")\n  print(missing_pkg)\n  print(\"Try to install them again or ask a helper for assistance.\")\n  print(\"=======================================================\")\n} else {\n  print(\"=======================================================\")\n  print(\"All packages installed properly! :)\")\n  print(\"=======================================================\")\n}\n\n[1] \"=======================================================\"\n[1] \"All packages installed properly! :)\"\n[1] \"=======================================================\"\n\n\n\n\n\n\n\n\nTo do before the workshop begins\n\n\n\n\nPlease join the workshop with a computer that has the following installed (all available for free):\n\nA recent version of R, available at https://cran.r-project.org/;\nA recent version of RStudio Desktop (RStudio Desktop Open Source License, at least v2022.02), available at https://www.rstudio.com/download;\n\nPlease copy and paste the code above in your Rstudio, run it and if there are any errors please let us know via email (sih.training@sydney.edu.au) BEFORE the workshop begins"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning with R and the tidyverse",
    "section": "",
    "text": "This is the landing page for the Sydney Informatics Hub’s “Machine learning with R and the tidyverse”."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Machine learning with R and the tidyverse",
    "section": "Schedule",
    "text": "Schedule\nThis course is designed to be delivered either as 4 half day sessions in person.\nYou are welcome to enrol in whichever session you are interested in, but if you complete all four sessions you will be eligible to receive a certificate of completion.\n\n\n\nSession 1: Introduction to Machine Learning\n\n\n\nPart 1\nIntroduction to machine learning\n\n\nPart 2\nConcepts and ideas in practice\n\n\nSession 2: Exploratory Data Analysis (EDA)\n\n\n\nPart 1\nA guided intro to EDA\n\n\nPart 2\nChoose your own EDA adventure\n\n\nSession 3: Regression\n\n\n\nPart 1\nData preprocessing and model building\n\n\nPart 2\nModel tuning and evaluation\n\n\nSession 4: Classification\n\n\n\nPart 1\nData preprocessing and model building\n\n\nPart 2\nModel tuning and evaluation\n\n\n\n\n\n\n\n\n\nPrerequisites\n\n\n\n\nThis course assumes intermediate R knowledge. This workshop is for you if:\n\nYou can use the magrittr pipe %>%;\nYou are familiar with functions from dplyr, tidyr, and ggplot2;\nYou can read data into R, transform and reshape data, and make a wide variety of graphs.\n\nWe expect participants to have some exposure to basic statistical concepts, but NOT intermediate or expert familiarity with modeling or machine learning;\nYou need your own laptop with R and a few key packages installed. See setup instructions for more details."
  },
  {
    "objectID": "tidymodels.html",
    "href": "tidymodels.html",
    "title": "Tidymodels",
    "section": "",
    "text": "load packages\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n\n✔ broom        1.0.4     ✔ recipes      1.0.5\n✔ dials        1.1.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.1     ✔ tibble       3.2.1\n✔ ggplot2      3.4.1     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.2     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rsample)\nlibrary(vip) \n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(qs)\n\nqs 0.25.5\n\ntheme_set(theme_minimal())\n\n\names_data <- qread(\"_models/ames_dataset_filt.qs\")"
  },
  {
    "objectID": "Tidymodels_tips_tricks.html#step_mutate---step_select",
    "href": "Tidymodels_tips_tricks.html#step_mutate---step_select",
    "title": "Sydney Informatics Hub",
    "section": "step_mutate() - step_select()",
    "text": "step_mutate() - step_select()\nstep_mutate() creates a specification of a recipe step that will add variables using dplyr::mutate(). We did this step during EDA in Session 1 but we could have easily introduced it in our recipe ames_rec object in this way:\names_rec <- recipe(sale_price ~ ., data = ames_train) %>% \n            step_mutate(time_since_remodel = year_sold - year_remod_add, \n                        house_age = year_sold - year_built) %>%\n            step_select(-year_remod_add, -year_built)\nThe advantage of performing these preprocessing steps with the recipe package is that all the feature engineering you want to perform to your data can be put into a single object, you can save that object, you can carry it around. It’s not in a bunch of scripts. It’s been unit tested and it has a lot of features in it."
  },
  {
    "objectID": "Tidymodels_tips_tricks.html#update",
    "href": "Tidymodels_tips_tricks.html#update",
    "title": "Sydney Informatics Hub",
    "section": "update()",
    "text": "update()\nThis step method for update() updates steps within a recipe object.\nIn the example below, the [[2]] is used to access the 2 step in the recipe object’s steps list.\nThe update() function is then used to modify this step by changing the threshold used in the step_nzv() function to 0.05.\nFor a step to be updated, it must not already have been trained.\names_rec$steps[[2]] <- update(ames_rec$steps[[2]], threshold = 0.05)"
  },
  {
    "objectID": "Tidymodels_tips_tricks.html#use_",
    "href": "Tidymodels_tips_tricks.html#use_",
    "title": "Sydney Informatics Hub",
    "section": "use_*()",
    "text": "use_*()\nThe usemodels package is a helpful way of quickly creating code snippets to fit models using the tidymodels framework. Given a simple formula and a data set, the use_* functions can create code that appropriate for the data (given the model). The package includes these templates:\n\nlibrary(usemodels)\nls(\"package:usemodels\", pattern = \"use_\")\n\n[1] \"use_C5.0\"             \"use_cubist\"           \"use_earth\"           \n[4] \"use_glmnet\"           \"use_kernlab_svm_poly\" \"use_kernlab_svm_rbf\" \n[7] \"use_kknn\"             \"use_ranger\"           \"use_xgboost\"         \n\n\n\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\nuse_ranger(diabetes ~ ., data = PimaIndiansDiabetes)\n\nranger_recipe <- \n  recipe(formula = diabetes ~ ., data = PimaIndiansDiabetes) \n\nranger_spec <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") \n\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec) \n\nset.seed(58662)\nranger_tune <-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))"
  },
  {
    "objectID": "Tidymodels_tips_tricks.html#tune_race_anova",
    "href": "Tidymodels_tips_tricks.html#tune_race_anova",
    "title": "Sydney Informatics Hub",
    "section": "tune_race_anova()",
    "text": "tune_race_anova()\nThe problem with grid search is that you don’t know if some of those choices you made about the candidate parameters are any good until you’re done with all the computations. The tune_race_anova() function from the finetune package is a dynamic way of doing grid search. What racing does is as you start to do the model tuning, it looks at the results as they happen and eliminates tuning parameter combinations that are unlikely to be the best results using a repeated measure ANOVA model:\ninstall.packages(\"finetune\")\nlibrary(finetune)\n\nrf_tune_wf <- rf_workflow %>%\n              tune_race_anova(resamples = diabetes_folds,\n                              grid = rf_grid)"
  },
  {
    "objectID": "ML_concepts.html",
    "href": "ML_concepts.html",
    "title": "ML Concepts in Practice",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nGain familiarity with some of the main ML algorithms;\nUnderstand how to evaluate performance of ML models;\nUnderstand how to use ML models in practice."
  },
  {
    "objectID": "ML_concepts.html#machine-learning-algorithms",
    "href": "ML_concepts.html#machine-learning-algorithms",
    "title": "ML Concepts in Practice",
    "section": "Machine Learning Algorithms",
    "text": "Machine Learning Algorithms\nThe phrase “machine learning” brings to mind ideas of science fiction artificial intelligences, but machine learning algorithms don’t have to be foreign and uninterpretable. A good way to get started thinking about algorithms is with linear equations.\nMost of us have probably come across this equation before:\n\\[\ny = mx+b\n\\]\nThis is the slope-intercept form, and is an equation often used to teach how to create straight lines. Values of y are calculated based on values of x, modified by two other variables: the slope m of the line and the point where the line intercepts the y-axis b.\nWe can use similar equations to build powerful machine learning predictors.\n\nprint(\"meow test\")\n\n[1] \"meow test\""
  }
]