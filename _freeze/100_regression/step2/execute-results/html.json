{
  "hash": "15f50045f59334526f44a928cd537f7c",
  "result": {
    "markdown": "# Machine learning with Tidymodels\n\n:::{.callout-note}\n## Learning objective:\n\n- From base R to tidymodels;\n- Split our data into training and test sets;\n- Preprocess the training data;\n- Specify a linear regression model;\n- Train our model on the training data;\n- Transform the test data and obtain predictions using our trained model.\n:::\n\n:::{.callout-tip}\n## Exercise:\n\nIn this case study, you will predict houses selling price from characteristics of these houses, like size and layout of the living space in the house.\nWhat kind of model will you build?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\nTo predict a continuous, numeric quantity like selling price, use regression models.\n:::\n\nLoad in the packages we’ll be using for modelling:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='step2_cache/html/loadpackages_4993353beba17f07378e1e1d14e9159f'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(vip) \ntheme_set(theme_minimal())\n```\n:::\n\n::: {.cell hash='step2_cache/html/getData_44b190869a1c4ec61b8c67f9c28b3e7c'}\n\n:::\n\n\n## Build a simple linear regression model using base R\n\n---\n\nIn a linear model, we assume that there is a linear relationship between the input variable(s) and the output variable. This means that as the input variable(s) increase or decrease, the output variable changes in a straight line.\n\nImagine you have a scatter plot with your data points all over it. A linear model is like drawing a straight line through the scatter plot that best fits all the points. The slope and intercept of this line are chosen in such a way that the distance between the line and all the points is minimized. This line is then used to predict the output for new input values. \n\n![Example of a linear model](../fig/lm.png){width=500 fig-align=\"left\"}\n\nThe straight red dotted line represents the linear model equation $y=mx+c$, where $c$ is the y-intercept of the regression line, $m$ is the slope of the regression line, and $y$ is the expected value for y for the given $x$ value.\n\n\n::: {.cell hash='step2_cache/html/lm_95a9a672f35a2a0aaad702a6a0f71bc6'}\n\n```{.r .cell-code}\n#fit a linear model\names_lm <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_data)\n\n#Print the summary of the model\nsummary(ames_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94258 -0.06622  0.01359  0.07298  0.39246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.835e+00  7.406e-03  652.80   <2e-16 ***\nGr_Liv_Area 2.579e-04  4.714e-06   54.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.124 on 2923 degrees of freedom\nMultiple R-squared:  0.506,\tAdjusted R-squared:  0.5058 \nF-statistic:  2994 on 1 and 2923 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nR-squared value explains the variability of y with respect to x:\n\n- varies between 0 to 1 (0-100%);\n- R-squared values closer to 0 mean the regression relationship is very low;\n- R-squared values closer to 1 mean the regression relationship is very strong.\n\nLet's plot our linear regression model:\n\n::: {.cell hash='step2_cache/html/unnamed-chunk-2_608d60252503b7a5142f1a943a607eec'}\n\n```{.r .cell-code}\nplot(ames_data$Gr_Liv_Area, ames_data$Sale_Price,\n     xlab=\"Gr_Liv_Area\",\n     ylab=\"Sale_Price\", \n     col = \"blue\")\nabline(ames_lm, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## Build a linear regression model using Tidymodels\n\n---\n\nWhen you type `library(tidymodels)`, you load a collection of packages for modeling and machine learning using tidyverse principles. All the packages are designed to be consistent, modular, and to support good modeling practices. The first thing we are going to practice is splitting your data into a **training set** and a **testing set**. The tidymodels package `rsample` has functions that help you specify training and testing sets:\n\n\n::: {.cell hash='step2_cache/html/SplitTestTrain_48fa2dcde1dafcca421f20b5d91c752a'}\n\n```{.r .cell-code}\nset.seed(42) #so we all get the same results\names_split <- ames_data %>%\n    initial_split(prop = 0.8,\n                  strata = Sale_Price) #stratification\n\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\nsaveRDS(ames_train, \"../_models/ames_train.Rds\")\nsaveRDS(ames_test, \"../_models/ames_test.Rds\")\n```\n:::\n\nStratified sampling would split within each quartile:\n![strata](../fig/strata.png)\n\nThe code here takes an input data set and puts 80% of it into a training dataset and 20% of it into a testing dataset; it chooses the individual cases so that both sets are balanced in selling price.\n\nLet's check if the distribution of the selling price is the same in the testing and training datasets:\n\n\n::: {.cell hash='step2_cache/html/distr test and train_e166d48159dca15356b8a85f9754bfc5'}\n\n```{.r .cell-code}\names_train %>% \n  ggplot(aes(x = log(Sale_Price),  col = \"red\", fill = NULL)) + \n  geom_density() + theme_minimal() +\n  geom_line(data = ames_test,\n            stat = \"density\",\n            col = \"blue\") + theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/distr test and train-1.png){width=672}\n:::\n:::\n\n### Feature engineering\n\n---\n\nWe might want to modify our predictors columns for a few reasons:\n\n- The model requires them in a different format;\n- The model needs certain data qualities;\n- The outcome is better predicted when one or more columns are transformed in some way (a.k.a “feature engineering”).\n\n**In tidymodels, you can use the `recipes` package, an extensible framework for pipeable sequences of feature engineering steps that provide preprocessing tools to be applied to data.**\n\nSome of these steps can include:\n\n- Scaling and centering numeric predictors;\n- Removing skewness from numeric variables;\n- One-hot and dummy variable encoding for categorical variables;\n- Removing correlated predictors and zero variance variables;\n- Imputing missing data.\n\n**Statistical parameters for the steps can be estimated from an initial data set and then applied to other data sets.**\n\n**The resulting processed output can be used as inputs for statistical or machine learning models.**\n\n\n\n::: {.cell hash='step2_cache/html/recipe_bf93eefcfec85427ef6e1af411c3ee88'}\n\n```{.r .cell-code}\names_rec <-\n  recipe(Sale_Price ~ ., data = ames_train) %>% #assigns columns to roles of “outcome” or “predictor” using the formula\n  step_other(all_nominal(), threshold = 0.01) %>% #useful when you have some factor levels with very few observations, all_nominal selects both characters and factors, pools infrequently occurring values (frequency less than 0.01) into an \"other\" category\n  step_nzv(all_predictors()) %>% #remove predictors that are highly sparse and unbalanced\n  step_center(all_numeric_predictors()) %>% #subtracts the column mean from predictors\n  step_scale(all_numeric_predictors()) %>% #divides by the standard deviation\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% #for any nominal predictor, make binary indicators\n  step_lincomb(all_numeric_predictors()) #remove redundancies in the predictors, if present\n\names_rec\n```\n:::\n\nNote that each successive `step()` function adds a preprocessing step to our recipe object in the order that they are provided.\nThe preprocessing recipe `ames_rec` has been defined but no values have been estimated.\n\n:::{.callout-tip}\n### Challenge X\n\nDoes it make sense to apply these preprocessing steps to the test set?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\nNo, it doesn't. You want the set test to look like new data that your model will see in the future.\n:::\n\n### prep(), juice(), bake()\n\n---\n\n:::{.border}\n- The `prep()` function takes a recipe and computes everything so that the preprocessing steps can be executed. Note that this is done with the training data.\n:::\n\n\n::: {.cell hash='step2_cache/html/prep_e20e78ef0b6b57205900440562b6b972'}\n\n```{.r .cell-code}\names_prep <- prep(ames_rec)\n\names_prep\n```\n:::\n\n\nThe `bake()` and `juice()` functions both return data, not a preprocessing recipe object.\n\n:::{.border}\n- The `bake()` function takes a prepped recipe (one that has had all quantities estimated from training data) and applies it to `new_data`. That new_data could be the training data again or it could be the testing data (with the TRAINING parameters).\n:::\n\n\n::: {.cell hash='step2_cache/html/bake_60a30bb95daa13343950eeb096dedf9f'}\n\n```{.r .cell-code}\nbake(ames_prep, new_data = ames_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 586 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1        0.675    0.172 -0.569    0.819   0.552 -0.657  -0.388   -0.689  -0.786\n 2        0.705    0.488  0.0527  -1.42   -0.298 -0.347   0.670    0.456  -0.786\n 3        0.916    0.145  1.45    -0.523  -0.298 -0.336   1.92     1.85   -0.786\n 4        0.224   -0.207 -0.569   -0.970  -0.298 -0.632   0.0242  -0.266  -0.786\n 5       -0.949   -0.516 -0.569   -0.523  -0.298 -0.466   0.850    0.477  -0.786\n 6       -1.10    -1.01   2.33     0.819  -0.298 -0.527  -1.33    -1.78    0.392\n 7       -1.10    -1.01   2.26     0.819  -0.298 -0.759  -1.23    -1.67    0.539\n 8       -1.10    -1.01   1.62     1.27   -0.298 -0.0770 -1.23    -1.67    0.539\n 9       -1.01    -0.943 -0.569   -1.42   -0.298 -0.495  -0.452   -0.797   0.618\n10        1.58     0.492  5.74    -0.523  -0.298  2.08    4.26     4.07   -0.786\n# … with 576 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n```\n:::\n:::\n\n\n:::{.border}\n- The `juice()` function is a nice little shortcut. When we `juice()` the recipe, we squeeze that training data back out, transformed in the ways we specified. \n:::\n\nLet's compare the `bake()` and `juice()` outputs:\n\n::: {.cell hash='step2_cache/html/juice_461f37377bd1ef840e5384fdd5e2565e'}\n\n```{.r .cell-code}\nbake(ames_prep, new_data = ames_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n```\n:::\n\n```{.r .cell-code}\njuice(ames_prep) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 187\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n```\n:::\n:::\n\nNote that the `juice()` output is the same as `bake(ames_rep, new_data = ames_train)` and is just a shortcut that we are going to use later.\n\n### Build the model\n\n---\n\nIn tidymodels, you specify models using three concepts:\n\n- **type** differentiates models such as logistic regression, linear regression, and so forth;\n- **mode** includes common options like regression and classification, some model types support either of these while some only have one mode;\n- **engine** is the computational tool which will be used to fit the model. \n\nWe will specify the model using the `parsnip` package.\nMany functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values.\n\n\n::: {.cell hash='step2_cache/html/parnsip_c5daf5385065d6e62b73e1bc6696a42f'}\n\n```{.r .cell-code}\n#a linear regression model specification\names_model <- linear_reg() %>% #pick a model\n  set_engine(\"lm\")           #set the engine\n                             #set_mode(\"regression\") we don't need this as the model linear_reg() only does regression\n\n#view model properties\names_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\n### Fit the model \n\n---\n\nNow we are ready to train our model object on the training data. \nWe can do this using the `fit()` function from the parsnip package. \nThe `fit()` function takes the following arguments:\n  \n- a parnsip model object specification;\n- a model formula\n- a data frame with the training data\n\nThe code below trains our linear regression model on the prepped training data. In our formula, we have specified that Sale_Price is the response variable and included all the rest as our predictor variables.\n\n::: {.cell hash='step2_cache/html/fit_77bf82326a3910d9adfa792d1fc64559'}\n\n```{.r .cell-code}\names_fit <- ames_model %>%\n  fit(Sale_Price ~ .,\n      data=juice(ames_prep))\n\n# View lm_fit properties\names_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ ., data = data)\n\nCoefficients:\n                                          (Intercept)  \n                                            4.225e+00  \n                                         Lot_Frontage  \n                                            1.510e-03  \n                                             Lot_Area  \n                                            5.634e-03  \n                                         Mas_Vnr_Area  \n                                            3.178e-03  \n                                         BsmtFin_SF_1  \n                                            2.910e-02  \n                                         BsmtFin_SF_2  \n                                           -2.438e-03  \n                                          Bsmt_Unf_SF  \n                                           -9.865e-03  \n                                        Total_Bsmt_SF  \n                                            3.052e-02  \n                                         First_Flr_SF  \n                                            4.657e-03  \n                                        Second_Flr_SF  \n                                            9.635e-03  \n                                          Gr_Liv_Area  \n                                            4.895e-02  \n                                       Bsmt_Full_Bath  \n                                            4.946e-03  \n                                       Bsmt_Half_Bath  \n                                            1.160e-03  \n                                            Full_Bath  \n                                            7.528e-03  \n                                            Half_Bath  \n                                            6.568e-03  \n                                        Bedroom_AbvGr  \n                                           -3.780e-03  \n                                        TotRms_AbvGrd  \n                                           -4.477e-05  \n                                           Fireplaces  \n                                            5.691e-03  \n                                          Garage_Cars  \n                                            8.626e-03  \n                                          Garage_Area  \n                                            6.286e-03  \n                                         Wood_Deck_SF  \n                                            1.943e-03  \n                                        Open_Porch_SF  \n                                            1.901e-03  \n                                              Mo_Sold  \n                                           -3.097e-04  \n                                            Year_Sold  \n                                           -2.016e-03  \n                                            Longitude  \n                                           -8.325e-03  \n                                             Latitude  \n                                           -4.980e-03  \n                                   Time_Since_Remodel  \n                                           -5.556e-03  \n                                            House_Age  \n                                           -1.929e-02  \n      MS_SubClass_One_Story_1946_and_Newer_All_Styles  \n                                            6.196e-03  \n                 MS_SubClass_One_Story_1945_and_Older  \n                                           -2.177e-02  \n     MS_SubClass_One_and_Half_Story_Finished_All_Ages  \n                                            2.813e-02  \n                 MS_SubClass_Two_Story_1946_and_Newer  \n                                           -1.093e-02  \n                 MS_SubClass_Two_Story_1945_and_Older  \n                                            1.847e-02  \n                      MS_SubClass_Split_or_Multilevel  \n                                           -2.027e-02  \n                              MS_SubClass_Split_Foyer  \n                                            3.816e-03  \n               MS_SubClass_Duplex_All_Styles_and_Ages  \n                                           -2.177e-03  \n             MS_SubClass_One_Story_PUD_1946_and_Newer  \n                                            2.046e-02  \n             MS_SubClass_Two_Story_PUD_1946_and_Newer  \n                                           -2.533e-02  \nMS_SubClass_Two_Family_conversion_All_Styles_and_Ages  \n                                            2.991e-03  \n                                    MS_SubClass_other  \n                                                   NA  \n               MS_Zoning_Floating_Village_Residential  \n                                            3.984e-02  \n                    MS_Zoning_Residential_Low_Density  \n                                            3.224e-02  \n                 MS_Zoning_Residential_Medium_Density  \n                                            2.443e-02  \n                                    Lot_Shape_Regular  \n                                            4.462e-03  \n                         Lot_Shape_Slightly_Irregular  \n                                            4.294e-03  \n                       Lot_Shape_Moderately_Irregular  \n                                            1.805e-02  \n                                    Lot_Config_Corner  \n                                            5.998e-03  \n                                   Lot_Config_CulDSac  \n                                            1.298e-02  \n                                       Lot_Config_FR2  \n                                           -5.176e-03  \n                                    Lot_Config_Inside  \n                                            3.934e-03  \n                              Neighborhood_North_Ames  \n                                           -1.547e-02  \n                           Neighborhood_College_Creek  \n                                           -3.736e-02  \n                                Neighborhood_Old_Town  \n                                           -3.894e-02  \n                                 Neighborhood_Edwards  \n                                           -4.494e-02  \n                                Neighborhood_Somerset  \n                                            1.495e-02  \n                      Neighborhood_Northridge_Heights  \n                                            1.344e-02  \n                                 Neighborhood_Gilbert  \n                                           -8.690e-03  \n                                  Neighborhood_Sawyer  \n                                           -2.252e-02  \n                          Neighborhood_Northwest_Ames  \n                                           -1.568e-02  \n                             Neighborhood_Sawyer_West  \n                                           -3.405e-02  \n                                Neighborhood_Mitchell  \n                                           -2.449e-02  \n                               Neighborhood_Brookside  \n                                           -1.037e-02  \n                                Neighborhood_Crawford  \n                                            1.911e-02  \n                  Neighborhood_Iowa_DOT_and_Rail_Road  \n                                           -4.515e-02  \n                              Neighborhood_Timberland  \n                                           -2.225e-02  \n                              Neighborhood_Northridge  \n                                            1.195e-02  \n                             Neighborhood_Stone_Brook  \n                                            3.834e-02  \n Neighborhood_South_and_West_of_Iowa_State_University  \n                                           -3.175e-02  \n                             Neighborhood_Clear_Creek  \n                                           -1.395e-02  \n                          Neighborhood_Meadow_Village  \n                                           -6.325e-02  \n                                   Condition_1_Artery  \n                                           -1.792e-02  \n                                    Condition_1_Feedr  \n                                           -1.227e-02  \n                                     Condition_1_Norm  \n                                            6.575e-03  \n                                     Condition_1_PosN  \n                                            1.180e-02  \n                                     Condition_1_RRAn  \n                                           -1.178e-02  \n                                     Bldg_Type_OneFam  \n                                            3.635e-02  \n                                   Bldg_Type_TwoFmCon  \n                                            2.470e-02  \n                                      Bldg_Type_Twnhs  \n                                           -1.657e-02  \n                         House_Style_One_and_Half_Fin  \n                                           -3.279e-02  \n                                House_Style_One_Story  \n                                           -9.496e-03  \n                                   House_Style_SFoyer  \n                                            5.423e-04  \n                                     House_Style_SLvl  \n                                            1.420e-02  \n                                House_Style_Two_Story  \n                                           -8.472e-03  \n                                    Overall_Qual_Fair  \n                                           -9.709e-03  \n                           Overall_Qual_Below_Average  \n                                           -7.711e-03  \n                                 Overall_Qual_Average  \n                                            1.465e-02  \n                           Overall_Qual_Above_Average  \n                                            2.453e-02  \n                                    Overall_Qual_Good  \n                                            3.288e-02  \n                               Overall_Qual_Very_Good  \n                                            5.065e-02  \n                               Overall_Qual_Excellent  \n                                            5.328e-02  \n                                    Overall_Cond_Fair  \n                                            8.424e-02  \n                           Overall_Cond_Below_Average  \n                                            1.386e-01  \n                                 Overall_Cond_Average  \n                                            1.647e-01  \n                           Overall_Cond_Above_Average  \n                                            1.786e-01  \n                                    Overall_Cond_Good  \n                                            1.976e-01  \n                               Overall_Cond_Very_Good  \n                                            2.004e-01  \n                               Overall_Cond_Excellent  \n                                            2.172e-01  \n                                     Roof_Style_Gable  \n                                            3.064e-03  \n                                       Roof_Style_Hip  \n                                            1.400e-03  \n                                 Exterior_1st_AsbShng  \n                                           -2.269e-02  \n                                 Exterior_1st_BrkFace  \n                                            1.762e-02  \n                                 Exterior_1st_CemntBd  \n                                           -6.648e-02  \n                                 Exterior_1st_HdBoard  \n                                           -2.029e-02  \n                                 Exterior_1st_MetalSd  \n                                           -9.188e-03  \n                                 Exterior_1st_Plywood  \n                                           -1.778e-02  \n                                  Exterior_1st_Stucco  \n                                           -2.030e-02  \n                                 Exterior_1st_VinylSd  \n                                           -3.227e-02  \n                                 Exterior_1st_Wd.Sdng  \n                                           -1.530e-02  \n                                 Exterior_1st_WdShing  \n                                           -2.727e-02  \n                                 Exterior_2nd_AsbShng  \n                                           -2.730e-02  \n                                 Exterior_2nd_BrkFace  \n                                           -2.295e-02  \n                                 Exterior_2nd_CmentBd  \n                                            4.882e-02  \n                                 Exterior_2nd_HdBoard  \n                                           -5.905e-03  \n                                 Exterior_2nd_MetalSd  \n                                           -5.823e-03  \n                                 Exterior_2nd_Plywood  \n                                           -7.550e-03  \n                                  Exterior_2nd_Stucco  \n                                            7.010e-03  \n                                 Exterior_2nd_VinylSd  \n                                            9.093e-03  \n                                 Exterior_2nd_Wd.Sdng  \n                                           -3.418e-03  \n                                 Exterior_2nd_Wd.Shng  \n                                            3.114e-03  \n                                 Mas_Vnr_Type_BrkFace  \n                                            1.391e-02  \n                                    Mas_Vnr_Type_None  \n                                            1.470e-02  \n                                   Mas_Vnr_Type_Stone  \n                                            2.233e-02  \n                                 Exter_Qual_Excellent  \n                                            4.355e-02  \n                                      Exter_Qual_Fair  \n                                           -1.018e-02  \n                                      Exter_Qual_Good  \n                                            7.081e-03  \n                                      Exter_Cond_Fair  \n                                           -5.441e-02  \n                                      Exter_Cond_Good  \n                                           -2.790e-02  \n                                   Exter_Cond_Typical  \n                                           -2.038e-02  \n                                    Foundation_BrkTil  \n                                           -2.178e-02  \n                                    Foundation_CBlock  \n                                           -1.965e-02  \n                                     Foundation_PConc  \n                                           -1.085e-02  \n                                      Foundation_Slab  \n                                            6.116e-05  \n                                  Bsmt_Qual_Excellent  \n                                            3.459e-02  \n                                       Bsmt_Qual_Fair  \n                                            6.404e-03  \n                                       Bsmt_Qual_Good  \n                                            2.045e-02  \n                                Bsmt_Qual_No_Basement  \n                                            2.428e-02  \n                                    Bsmt_Qual_Typical  \n                                            1.874e-02  \n                                     Bsmt_Exposure_Av  \n                                            3.624e-03  \n                                     Bsmt_Exposure_Gd  \n                                            2.669e-02  \n                                     Bsmt_Exposure_Mn  \n                                            5.688e-05  \n                                     Bsmt_Exposure_No  \n                                           -4.150e-03  \n                                   BsmtFin_Type_1_ALQ  \n                                            8.386e-02  \n                                   BsmtFin_Type_1_BLQ  \n                                            6.682e-02  \n                                   BsmtFin_Type_1_GLQ  \n                                            5.941e-02  \n                                   BsmtFin_Type_1_LwQ  \n                                            3.318e-02  \n                                   BsmtFin_Type_1_Rec  \n                                            7.956e-03  \n                                 Heating_QC_Excellent  \n                                            7.722e-01  \n                                      Heating_QC_Fair  \n                                            7.472e-01  \n                                      Heating_QC_Good  \n                                            7.654e-01  \n                                   Heating_QC_Typical  \n                                            7.598e-01  \n                                        Central_Air_N  \n                                           -2.408e-02  \n                                     Electrical_FuseA  \n                                           -1.545e-02  \n                                     Electrical_FuseF  \n                                           -1.968e-02  \n                                     Electrical_SBrkr  \n                                           -1.951e-02  \n                               Kitchen_Qual_Excellent  \n                                            3.178e-02  \n                                    Kitchen_Qual_Fair  \n                                           -3.273e-03  \n                                    Kitchen_Qual_Good  \n                                            5.608e-03  \n                               Fireplace_Qu_Excellent  \n                                           -1.090e-02  \n                                    Fireplace_Qu_Fair  \n                                           -5.742e-03  \n                                    Fireplace_Qu_Good  \n                                            3.799e-03  \n                            Fireplace_Qu_No_Fireplace  \n                                           -2.870e-03  \n                                    Fireplace_Qu_Poor  \n                                           -8.559e-03  \n                                   Garage_Type_Attchd  \n                                            2.717e-02  \n                                  Garage_Type_Basment  \n                                            1.841e-02  \n                                  Garage_Type_BuiltIn  \n                                            2.514e-02  \n                                   Garage_Type_Detchd  \n                                            2.309e-02  \n                                Garage_Type_No_Garage  \n                                            4.121e-04  \n                                    Garage_Finish_Fin  \n                                            9.351e-04  \n                              Garage_Finish_No_Garage  \n                                           -5.710e-03  \n                                    Garage_Finish_RFn  \n                                           -2.171e-03  \n                                     Garage_Qual_Fair  \n                                           -2.731e-02  \n                                  Garage_Qual_Typical  \n                                           -1.877e-02  \n                                     Garage_Cond_Fair  \n                                           -2.552e-02  \n                                  Garage_Cond_Typical  \n                                           -1.678e-03  \n                              Paved_Drive_Dirt_Gravel  \n                                           -4.599e-03  \n                         Paved_Drive_Partial_Pavement  \n                                           -7.736e-03  \n                                   Fence_Good_Privacy  \n                                           -4.646e-03  \n                                      Fence_Good_Wood  \n                                           -1.104e-02  \n                                Fence_Minimum_Privacy  \n                                           -1.837e-03  \n                                       Fence_No_Fence  \n                                           -3.494e-03  \n                                        Sale_Type_COD  \n                                           -1.792e-02  \n                                        Sale_Type_New  \n                                            1.829e-02  \n                                        Sale_Type_WD.  \n                                           -1.673e-02  \n                               Sale_Condition_Abnorml  \n                                           -4.424e-02  \n                                Sale_Condition_Family  \n                                           -2.159e-02  \n                                Sale_Condition_Normal  \n                                           -1.563e-03  \n                               Sale_Condition_Partial  \n                                           -2.112e-02  \n```\n:::\n:::\n\nTo obtain the detailed results from our trained linear regression model in a data frame, we can use the `tidy()` and `glance()` functions directly on our trained parsnip model, ames_fit.\n\n- The `tidy()` function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values;\n- The `glance()` function returns performance metrics obtained on the training data;\n- We can also use the `vip()` function to plot the variable importance for each predictor in our model. The importance value is determined based on the F-statistics and estimate coefficents in our trained model object.\n\n\n::: {.cell hash='step2_cache/html/metrics_79940baf1433edd023985cec867af454'}\n\n```{.r .cell-code}\n# Data frame of estimated coefficients\ntidy(ames_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 187 × 5\n   term          estimate std.error statistic   p.value\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)    4.22      0.0965     43.8   8.49e-300\n 2 Lot_Frontage   0.00151   0.00121     1.24  2.14e-  1\n 3 Lot_Area       0.00563   0.00129     4.35  1.42e-  5\n 4 Mas_Vnr_Area   0.00318   0.00168     1.89  5.90e-  2\n 5 BsmtFin_SF_1   0.0291    0.0248      1.17  2.41e-  1\n 6 BsmtFin_SF_2  -0.00244   0.00119    -2.05  4.03e-  2\n 7 Bsmt_Unf_SF   -0.00986   0.00209    -4.71  2.61e-  6\n 8 Total_Bsmt_SF  0.0305    0.00326     9.36  2.01e- 20\n 9 First_Flr_SF   0.00466   0.00910     0.512 6.09e-  1\n10 Second_Flr_SF  0.00963   0.0100      0.960 3.37e-  1\n# … with 177 more rows\n```\n:::\n\n```{.r .cell-code}\n# Performance metrics on training data\nglance(ames_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹  sigma stati…² p.value    df logLik    AIC    BIC devia…³\n      <dbl>      <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.934      0.928 0.0479    164.       0   185  3884. -7395. -6318.    4.94\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n\n```{.r .cell-code}\n# Plot variable importance\nvip(ames_fit)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/metrics-1.png){width=672}\n:::\n:::\n\n\n### Evaluating the model\n\n---\n\nTo assess the accuracy of our trained linear regression model, we must use it to make predictions on new data. \nThis is done with the `predict()` function from parnsip. This function takes two important arguments:\n\n- a trained parnsip model object;\n- new_data for which to generate predictions.\n\nLet's first check how the model performs on our training dataset.\nThe code below uses the `predict()` function to generate a data frame with a single column, *.pred*, which contains the predicted Sale Price values on the ames_train data.\n\n::: {.cell hash='step2_cache/html/predict_d35017fe34303ae8e45163e1c27a62f8'}\n\n```{.r .cell-code}\npredict(ames_fit, new_data = juice(ames_prep))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 1\n   .pred\n   <dbl>\n 1  5.09\n 2  5.01\n 3  5.13\n 4  5.06\n 5  5.08\n 6  5.07\n 7  4.90\n 8  5.10\n 9  5.06\n10  5.16\n# … with 2,329 more rows\n```\n:::\n:::\n\nGenerally it’s best to combine the new data set and the predictions into a single data frame. We create a data frame with the predictions on the ames_test data and then use `bind_cols()` to add the ames_test data to the results.\n\n\n\n::: {.cell hash='step2_cache/html/merge test data and predictions_3a495f1184e001aa747ee6157ffe7ec9'}\n\n```{.r .cell-code}\names_train_results <- predict(ames_fit, new_data = juice(ames_prep)) %>% \n  bind_cols(juice(ames_prep))\n\n# View results\names_train_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 188\n   .pred Lot_F…¹ Lot_A…² Mas_V…³ BsmtF…⁴ BsmtF…⁵ Bsmt_…⁶ Total…⁷ First…⁸ Secon…⁹\n   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.09  0.374  -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2  5.01  0.374   0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3  5.13 -0.137  -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4  5.06 -1.01   -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5  5.08 -0.0770 -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6  5.07 -0.227  -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7  4.90  0.374  -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8  5.10  0.314  -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9  5.06 -1.73   -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10  5.16 -1.73   -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 178 more variables: Gr_Liv_Area <dbl>,\n#   Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>, Full_Bath <dbl>,\n#   Half_Bath <dbl>, Bedroom_AbvGr <dbl>, TotRms_AbvGrd <dbl>,\n#   Fireplaces <dbl>, Garage_Cars <dbl>, Garage_Area <dbl>, Wood_Deck_SF <dbl>,\n#   Open_Porch_SF <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, Longitude <dbl>,\n#   Latitude <dbl>, Time_Since_Remodel <dbl>, House_Age <dbl>,\n#   Sale_Price <dbl>, MS_SubClass_One_Story_1946_and_Newer_All_Styles <dbl>, …\n```\n:::\n:::\n\nNow we have the model results and the training data in a single data frame. \n\n### Metrics for model performance\n\n---\n\n:::{.border}\n- **R-squared (rsq)**: squared correlation between the predicted and observed values; \n- **Root Mean Square Error (RMSE)**: difference between the predicted and observed values (*loss of function*);\n:::\n\nTo obtain the rmse and rsq values on our results, we can use the `rmse()` and `rsq()` functions.\nBoth functions take the following arguments:\n\n- a data frame with columns that have the true values and predictions;\n- the column with the true response values;\n- the column with predicted values.\n\nIn the examples below we pass our ames_test_results to these functions to obtain these values for our test set. \nResults are always returned as a data frame with the following columns: .metric, .estimator, and .estimate.\n\n::: {.cell hash='step2_cache/html/rmse_eaa2720c4926a31c02b1d15c0afbc756'}\n\n```{.r .cell-code}\n#RMSE on train set\ntrain_rmse <- rmse(ames_train_results, \n     truth = Sale_Price,\n     estimate = .pred)\n\n\n#rsq on train set\ntrain_rsq<- rsq(ames_train_results,\n    truth = Sale_Price,\n    estimate = .pred)\n```\n:::\n\n\nBefore exploring the results, let's evaluate the model on the test dataset.\n\n:::{.callout-tip}\n### Challenge X\nWe mentioned earlier that the `bake()` function takes a prepped recipe (ames_prep) and applies it to `new_data`. The new_data could be the training data again or it could be the testing data. We just evaluated our model on the training data, let's try to apply the `bake()` and `predict()` functions on the test data and compare the results.\n\n**Instructions**\n\n::: {.cell hash='step2_cache/html/Challenge X_2e26228a63ddeceb0f2b374a4923b7e2'}\n\n```{.r .cell-code}\n#bake() test data\n\n#predict() selling price on the test data\n\n#combine the test data set and the predictions into a single data frame\n\n#RMSE on test set\n\n#rsq on test set\n```\n:::\n\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\n::: {.cell hash='step2_cache/html/solution_84561f80938ece2c201eafba6be31576'}\n\n```{.r .cell-code}\n#bake() test data\names_test_proc <- bake(ames_prep, new_data = ames_test)\n#predict() selling price on the test data\names_test_results <-predict(ames_fit, new_data = ames_test_proc)\n#combine the training data set and the predictions into a single data frame\names_test_results <- ames_test_results %>%\n  bind_cols(ames_test_proc)\n#RMSE on training set\ntest_rmse <- rmse(ames_test_results, \n     truth = Sale_Price,\n     estimate = .pred)\n#rsq on training set\ntest_rsq <- rsq(ames_test_results,\n    truth = Sale_Price,\n    estimate = .pred)\n```\n:::\n\n:::\n\nLet's have a look at all the metrics for both our training and test datasets:\n\n::: {.cell hash='step2_cache/html/all metrics_319c7687fdfdcd27e4dd4f7b7f83b111'}\n\n```{.r .cell-code}\n#plot metrics for training and test datasets\ntrain_rsq %>%\n  mutate(dataset = \"training\") %>%\n  bind_rows(train_rmse %>%\n              mutate(dataset = \"training\")) %>%\n  bind_rows(test_rsq %>%\n              mutate(dataset = \"test\") %>%\n              bind_rows(test_rmse %>%\n                          mutate(dataset = \"test\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  .metric .estimator .estimate dataset \n  <chr>   <chr>          <dbl> <chr>   \n1 rsq     standard      0.934  training\n2 rmse    standard      0.0460 training\n3 rsq     standard      0.858  test    \n4 rmse    standard      0.0656 test    \n```\n:::\n:::\n\n\nLet's visualise the situation with an **R2 plot**:\n\n::: {.cell hash='step2_cache/html/plot results_4d454a2e81cab0e46cb0c19b702b9699'}\n\n```{.r .cell-code}\names_test_results %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(ames_train_results %>%\n              mutate(train = \"training\")) %>%\n  ggplot(aes(Sale_Price, .pred, color = train)) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", linewidth = 0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.15) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Actual Selling Price\",\n    y = \"Predicted Selling Price\",\n    color = \"Test/Training data\"\n  )\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/plot results-1.png){width=672}\n:::\n:::\n\nThis is a plot that can be used for any regression model.\nIt plots the actual values (Sale Prices) versus the model predictions (.pred) as a scatter plot. It also plot the line y = x through the origin. This line is a visually representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit.\nThe reason this plot is called an R2 plot, is because the R2 is the squared correlation between the true and predicted values, which are plotted as paired in the plot.\n\n### Resampling\n\n---\n\nYou just trained your model one time on the whole training set and then evaluated them on the testing set. Statisticians have come up with a slew of approaches to evaluate models in better ways than this; many important ones fall under the category of resampling.\n\nWe can resample the training set to produce an estimate of how the model will perform.The idea of resampling is to create simulated data sets that can be used to estimate the performance of your model, say, because you want to compare models. You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice). \nOne of these resampling methods is cross-validation.\n\n**Cross-validation** means taking your training set and randomly dividing it up evenly into subsets, sometimes called \"folds\". A fold here means a group or subset or partition.\n\nYou use one of the folds for validation and the rest for training, then you repeat these steps with all the subsets and combine the results, usually by taking the mean. Cross-validation allows you to get a more accurate estimate of how your model will perform on new data.\n\n:::{.callout-tip}\n## Challenge X\n\nWhen you implement 10-fold cross-validation repeated 5 times, you:\n  \n  - randomly divide your training data into 50 subsets and train on 49 at a time (assessing on the other subset), iterating through all 50 subsets for assessment.\n- randomly divide your training data into 10 subsets and train on 9 at a time (assessing on the other subset), iterating through all 10 subsets for assessment. Then you repeat that process 5 times.\n- randomly divide your training data into 5 subsets and train on 4 at a time (assessing on the other subset), iterating through all 5 subsets. Then you repeat that process 10 times.\n:::\n  \n:::{.callout-caution collapse=\"true\"}\n## Solution\n\nSimulations and practical experience show that 10-fold cross-validation repeated 5 times is a great resampling approach for many situations. This approach involves randomly dividing your training data into 10 folds, or subsets or groups, and training on only 9 while using the other fold for assessment. You iterate through all 10 folds being used for assessment; this is one round of cross-validation. You can then repeat the whole process multiple, perhaps 5, times.\n:::\n  \n\n::: {.cell hash='step2_cache/html/create cross-validation folds_cc245fc961cb5269c7cc75e5266a6f98'}\n\n```{.r .cell-code}\nset.seed(9)\n\names_folds <- vfold_cv(ames_train, v=10, repeats = 5, strata = Sale_Price)\n\nglimpse(ames_folds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 3\n$ splits <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2103 x …\n$ id     <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1…\n$ id2    <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06\", \"Fo…\n```\n:::\n:::\n\n\n\n### Create a Workflow\n\n---\n\nIn the previous section, we trained a linear regression model to the housing data step-by-step. In this section, we will go over how to combine all of the modeling steps into a single workflow.\n\nThe `workflow` package was designed to capture the entire modeling process and combine models and recipes into a single object. To create a workflow, we start with `workflow()` to create an empty workflow and then add out model and recipe with `add_model()` and `add_recipe()`.\n\n\n::: {.cell hash='step2_cache/html/workflow_e6e0ffefa4f7d065793b77dd7f7a6896'}\n\n```{.r .cell-code}\names_wf <- workflow() %>%\n  add_model(ames_model) %>% \n  add_recipe(ames_rec)\n\names_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_other()\n• step_nzv()\n• step_center()\n• step_scale()\n• step_dummy()\n• step_lincomb()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nOnce we have created a set of resamples, we can use the function `fit_resamples()` to fit a model to each resample and compute performance metrics for each.\n\n\n::: {.cell hash='step2_cache/html/evaluating models with resampling_319d0200f3b4bd3ee98f7eed8b8e8fa3'}\n\n```{.r .cell-code}\nset.seed(234)\names_res <- ames_wf %>%\n  fit_resamples(\n    ames_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nglimpse(ames_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 6\n$ splits       <list> [<vfold_split[2103 x 236 x 2339 x 81]>], [<vfold_split[2…\n$ id           <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"R…\n$ id2          <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     <list> [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>],…\n$ .notes       <list> [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>],…\n$ .predictions <list> [<tbl_df[236 x 4]>], [<tbl_df[236 x 4]>], [<tbl_df[236 x…\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(ames_res, \"../_models/ames_res.rds\")\n```\n:::\n\nThe column .metric contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n\n::: {.cell hash='step2_cache/html/resampled metrics_df2950586886d355ab5a20b4423b986d'}\n\n```{.r .cell-code}\n# Obtain performance metrics on resampled training data\names_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0540    50 0.00129 Preprocessor1_Model1\n2 rsq     standard   0.909     50 0.00368 Preprocessor1_Model1\n```\n:::\n:::\n\n- `vfold_cv()` creates folds for cross-validation;\n- `fit_resamples()` fits models to resamples; \n- `collect_metrics()` obtains performance metrics from the results.\n\nWe can see that the regression relationship is very strong: 90.8% of the variability in the selling price can be explained by the predictors and, on average, each element in the predicted selling price differs from the actual selling price by 0.05.\n\nWe can reliably measure performance using only the training data.\n\nIf we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance. \n\n\n::: {.cell hash='step2_cache/html/plot resampling_1bd8113fd8b6cb80f85c6cae8aebfbc0'}\n\n```{.r .cell-code}\names_res %>%\n  collect_predictions() %>%\n  ggplot(aes(Sale_Price, .pred, color = id)) + \n  geom_abline(lty = 2, col = \"gray\", linewidth = 1.5) +\n  geom_point(alpha = 0.15) +\n  coord_obs_pred()\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/plot resampling-1.png){width=672}\n:::\n:::\n\n\n### Back to the testing data\n\n---\n\nLet’s use the `last_fit()` function to evaluate once on the testing set:\n\n\n::: {.cell hash='step2_cache/html/final fit_88570fb8ef546cdedfe107386f757071'}\n\n```{.r .cell-code}\n#Final fit on test dataset\names_final <- ames_wf %>%\n  last_fit(ames_split)\n\n# Obtain performance metrics on test data\ncollect_metrics(ames_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0656 Preprocessor1_Model1\n2 rsq     standard      0.858  Preprocessor1_Model1\n```\n:::\n:::\n\n\nThe R2 and RMSE metrics are similar for both the training and testing datasets in our linear regression model. This is a good sign that the model is not over-fitting and can be used for making predictions on new data.\n\nWe can save the test set predictions by using the `collect_predictions()` function. This function returns a data frame which will have the response variables values from the test set and a column named .pred with the model predictions.\n\n\n::: {.cell hash='step2_cache/html/collect test predictions_7a2cb73943f48e5c5b0aed199d950bd8'}\n\n```{.r .cell-code}\n# Obtain test set predictions data frame\names_results_final <- ames_final %>% \n                 collect_predictions()\n# View results\names_results_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 586 × 5\n   id               .pred  .row Sale_Price .config             \n   <chr>            <dbl> <int>      <dbl> <chr>               \n 1 train/test split  5.03     2       5.02 Preprocessor1_Model1\n 2 train/test split  5.19     3       5.24 Preprocessor1_Model1\n 3 train/test split  5.39    18       5.60 Preprocessor1_Model1\n 4 train/test split  5.13    26       5.15 Preprocessor1_Model1\n 5 train/test split  5.23    29       5.26 Preprocessor1_Model1\n 6 train/test split  4.99    30       4.98 Preprocessor1_Model1\n 7 train/test split  4.99    31       5.02 Preprocessor1_Model1\n 8 train/test split  4.98    32       4.94 Preprocessor1_Model1\n 9 train/test split  5.14    34       5.18 Preprocessor1_Model1\n10 train/test split  5.77    47       5.70 Preprocessor1_Model1\n# … with 576 more rows\n```\n:::\n:::\n\n\nFinally, let’s use this data frame to make an R2 plot to visualize our model performance on the test data set:\n\n\n::: {.cell hash='step2_cache/html/plot final_d27edbf37a3003fe8c95f7bf9bdc7252'}\n\n```{.r .cell-code}\nggplot(data = ames_results_final,\n       mapping = aes(x = .pred, y = Sale_Price)) +\n  geom_point(color = '#006EA1', alpha = 0.25) +\n  geom_abline(intercept = 0, slope = 1, color = 'black', linewidth=0.5, linetype=\"dotted\") +\n  labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/plot final-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-note}\n### Key points\n\n- The workflows package enables a handy type of object that can bundle pre-processing and models together;\n- You don’t have to keep track of separate objects in your workspace;\n- The recipe prepping and model fitting can be executed using a single call to `fit()` instead of `prep()`-`juice()`-`fit()`;\n- The recipe baking and model predictions are handled with a single call to `predict()` instead of `bake()`-`predict()`;\n- Workflows are be able to evaluate different recipes and models at once.\n:::\n\n- *https://medium.com/@sachin.hs20/simple-linear-regression-using-example-e4e2a89df54c.*\n- *Adapted from \"Supervised Machine Learning: Case Studies in R!\". MIT - licensed. Available [here](https://github.com/juliasilge/supervised-ML-case-studies-course).*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}