{
  "hash": "e2a997ae473fac6d448c8a99b4a82b48",
  "result": {
    "markdown": "::: {.cell}\n\n:::\n\n:::{.callout-note}\n## Learning objective:\n\n- Build a ML model for predicting whether a person has diabetes or not;\n\n:::\n\n:::{.callout-tip}\n## Exercise:\n\nIn this case study, you could make predictions about whether a patient will develop diabetes or not based on their medical and demographic variables. What kind of model will you build?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\nUnlike the first case study, when we built regression models to predict a numeric or continuous variable, in this case study we are going to build classification models, to predict the class: diabetes or no diabetes.\n:::\n\n## What is a classifier?\n\n---\n\nA classifier is some kind of rule / black box / widget that you can feed a new example and it will spit out whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.\n\n![A classifier for cats and not cats.](../fig/CatNotCat.jpg)\n\nYou can have classifiers for anything you can have a yes/no answer to, e.g.\n\n- Is this a cat? ðŸ±\n- Do these test results indicate cancer? ðŸš‘\n- Is this email spam or not spam? ðŸ“§\n\nYou can also have classifiers that categorise things into multiple (more than two) categories e.g.\n\n- Which animal is this, out of the 12 animals I have trained my model on? ðŸ±\n- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? ðŸš‘\n- Is this email important, not important but not spam, or spam? ðŸ“§\n\nIt is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.\n\n## Model evaluation (classification)\n\n---\n\nFor now, let's imagine we have a classifier already. How can we test it to see how good it is?\nA good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.\n\n![Demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/_CatConfusion.jpg)\n\n### Confusion Matrix\n\n---\n\nWhen applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares \"the truth\" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and if we correctly label something which is not a cat, as not a cat, then this is a true negative.\nIn our case, the confusion matrix will look like this:\n\n- **true positive (TP)** : Diabetic correctly identified as diabetic\n- **true negative (TN)** : Healthy correctly identified as healthy\n- **false positive (FP)** : Healthy incorrectly identified as diabetic\n- **false negative (FN)** : Diabetic incorrectly identified as healthy\n\n### Some common classification metrics\n\n---\n\nDon't worry if you forget some of these - there are so many different words used to describe different ways to divide up the confusion matrix, it can get very confusing. I swear each time [I just look up wikipedia again](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) to figure out which part of the confusion matrix to look at. There are even more there that we won't even bother talking about here.\n\n:::{.border}\n#### **Accuracy**:\nHow often does the classifier label examples correctly? Objective: maximize. Example:\n\n$$\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correctly labelled examples}}{\\text{All examples}}=\\frac{31+52}{31+52+10+7}=83\\%$$\n\nAccuracy is the opposite of the misclassification rate. So,\n\n$$\\text{Misclassification rate} = 1 - \\text{Accuracy} = \\frac{\\text{Incorrectly labelled examples}}{\\text{All examples}}$$\n:::\n\n\n:::{.border}\n#### **Precision**:\nWhat fraction of things labelled as a cat were actually cats? Objective: maximize. Example:\n\n$$\\frac{TP}{TP+FP} = \\frac{\\text{Correctly labelled cats}}{\\text{All things labelled as cats}}=\\frac{31}{31+10}=76\\%$$\n:::\n\n\n:::{.border}\n#### **Sensitivity / Recall**:\nHow often does the classifier label a cat as a cat? Objective: maximize. Example:\n\n$$\\frac{TP}{TP+FN} = \\frac{\\text{Correctly labelled cats}}{\\text{All true cats}}=\\frac{31}{31+7}=81\\%$$\n:::\n\n\n:::{.border}\n#### **Specificity**:\nHow often does it label a not-cat as a not-cat? Objective: maximize. Example:\n\n$$\\frac{TN}{TN+FP} = \\frac{\\text{Correctly labelled not-cats}}{\\text{All true not-cats}}=\\frac{52}{52+10}=84\\%$$\n:::\n\n\n:::{.border}\n#### **F1-score**:\n\nThis is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;\n\n$$\\frac{1}{F_1} = \\frac{1}{2}\\left(\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Sensitivity}}\\right)$$\n\n\nSo that\n\n$$F_1 = 2\\cdot\\left(\\frac{1}{\\frac{1}{81\\%}+\\frac{1}{83\\%}}\\right) = 82\\%$$\n:::\n\n\n### AUC: Area under the curve\n\n---\n\nA good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.\n\nTo capture this balance, we often use a Reciever Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.\n\n![A Reciever Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/_CatArea.jpg)\n\n>For additional discussion of classification error metrics, see [Tharwat 2018](https://doi.org/10.1016/j.aci.2018.08.003), for example.\n\n:::{.callout-tip}\n### Challenge 1\n\n- In the case of patients with a rare disease, what can be the problem of using accuracy to evaluate the performance of a machine learning model.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\nAccuracy is calculated as the (TP + TN)/(total) number of cases in the dataset. If you have very few positive cases, such as when working with a rare disease, the numerator of this fraction will be dominated by the true negatives you accurately predict in your dataset - so not very informative when assessing whether your classifier predicts the disease well at all!\n:::\n\n\n::: {.cell hash='step2_cache/html/libraries_8bcde3eb890a1dcfc84f4d4e1c3d6b5b'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(tune)\ntheme_set(theme_minimal())\n\ndiabetes_rec <- readRDS(\"../_models/diabetes_rec.rds\")\ndiabetes_folds <- readRDS(\"../_models/diabetes_folds.rds\")\n```\n:::\n\n\n### Tree-based models\n\n### Logistic regression\n\n## Tune model hyperparameters\n\n---\n\nSome model parameters cannot be learned directly from a dataset during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of randomly selected variables to be considered at each split in a tree-based model (called `mtry` in tidymodels).\n\nInstead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on a resampled data set (like the cross-validation folds we have previously created) and measuring how well all these models perform. This process is called ***tuning**.\n\nYou can identify which parameters to `tune()` in a model specification.\n\nWe will focus on \n\n\n::: {.cell hash='step2_cache/html/tune_06043bcc80ca500f4bc7a601391093d5'}\n\n```{.r .cell-code}\nrf_model_diabetes <- \n  # specify that the model is a random forest\n  rand_forest(mtry = tune()) %>%\n  # specify that the `mtry` parameter needs to be tuned\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\") %>%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n```\n:::\n\n\n> **Note** Nothing about this model specification is specific to the diabetes dataset.\n\n### Find which parameters will give the model its best accuracy\n\n:::{.borders}\n- Try different values and measure their performance;\n- Find good values for these parameters;\n- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n:::\n\nYou have a couple of options for how to choose which possible values for the tuning parameters to try. \n\n- We can use the function `tune_grid()` to tune either a workflow or a model specification with a set of resampled data, such as the cross-validation we created. Grid search, combined with resampling, requires fitting a lot of models!\nThese models donâ€™t depend on one another and can be run in parallel.\n\n\n::: {.cell hash='step2_cache/html/tune_grid_9f57b5eb8156b9c992bfa42d8ea8dfc2'}\n\n```{.r .cell-code}\n#Speed up computation\ncores <- parallel::detectCores(logical = FALSE) #detect the number of available CPU cores on your machine \ncl <- parallel::makePSOCKcluster(cores) #creates a cluster of worker processes\ndoParallel::registerDoParallel(cl) #enable parallel computing\n\nset.seed(245)\n\nrf_grid <- tune_grid(\n  rf_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  metrics = metric_set(accuracy, roc_auc, sensitivity, specificity))\n\n# Shut down parallel backend with:\nforeach::registerDoSEQ()\nparallel::stopCluster(cl)\n\nrf_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation repeated 5 times using stratification \n# A tibble: 50 Ã— 5\n   splits           id      id2    .metrics          .notes          \n   <list>           <chr>   <chr>  <list>            <list>          \n 1 <split [483/54]> Repeat1 Fold01 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 2 <split [483/54]> Repeat1 Fold02 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 3 <split [483/54]> Repeat1 Fold03 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 4 <split [483/54]> Repeat1 Fold04 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 5 <split [483/54]> Repeat1 Fold05 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 6 <split [483/54]> Repeat1 Fold06 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 7 <split [483/54]> Repeat1 Fold07 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 8 <split [484/53]> Repeat1 Fold08 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n 9 <split [484/53]> Repeat1 Fold09 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n10 <split [484/53]> Repeat1 Fold10 <tibble [28 Ã— 5]> <tibble [0 Ã— 3]>\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\n\nUse `collect_metrics` to extract the metrics calculated from the cross-validation performance across the different values of the parameters:\n\n::: {.cell hash='step2_cache/html/collect tuning metrics_0eb89a36abfdbdbb53e666e70cf26331'}\n\n```{.r .cell-code}\n#collect metrics\nrf_grid %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 Ã— 7\n    mtry .metric  .estimator  mean     n std_err .config             \n   <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n 1     7 accuracy binary     0.762    50 0.00796 Preprocessor1_Model1\n 2     7 roc_auc  binary     0.839    50 0.00766 Preprocessor1_Model1\n 3     8 accuracy binary     0.759    50 0.00850 Preprocessor1_Model2\n 4     8 roc_auc  binary     0.837    50 0.00764 Preprocessor1_Model2\n 5     1 accuracy binary     0.761    50 0.00843 Preprocessor1_Model3\n 6     1 roc_auc  binary     0.846    50 0.00762 Preprocessor1_Model3\n 7     2 accuracy binary     0.765    50 0.00805 Preprocessor1_Model4\n 8     2 roc_auc  binary     0.846    50 0.00750 Preprocessor1_Model4\n 9     6 accuracy binary     0.760    50 0.00805 Preprocessor1_Model5\n10     6 roc_auc  binary     0.839    50 0.00770 Preprocessor1_Model5\n11     5 accuracy binary     0.766    50 0.00725 Preprocessor1_Model6\n12     5 roc_auc  binary     0.841    50 0.00769 Preprocessor1_Model6\n13     4 accuracy binary     0.767    50 0.00794 Preprocessor1_Model7\n14     4 roc_auc  binary     0.844    50 0.00742 Preprocessor1_Model7\n```\n:::\n\n```{.r .cell-code}\n#see which model performed the best, in terms of some given metric\nrf_grid %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 7\n   mtry .metric .estimator  mean     n std_err .config             \n  <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1     2 roc_auc binary     0.846    50 0.00750 Preprocessor1_Model4\n2     1 roc_auc binary     0.846    50 0.00762 Preprocessor1_Model3\n3     4 roc_auc binary     0.844    50 0.00742 Preprocessor1_Model7\n4     5 roc_auc binary     0.841    50 0.00769 Preprocessor1_Model6\n5     6 roc_auc binary     0.839    50 0.00770 Preprocessor1_Model5\n```\n:::\n\n```{.r .cell-code}\nrf_grid %>%\n  show_best(\"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1     4 accuracy binary     0.767    50 0.00794 Preprocessor1_Model7\n2     5 accuracy binary     0.766    50 0.00725 Preprocessor1_Model6\n3     2 accuracy binary     0.765    50 0.00805 Preprocessor1_Model4\n4     7 accuracy binary     0.762    50 0.00796 Preprocessor1_Model1\n5     1 accuracy binary     0.761    50 0.00843 Preprocessor1_Model3\n```\n:::\n:::\n\n\n\n:::{.callout-tip}\n### Challenge X\n\nUse `tune_grid` and `collect_metrics` to tune a workflow. Hints:\n\nUse `workflow()` to define the workflow:\n\n::: {.cell hash='step2_cache/html/challenge x worflow_b51a6f59255465e21484b0d12fa8cbeb'}\n\n```{.r .cell-code}\n#set the workflow\n\n#add the recipe\n\n#add the model\n```\n:::\n\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\n\n::: {.cell hash='step2_cache/html/challenge x worflow solution_7a4f1337483bc5b5cd8c0d53278acb98'}\n\n```{.r .cell-code}\n#set the workflow\nrf_worflow <- workflow() %>%\n#add the recipe\nadd_recipe(diabetes_rec) %>%\n#add the model\n  add_model(rf_model_diabetes)\n\n#tune the workflow\nset.seed(22)\n\nrf_tune_diabetes <- rf_worflow %>%\n  tune_grid(resamples = diabetes_folds,\n            metrics = metric_set(accuracy, roc_auc, sensitivity, specificity))\n\nrf_tune_diabetes %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 Ã— 7\n    mtry .metric     .estimator  mean     n std_err .config             \n   <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n 1     5 accuracy    binary     0.771    50 0.00658 Preprocessor1_Model1\n 2     5 roc_auc     binary     0.844    50 0.00619 Preprocessor1_Model1\n 3     5 sensitivity binary     0.859    50 0.00765 Preprocessor1_Model1\n 4     5 specificity binary     0.604    50 0.0150  Preprocessor1_Model1\n 5     6 accuracy    binary     0.762    50 0.00725 Preprocessor1_Model2\n 6     6 roc_auc     binary     0.842    50 0.00626 Preprocessor1_Model2\n 7     6 sensitivity binary     0.853    50 0.00796 Preprocessor1_Model2\n 8     6 specificity binary     0.591    50 0.0161  Preprocessor1_Model2\n 9     4 accuracy    binary     0.768    50 0.00737 Preprocessor1_Model3\n10     4 roc_auc     binary     0.846    50 0.00599 Preprocessor1_Model3\n# â€¦ with 22 more rows\n```\n:::\n:::\n\n:::\n\nLet's visualise our results:\n\n\n::: {.cell hash='step2_cache/html/autoplot_124378005cb4270904e8f391cb53ea88'}\n\n```{.r .cell-code}\nautoplot(rf_grid)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rf_tune_diabetes)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-2.png){width=672}\n:::\n:::\n\n### Updating the workflow\n\n---\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}