{
  "hash": "e83145c6d2a97923097bbe57cb010071",
  "result": {
    "markdown": "\n:::{.callout-note}\n## Learning objective:\n\n- From base R to tidymodels;\n- Split our data into training and test sets;\n- Preprocess the training data;\n- Specify a linear regression model;\n- Train our model on the training data;\n- Transform the test data and obtain predictions using our trained model.\n:::\n\n:::{.callout-tip}\n## Exercise:\n\nIn this case study, you will predict houses selling price from characteristics of these houses, like size and layout of the living space in the house.\nWhat kind of model will you build?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\nTo predict a continuous, numeric quantity like selling price, use regression models.\n:::\n\nLoad in the packages we’ll be using for modelling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(vip) \n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Build a simple linear regression model using base R\n\n---\n\nIn a linear model, we assume that there is a linear relationship between the input variable(s) and the output variable. This means that as the input variable(s) increase or decrease, the output variable changes in a straight line.\n\nImagine you have a scatter plot with your data points all over it. A linear model is like drawing a straight line through the scatter plot that best fits all the points. The slope and intercept of this line are chosen in such a way that the distance between the line and all the points is minimized. This line is then used to predict the output for new input values. \n\n![Example of a linear model](../fig/lm.png){width=500 fig-align=\"left\"}\n\nThe straight red dotted line represents the linear model equation $y=mx+c$, where $c$ is the y-intercept of the regression line, $m$ is the slope of the regression line, and $y$ is the expected value for y for the given $x$ value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit a linear model\names_lm <- lm(Sale_Price ~ Gr_Liv_Area, data = ames_data)\n\n#Print the summary of the model\nsummary(ames_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94258 -0.06622  0.01359  0.07298  0.39246 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.835e+00  7.406e-03  652.80   <2e-16 ***\nGr_Liv_Area 2.579e-04  4.714e-06   54.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.124 on 2923 degrees of freedom\nMultiple R-squared:  0.506,\tAdjusted R-squared:  0.5058 \nF-statistic:  2994 on 1 and 2923 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nR-squared value explains the variability of y with respect to x:\n\n- varies between 0 to 1;\n- R-squared values closer to 0 mean the regression relationship is very low;\n- R-squared values closer to 1 mean the regression relationship is very strong.\n\nLet's plot our linear regression model:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ames_data$Gr_Liv_Area, ames_data$Sale_Price,\n     xlab=\"Gr_Liv_Area\",\n     ylab=\"Sale_Price\", \n     col = \"blue\")\nabline(ames_lm, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n### Build a linear regression model using Tidymodels\n\n---\n\nWhen you type library(tidymodels), you load a collection of packages for modeling and machine learning using tidyverse principles. All the packages are designed to be consistent, modular, and to support good modeling practices. The first thing we are going to practice is splitting your data into a training set and a testing set. The tidymodels package `rsample` has functions that help you specify training and testing sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) #so we all get the same results\names_split <- ames_data %>%\n    initial_split(prop = 0.8,\n                  strata = Sale_Price) #stratification\n```\n:::\n\nStratified sampling would split within each quartile:\n![strata](../fig/strata.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train <- training(ames_split)\names_test <- testing(ames_split)\n\nsaveRDS(ames_train, \"../_models/ames_train.Rds\")\nsaveRDS(ames_test, \"../_models/ames_test.Rds\")\n```\n:::\n\n\nThe code here takes an input data set and puts 80% of it into a training dataset and 20% of it into a testing dataset; it chooses the individual cases so that both sets are balanced in selling price.\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_train %>% \n  ggplot(aes(x = log(Sale_Price),  col = \"red\", fill = NULL)) + \n  geom_density() + theme_minimal() +\n  geom_line(data = ames_test,\n            stat = \"density\",\n            col = \"blue\") + theme(legend.position=\"none\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/distr test and train-1.png){width=672}\n:::\n:::\n\n## Feature engineering\n\n\nIn tidymodels, you can preprocess your data using the *recipes* package.\nTypical preprocessing steps include:\n\n- Scaling and centering numeric predictors;\n- Removing skewness from numeric variables;\n- One-hot and dummy variable encoding for categorical variables;\n- Removing correlated predictors and zero variance variables;\n- Imputing missing data.\n\nEach successive step() function adds a preprocessing step to our recipe object in the order that they are provided:\n\n::: {.cell}\n\n```{.r .cell-code}\names_rec <-\n  recipe(Sale_Price ~ ., data = ames_train) %>%\n  step_other(all_nominal(), threshold = 0.01) %>% #useful when you have some factor levels with very few observations, all_nominal selects both characters and factors, pools infrequently occurring values (frequency less than 0.01) into an \"other\" category\n  step_nzv(all_nominal()) %>% #remove variables that are highly sparse and unbalanced\n  step_center(all_numeric(), -all_outcomes()) %>% #subtracts the column mean from a variable\n  step_scale(all_numeric(), -all_outcomes()) %>% #divides by the standard deviation\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) #create dummy variables for all nominal variables except the outcome variable \n\names_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nOperations:\n\nCollapsing factor levels for all_nominal()\nSparse, unbalanced variable filter on all_nominal()\nCentering for all_numeric(), -all_outcomes()\nScaling for all_numeric(), -all_outcomes()\nDummy variables from all_nominal(), -all_outcomes()\n```\n:::\n:::\n\n\nThe preprocessing recipe `ames_rec` has been defined but no values have been estimated.\n\n- The `prep()` function takes that defined object and computes everything so that the preprocessing steps can be executed. Note that This is done with the training data.\n\n::: {.cell}\n\n```{.r .cell-code}\names_prep <- prep(ames_rec)\n\names_prep\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         80\n\nTraining data contained 2339 data points and no missing data.\n\nOperations:\n\nCollapsing factor levels for MS_SubClass, MS_Zoning, Street, Lot_Shape, Util... [trained]\nSparse, unbalanced variable filter removed Street, Alley, Land_Contour, Utilities,... [trained]\nCentering for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nScaling for Lot_Frontage, Lot_Area, Mas_Vnr_Area, BsmtFin_S... [trained]\nDummy variables from MS_SubClass, MS_Zoning, Lot_Shape, Lot_Config, Neighborho... [trained]\n```\n:::\n:::\n\nThe `bake() and juice() functions both return data, not a preprocessing recipe object. \n- The `bake()` function takes a prepped recipe (one that has had all quantities estimated from training data) and applies it to `new_data`. That new_data could be the training data again or it could be the testing data (with the TRAINING parameters)\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_test_proc <- bake(ames_prep, new_data = ames_test)\n```\n:::\n\n\n- The `juice()` function is a nice little shortcut. When we `juice()` the recipe, we squeeze that training data back out, transformed in the ways we specified. \nLet's compare the `bake()` and `juice()` outputs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbake(ames_prep, new_data = ames_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 231\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n```\n:::\n\n```{.r .cell-code}\njuice(ames_prep) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,339 × 231\n   Lot_Frontage Lot_Area Mas_V…¹ BsmtF…² BsmtF…³ Bsmt_…⁴ Total…⁵ First…⁶ Secon…⁷\n          <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1       0.374   -0.212   -0.569  -1.42    0.162  -1.27  -0.388   -0.726  -0.786\n 2       0.374    0.0383  -0.569  -1.42   -0.298  -0.288 -0.430   -0.774  -0.786\n 3      -0.137   -0.733   -0.569  -1.42   -0.298   0.341  0.0550  -0.231  -0.786\n 4      -1.01    -0.943   -0.569   1.27   -0.298   0.630 -0.497   -0.848  -0.786\n 5      -0.0770  -0.273   -0.569   1.27   -0.298   0.816 -0.303   -0.631  -0.786\n 6      -0.227   -0.359   -0.569  -1.42    0.416  -1.27  -0.714   -1.07   -0.786\n 7       0.374   -0.0453  -0.569   1.27   -0.298   0.584 -0.544   -0.382  -0.786\n 8       0.314   -0.149   -0.569   0.372  -0.298  -1.27  -2.48     0.427   0.579\n 9      -1.73    -0.0430  -0.391  -0.970  -0.298  -0.288 -0.388   -0.678  -0.786\n10      -1.73    -0.392   -0.569  -1.42   -0.298  -0.404 -0.0137  -0.308  -0.786\n# … with 2,329 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n```\n:::\n:::\n\nIt is the same as bake(ames_rep, new_data = ames_train) and is just a shortcut that we are going to use later.\n\n:::{.callout-tip}\n### Challenge X\n\nDoes it make sense to apply these preprocessing steps to the test set?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\nNo, it doesn't. You want the set test to look like new data that your model will see in the future, so you don't want to mess with the class balance there; you want to see how your model will perform on imbalanced data, even if you have trained it on artificially balanced data.\n:::\n\n### Build the model\n\nIn tidymodels, you specify models using three concepts.\n\n- Model **type** differentiates models such as logistic regression, decision tree models, and so forth;\n- Model **mode** includes common options like regression and classification, some model types support either of these while some only have one mode;\n- Model **engine** is the computational tool which will be used to fit the model. \n\nWe will specify the model using the `parsnip` package - Many functions have different interfaces and arguments names and parsnip standardizes the interface for fitting models as well as the return values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#a linear regression model specification\names_model <- linear_reg() %>% #pick a model\n  set_engine(\"lm\")           #set the engine\n                             #set_mode(\"regression\") we don't need this as the model linear_reg() only does regression\n\n#view model properties\names_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nNow we are ready to train our model object on the training data. \nWe can do this using the `fit()` function from the parsnip package. \nThe `fit()` function takes the following arguments:\n  \n- a parnsip model object specification;\n- a model formula\n- a data frame with the training data\n\nThe code below trains our linear regression model on the prepped training data. In our formula, we have specified that Sale_Price is the response variable and included all the rest as our predictor variables.\n\n::: {.cell}\n\n```{.r .cell-code}\names_fit <- ames_model %>%\n  fit(Sale_Price ~ .,\n      data=juice(ames_prep))\n\n# View lm_fit properties\names_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Sale_Price ~ ., data = data)\n\nCoefficients:\n                                          (Intercept)  \n                                            4.2344150  \n                                         Lot_Frontage  \n                                            0.0010509  \n                                             Lot_Area  \n                                            0.0056766  \n                                         Mas_Vnr_Area  \n                                            0.0033526  \n                                         BsmtFin_SF_1  \n                                            0.0288579  \n                                         BsmtFin_SF_2  \n                                           -0.0024142  \n                                          Bsmt_Unf_SF  \n                                           -0.0096332  \n                                        Total_Bsmt_SF  \n                                            0.0293221  \n                                         First_Flr_SF  \n                                            0.0428570  \n                                        Second_Flr_SF  \n                                            0.0514152  \n                                      Low_Qual_Fin_SF  \n                                            0.0043645  \n                                          Gr_Liv_Area  \n                                                   NA  \n                                       Bsmt_Full_Bath  \n                                            0.0046797  \n                                       Bsmt_Half_Bath  \n                                            0.0008760  \n                                            Full_Bath  \n                                            0.0075337  \n                                            Half_Bath  \n                                            0.0063545  \n                                        Bedroom_AbvGr  \n                                           -0.0042524  \n                                        Kitchen_AbvGr  \n                                           -0.0032240  \n                                        TotRms_AbvGrd  \n                                            0.0010374  \n                                           Fireplaces  \n                                            0.0048444  \n                                          Garage_Cars  \n                                            0.0088917  \n                                          Garage_Area  \n                                            0.0054821  \n                                         Wood_Deck_SF  \n                                            0.0028177  \n                                        Open_Porch_SF  \n                                            0.0018956  \n                                       Enclosed_Porch  \n                                            0.0035592  \n                                   Three_season_porch  \n                                            0.0006254  \n                                         Screen_Porch  \n                                            0.0057138  \n                                            Pool_Area  \n                                           -0.0001730  \n                                             Misc_Val  \n                                            0.0008810  \n                                              Mo_Sold  \n                                           -0.0003984  \n                                            Year_Sold  \n                                           -0.0019209  \n                                            Longitude  \n                                           -0.0097179  \n                                             Latitude  \n                                           -0.0059391  \n                                   Time_Since_Remodel  \n                                           -0.0057059  \n                                            House_Age  \n                                           -0.0214642  \n      MS_SubClass_One_Story_1946_and_Newer_All_Styles  \n                                            0.0061643  \n                 MS_SubClass_One_Story_1945_and_Older  \n                                           -0.0206969  \n     MS_SubClass_One_and_Half_Story_Finished_All_Ages  \n                                            0.0282382  \n                 MS_SubClass_Two_Story_1946_and_Newer  \n                                           -0.0138448  \n                 MS_SubClass_Two_Story_1945_and_Older  \n                                            0.0158489  \n                      MS_SubClass_Split_or_Multilevel  \n                                           -0.0153464  \n                              MS_SubClass_Split_Foyer  \n                                            0.0073930  \n               MS_SubClass_Duplex_All_Styles_and_Ages  \n                                            0.0123223  \n             MS_SubClass_One_Story_PUD_1946_and_Newer  \n                                            0.0186726  \n             MS_SubClass_Two_Story_PUD_1946_and_Newer  \n                                           -0.0290154  \nMS_SubClass_Two_Family_conversion_All_Styles_and_Ages  \n                                            0.0126693  \n                                    MS_SubClass_other  \n                                                   NA  \n               MS_Zoning_Floating_Village_Residential  \n                                            0.0369229  \n                    MS_Zoning_Residential_Low_Density  \n                                            0.0295723  \n                 MS_Zoning_Residential_Medium_Density  \n                                            0.0224024  \n                                      MS_Zoning_other  \n                                                   NA  \n                                    Lot_Shape_Regular  \n                                            0.0047757  \n                         Lot_Shape_Slightly_Irregular  \n                                            0.0045442  \n                       Lot_Shape_Moderately_Irregular  \n                                            0.0167009  \n                                      Lot_Shape_other  \n                                                   NA  \n                                    Lot_Config_Corner  \n                                            0.0061984  \n                                   Lot_Config_CulDSac  \n                                            0.0124304  \n                                       Lot_Config_FR2  \n                                           -0.0034930  \n                                    Lot_Config_Inside  \n                                            0.0041820  \n                                     Lot_Config_other  \n                                                   NA  \n                              Neighborhood_North_Ames  \n                                           -0.0144052  \n                           Neighborhood_College_Creek  \n                                           -0.0403143  \n                                Neighborhood_Old_Town  \n                                           -0.0395437  \n                                 Neighborhood_Edwards  \n                                           -0.0485293  \n                                Neighborhood_Somerset  \n                                            0.0158820  \n                      Neighborhood_Northridge_Heights  \n                                            0.0152055  \n                                 Neighborhood_Gilbert  \n                                           -0.0076228  \n                                  Neighborhood_Sawyer  \n                                           -0.0245480  \n                          Neighborhood_Northwest_Ames  \n                                           -0.0147878  \n                             Neighborhood_Sawyer_West  \n                                           -0.0382249  \n                                Neighborhood_Mitchell  \n                                           -0.0254694  \n                               Neighborhood_Brookside  \n                                           -0.0110661  \n                                Neighborhood_Crawford  \n                                            0.0154749  \n                  Neighborhood_Iowa_DOT_and_Rail_Road  \n                                           -0.0481840  \n                              Neighborhood_Timberland  \n                                           -0.0246553  \n                              Neighborhood_Northridge  \n                                            0.0109785  \n                             Neighborhood_Stone_Brook  \n                                            0.0376989  \n Neighborhood_South_and_West_of_Iowa_State_University  \n                                           -0.0339271  \n                             Neighborhood_Clear_Creek  \n                                           -0.0170590  \n                          Neighborhood_Meadow_Village  \n                                           -0.0629992  \n                                   Neighborhood_other  \n                                                   NA  \n                                   Condition_1_Artery  \n                                           -0.0179882  \n                                    Condition_1_Feedr  \n                                           -0.0119543  \n                                     Condition_1_Norm  \n                                            0.0063617  \n                                     Condition_1_PosN  \n                                            0.0101914  \n                                     Condition_1_RRAn  \n                                           -0.0142278  \n                                    Condition_1_other  \n                                                   NA  \n                                     Bldg_Type_OneFam  \n                                            0.0352264  \n                                   Bldg_Type_TwoFmCon  \n                                            0.0217637  \n                                     Bldg_Type_Duplex  \n                                                   NA  \n                                      Bldg_Type_Twnhs  \n                                           -0.0164880  \n                                     Bldg_Type_TwnhsE  \n                                                   NA  \n                         House_Style_One_and_Half_Fin  \n                                           -0.0334739  \n                                House_Style_One_Story  \n                                           -0.0113905  \n                                   House_Style_SFoyer  \n                                           -0.0045098  \n                                     House_Style_SLvl  \n                                            0.0080461  \n                                House_Style_Two_Story  \n                                           -0.0068768  \n                                    House_Style_other  \n                                                   NA  \n                                    Overall_Qual_Fair  \n                                           -0.0090491  \n                           Overall_Qual_Below_Average  \n                                           -0.0081678  \n                                 Overall_Qual_Average  \n                                            0.0135609  \n                           Overall_Qual_Above_Average  \n                                            0.0228156  \n                                    Overall_Qual_Good  \n                                            0.0317333  \n                               Overall_Qual_Very_Good  \n                                            0.0506332  \n                               Overall_Qual_Excellent  \n                                            0.0543703  \n                                   Overall_Qual_other  \n                                                   NA  \n                                    Overall_Cond_Fair  \n                                            0.0861003  \n                           Overall_Cond_Below_Average  \n                                            0.1382136  \n                                 Overall_Cond_Average  \n                                            0.1659276  \n                           Overall_Cond_Above_Average  \n                                            0.1795836  \n                                    Overall_Cond_Good  \n                                            0.1982810  \n                               Overall_Cond_Very_Good  \n                                            0.2021602  \n                               Overall_Cond_Excellent  \n                                            0.2216273  \n                                   Overall_Cond_other  \n                                                   NA  \n                                     Roof_Style_Gable  \n                                            0.0041041  \n                                       Roof_Style_Hip  \n                                            0.0013475  \n                                     Roof_Style_other  \n                                                   NA  \n                                 Exterior_1st_AsbShng  \n                                           -0.0197379  \n                                 Exterior_1st_BrkFace  \n                                            0.0218545  \n                                 Exterior_1st_CemntBd  \n                                           -0.0675824  \n                                 Exterior_1st_HdBoard  \n                                           -0.0177946  \n                                 Exterior_1st_MetalSd  \n                                           -0.0041764  \n                                 Exterior_1st_Plywood  \n                                           -0.0147857  \n                                  Exterior_1st_Stucco  \n                                           -0.0150856  \n                                 Exterior_1st_VinylSd  \n                                           -0.0269952  \n                                 Exterior_1st_Wd.Sdng  \n                                           -0.0127229  \n                                 Exterior_1st_WdShing  \n                                           -0.0219538  \n                                   Exterior_1st_other  \n                                                   NA  \n                                 Exterior_2nd_AsbShng  \n                                           -0.0294565  \n                                 Exterior_2nd_BrkFace  \n                                           -0.0252434  \n                                 Exterior_2nd_CmentBd  \n                                            0.0511306  \n                                 Exterior_2nd_HdBoard  \n                                           -0.0066992  \n                                 Exterior_2nd_MetalSd  \n                                           -0.0094713  \n                                 Exterior_2nd_Plywood  \n                                           -0.0088774  \n                                  Exterior_2nd_Stucco  \n                                            0.0028778  \n                                 Exterior_2nd_VinylSd  \n                                            0.0061450  \n                                 Exterior_2nd_Wd.Sdng  \n                                           -0.0049424  \n                                 Exterior_2nd_Wd.Shng  \n                                           -0.0005624  \n                                   Exterior_2nd_other  \n                                                   NA  \n                                 Mas_Vnr_Type_BrkFace  \n                                            0.0119788  \n                                    Mas_Vnr_Type_None  \n                                            0.0132996  \n                                   Mas_Vnr_Type_Stone  \n                                            0.0202174  \n                                   Mas_Vnr_Type_other  \n                                                   NA  \n                                 Exter_Qual_Excellent  \n                                            0.0412905  \n                                      Exter_Qual_Fair  \n                                           -0.0100946  \n                                      Exter_Qual_Good  \n                                            0.0067359  \n                                   Exter_Qual_Typical  \n                                                   NA  \n                                      Exter_Cond_Fair  \n                                           -0.0537435  \n                                      Exter_Cond_Good  \n                                           -0.0271431  \n                                   Exter_Cond_Typical  \n                                           -0.0190455  \n                                     Exter_Cond_other  \n                                                   NA  \n                                    Foundation_BrkTil  \n                                           -0.0221335  \n                                    Foundation_CBlock  \n                                           -0.0197832  \n                                     Foundation_PConc  \n                                           -0.0113600  \n                                      Foundation_Slab  \n                                           -0.0024282  \n                                     Foundation_other  \n                                                   NA  \n                                  Bsmt_Qual_Excellent  \n                                            0.0296809  \n                                       Bsmt_Qual_Fair  \n                                            0.0030970  \n                                       Bsmt_Qual_Good  \n                                            0.0160700  \n                                Bsmt_Qual_No_Basement  \n                                            0.0188896  \n                                    Bsmt_Qual_Typical  \n                                            0.0147692  \n                                      Bsmt_Qual_other  \n                                                   NA  \n                                     Bsmt_Exposure_Av  \n                                            0.0038457  \n                                     Bsmt_Exposure_Gd  \n                                            0.0268377  \n                                     Bsmt_Exposure_Mn  \n                                            0.0001947  \n                                     Bsmt_Exposure_No  \n                                           -0.0028258  \n                            Bsmt_Exposure_No_Basement  \n                                                   NA  \n                                   BsmtFin_Type_1_ALQ  \n                                            0.0837734  \n                                   BsmtFin_Type_1_BLQ  \n                                            0.0662634  \n                                   BsmtFin_Type_1_GLQ  \n                                            0.0596601  \n                                   BsmtFin_Type_1_LwQ  \n                                            0.0329057  \n                           BsmtFin_Type_1_No_Basement  \n                                                   NA  \n                                   BsmtFin_Type_1_Rec  \n                                            0.0072722  \n                                   BsmtFin_Type_1_Unf  \n                                                   NA  \n                                 Heating_QC_Excellent  \n                                            0.7700744  \n                                      Heating_QC_Fair  \n                                            0.7444348  \n                                      Heating_QC_Good  \n                                            0.7633523  \n                                   Heating_QC_Typical  \n                                            0.7575195  \n                                     Heating_QC_other  \n                                                   NA  \n                                        Central_Air_N  \n                                           -0.0232337  \n                                        Central_Air_Y  \n                                                   NA  \n                                     Electrical_FuseA  \n                                           -0.0134919  \n                                     Electrical_FuseF  \n                                           -0.0188211  \n                                     Electrical_SBrkr  \n                                           -0.0171300  \n                                     Electrical_other  \n                                                   NA  \n                               Kitchen_Qual_Excellent  \n                                            0.0303544  \n                                    Kitchen_Qual_Fair  \n                                           -0.0005397  \n                                    Kitchen_Qual_Good  \n                                            0.0050291  \n                                 Kitchen_Qual_Typical  \n                                                   NA  \n                                   Kitchen_Qual_other  \n                                                   NA  \n                               Fireplace_Qu_Excellent  \n                                           -0.0115950  \n                                    Fireplace_Qu_Fair  \n                                           -0.0047983  \n                                    Fireplace_Qu_Good  \n                                            0.0043227  \n                            Fireplace_Qu_No_Fireplace  \n                                           -0.0029386  \n                                    Fireplace_Qu_Poor  \n                                           -0.0089229  \n                                 Fireplace_Qu_Typical  \n                                                   NA  \n                                   Garage_Type_Attchd  \n                                            0.0277848  \n                                  Garage_Type_Basment  \n                                            0.0183001  \n                                  Garage_Type_BuiltIn  \n                                            0.0259155  \n                                   Garage_Type_Detchd  \n                                            0.0246522  \n                                Garage_Type_No_Garage  \n                                            0.0012371  \n                                    Garage_Type_other  \n                                                   NA  \n                                    Garage_Finish_Fin  \n                                            0.0004798  \n                              Garage_Finish_No_Garage  \n                                           -0.0064210  \n                                    Garage_Finish_RFn  \n                                           -0.0033819  \n                                    Garage_Finish_Unf  \n                                                   NA  \n                                     Garage_Qual_Fair  \n                                           -0.0243010  \n                                Garage_Qual_No_Garage  \n                                                   NA  \n                                  Garage_Qual_Typical  \n                                           -0.0147644  \n                                    Garage_Qual_other  \n                                                   NA  \n                                     Garage_Cond_Fair  \n                                           -0.0288340  \n                                Garage_Cond_No_Garage  \n                                                   NA  \n                                  Garage_Cond_Typical  \n                                           -0.0058730  \n                                    Garage_Cond_other  \n                                                   NA  \n                              Paved_Drive_Dirt_Gravel  \n                                           -0.0044010  \n                         Paved_Drive_Partial_Pavement  \n                                           -0.0080910  \n                                    Paved_Drive_Paved  \n                                                   NA  \n                                   Fence_Good_Privacy  \n                                           -0.0068390  \n                                      Fence_Good_Wood  \n                                           -0.0129793  \n                                Fence_Minimum_Privacy  \n                                           -0.0040916  \n                                       Fence_No_Fence  \n                                           -0.0050151  \n                                          Fence_other  \n                                                   NA  \n                                        Sale_Type_COD  \n                                           -0.0202250  \n                                        Sale_Type_New  \n                                            0.0181473  \n                                        Sale_Type_WD.  \n                                           -0.0182848  \n                                      Sale_Type_other  \n                                                   NA  \n                               Sale_Condition_Abnorml  \n                                           -0.0451496  \n                                Sale_Condition_Family  \n                                           -0.0225266  \n                                Sale_Condition_Normal  \n                                           -0.0017995  \n                               Sale_Condition_Partial  \n                                           -0.0222955  \n                                 Sale_Condition_other  \n                                                   NA  \n```\n:::\n:::\n\nTo obtain the detailed results from our trained linear regression model in a data frame, we can use the `tidy()` and `glance()` functions directly on our trained parsnip model, ames_fit.\n- The `tidy()` function takes a linear regression object and returns a data frame of the estimated model coefficients and their associated F-statistics and p-values;\n- The `glance()` function will return performance metrics obtained on the training data such as the R2 value (r.squared) and the RMSE (sigma).\n- We can also use the `vip()` function to plot the variable importance for each predictor in our model. The importance value is determined based on the F-statistics and estimate coefficents in our trained model object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data frame of estimated coefficients\ntidy(ames_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 231 × 5\n   term          estimate std.error statistic   p.value\n   <chr>            <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)    4.23      0.0959     44.2   1.44e-303\n 2 Lot_Frontage   0.00105   0.00121     0.868 3.85e-  1\n 3 Lot_Area       0.00568   0.00129     4.40  1.12e-  5\n 4 Mas_Vnr_Area   0.00335   0.00167     2.01  4.50e-  2\n 5 BsmtFin_SF_1   0.0289    0.0247      1.17  2.42e-  1\n 6 BsmtFin_SF_2  -0.00241   0.00118    -2.04  4.13e-  2\n 7 Bsmt_Unf_SF   -0.00963   0.00208    -4.63  3.82e-  6\n 8 Total_Bsmt_SF  0.0293    0.00325     9.03  3.65e- 19\n 9 First_Flr_SF   0.0429    0.00291    14.7   7.55e- 47\n10 Second_Flr_SF  0.0514    0.00354    14.5   9.84e- 46\n# … with 221 more rows\n```\n:::\n\n```{.r .cell-code}\n# Performance metrics on training data\nglance(ames_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹  sigma stati…² p.value    df logLik    AIC    BIC devia…³\n      <dbl>      <dbl>  <dbl>   <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     0.935      0.929 0.0476    161.       0   191  3905. -7424. -6313.    4.86\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n```\n:::\n\n```{.r .cell-code}\n# Plot variable importance\nvip(ames_fit)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/metrics-1.png){width=672}\n:::\n:::\n\n\n#Evaluating the model\n\nTo assess the accuracy of our trained linear regression model, ames_fit, we must use it to make predictions on our test data, ames_test_proc. \nThis is done with the `predict()` function from parnsip. This function takes two important arguments:\n\n- a trained parnsip model object;\n- new_data for which to generate predictions.\n\nThe code below uses the predict() function to generate a data frame with a single column, *.pred*, which contains the predicted Sale Price values on the ames_test data.\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(ames_fit, new_data = ames_test_proc)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 586 × 1\n   .pred\n   <dbl>\n 1  5.04\n 2  5.22\n 3  5.39\n 4  5.14\n 5  5.23\n 6  4.99\n 7  4.99\n 8  4.98\n 9  5.14\n10  5.76\n# … with 576 more rows\n```\n:::\n:::\n\n*Warning: prediction from a rank-deficient fit may be misleading*\n  \nOne reason this warning occurs is that you have more model parameters than observations in the dataset. We refer to this as high dimensional data. With high dimensional data, it becomes impossible to find a model that can describe the relationship between the predictor variables and the response variable because we don’t have enough observations to train the model on. The easiest way to resolve this issue is to use a simpler model with less coefficients to estimate.We are not worrying about this today.\n\nGenerally it’s best to combine the test data set and the predictions into a single data frame. We create a data frame with the predictions on the ames_test data and then use `bind_cols()` to add the ames_test data to the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_test_results <- predict(ames_fit, new_data = ames_test_proc) %>% \n  bind_cols(ames_test_proc)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n```{.r .cell-code}\n# View results\names_test_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 586 × 232\n   .pred Lot_F…¹ Lot_A…² Mas_V…³ BsmtF…⁴ BsmtF…⁵ Bsmt_…⁶ Total…⁷ First…⁸ Secon…⁹\n   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  5.04   0.675   0.172 -0.569    0.819   0.552 -0.657  -0.388   -0.689  -0.786\n 2  5.22   0.705   0.488  0.0527  -1.42   -0.298 -0.347   0.670    0.456  -0.786\n 3  5.39   0.916   0.145  1.45    -0.523  -0.298 -0.336   1.92     1.85   -0.786\n 4  5.14   0.224  -0.207 -0.569   -0.970  -0.298 -0.632   0.0242  -0.266  -0.786\n 5  5.23  -0.949  -0.516 -0.569   -0.523  -0.298 -0.466   0.850    0.477  -0.786\n 6  4.99  -1.10   -1.01   2.33     0.819  -0.298 -0.527  -1.33    -1.78    0.392\n 7  4.99  -1.10   -1.01   2.26     0.819  -0.298 -0.759  -1.23    -1.67    0.539\n 8  4.98  -1.10   -1.01   1.62     1.27   -0.298 -0.0770 -1.23    -1.67    0.539\n 9  5.14  -1.01   -0.943 -0.569   -1.42   -0.298 -0.495  -0.452   -0.797   0.618\n10  5.76   1.58    0.492  5.74    -0.523  -0.298  2.08    4.26     4.07   -0.786\n# … with 576 more rows, 222 more variables: Low_Qual_Fin_SF <dbl>,\n#   Gr_Liv_Area <dbl>, Bsmt_Full_Bath <dbl>, Bsmt_Half_Bath <dbl>,\n#   Full_Bath <dbl>, Half_Bath <dbl>, Bedroom_AbvGr <dbl>, Kitchen_AbvGr <dbl>,\n#   TotRms_AbvGrd <dbl>, Fireplaces <dbl>, Garage_Cars <dbl>,\n#   Garage_Area <dbl>, Wood_Deck_SF <dbl>, Open_Porch_SF <dbl>,\n#   Enclosed_Porch <dbl>, Three_season_porch <dbl>, Screen_Porch <dbl>,\n#   Pool_Area <dbl>, Misc_Val <dbl>, Mo_Sold <dbl>, Year_Sold <dbl>, …\n```\n:::\n:::\n\nNow we have the model results and the test data in a single data frame. \n\n### Calculating rmse and rsq on the Test Data\n\nTo obtain the rmse and rsq values on our test set results, we can use the `rmse()` and `rsq()` functions.\nBoth functions take the following arguments:\n\n- a data frame with columns that have the true values and predictions;\n- the column with the true response values;\n- the column with predicted values.\n\nIn the examples below we pass our ames_test_results to these functions to obtain these values for our test set. \nResults are always returned as a data frame with the following columns: .metric, .estimator, and .estimate.\n\n::: {.cell}\n\n```{.r .cell-code}\n#RMSE on test set\ntest_rmse <- rmse(ames_test_results, \n     truth = Sale_Price,\n     estimate = .pred)\n\n#rsq on test set\ntest_rsq<- rsq(ames_test_results,\n    truth = Sale_Price,\n    estimate = .pred)\n```\n:::\n\n\n:::{.callout-tip}\n### Challenge X\nWe mentioned earlier that the `bake()` function takes a prepped recipe (ames_prep) and applies it to `new_data`. The new_data could be the training data again or it could be the testing data. We just evaluated our model on the test data, let's try to apply the `bake()` and `predict()` functions on the training data and compare the results.\n\n**Instructions**\n\n::: {.cell}\n\n```{.r .cell-code}\n#bake() training data\n\n#predict() selling price on the training data\n\n#combine the training data set and the predictions into a single data frame\n\n#RMSE on training set\n\n#rsq on training set\n```\n:::\n\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\n::: {.cell}\n\n```{.r .cell-code}\n#bake() training data\names_train_proc <- bake(ames_prep, new_data = ames_train)\n#predict() selling price on the training data\names_train_results <-predict(ames_fit, new_data = ames_train_proc)\n#combine the training data set and the predictions into a single data frame\names_train_results <- ames_train_results %>%\n  bind_cols(ames_train_proc)\n#RMSE on training set\ntrain_rmse <- rmse(ames_train_results, \n     truth = Sale_Price,\n     estimate = .pred)\n#rsq on training set\ntrain_rsq <- rsq(ames_train_results,\n    truth = Sale_Price,\n    estimate = .pred)\n```\n:::\n\n:::\n\nLet's have a look at all the metrics for both our training and test datasets:\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot metrics for training and test datasets\ntrain_rsq %>%\n  mutate(dataset = \"training\") %>%\n  bind_rows(train_rmse %>%\n              mutate(dataset = \"training\")) %>%\n  bind_rows(test_rsq %>%\n              mutate(dataset = \"test\") %>%\n              bind_rows(test_rmse %>%\n                          mutate(dataset = \"test\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  .metric .estimator .estimate dataset \n  <chr>   <chr>          <dbl> <chr>   \n1 rsq     standard      0.935  training\n2 rmse    standard      0.0456 training\n3 rsq     standard      0.862  test    \n4 rmse    standard      0.0647 test    \n```\n:::\n:::\n\nIf we look at the testing data, the rmse is higher than the training data.\nOur training data is not giving us a good idea of how our model is going to perform.\nIn this situation, our algorithm fits our existing data very well, but doesn’t generalise well on new data.\n\nLet's visualise the situation with an **R2 plot**:\n\n::: {.cell}\n\n```{.r .cell-code}\names_test_results %>%\n  mutate(train = \"testing\") %>%\n  bind_rows(ames_train_results %>%\n              mutate(train = \"training\")) %>%\n  ggplot(aes(Sale_Price, .pred, color = train)) +\n  geom_abline(intercept = 0, slope = 1, color = \"black\", size = 0.5, linetype=\"dotted\") +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~train) +\n  labs(\n    x = \"Actual Selling Price\",\n    y = \"Predicted Selling Price\",\n    color = \"Test/Training data\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](step2_files/figure-html/plot results-1.png){width=672}\n:::\n:::\n\nThis is a plot that can be used for any regression model.\nIt plots the actual values (Sale Prices) versus the model predictions (.pred) as a scatter plot. It also plot the line y = x through the origin. This line is a visually representation of the perfect model where all predicted values are equal to the true values in the test set. The farther the points are from this line, the worse the model fit.\nThe reason this plot is called an R2 plot, is because the R2 is the squared correlation between the true and predicted values, which are plotted as paired in the plot.\n\n### Resampling\n\nWe made not such a great decision in the previous section; we expected the model evaluated once on the whole training set to help us understand something about how it would perform on new data. Fortunately, we have some options. We can resample the training set to produce an estimate of how the model will perform.The idea of resampling is to create simulated data sets that can be used to estimate the performance of your model, say, because you want to compare models. You can create these resampled data sets instead of using either your training set (which can give overly optimistic results, especially for powerful ML algorithms) or your testing set (which is extremely valuable and can only be used once or at most twice). \nOne of these resampling methods is cross-validation.\n\n**Cross-validation** means taking your training set and randomly dividing it up evenly into subsets, sometimes called \"folds\". A fold here means a group or subset or partition.\n\nYou use one of the folds for validation and the rest for training, then you repeat these steps with all the subsets and combine the results, usually by taking the mean. Cross-validation allows you to get a more accurate estimate of how your model will perform on new data.\n\n:::{.callout-tip}\n## Challenge X\n\nWhen you implement 10-fold cross-validation repeated 5 times, you:\n  \n  - randomly divide your training data into 50 subsets and train on 49 at a time (assessing on the other subset), iterating through all 50 subsets for assessment.\n- randomly divide your training data into 10 subsets and train on 9 at a time (assessing on the other subset), iterating through all 10 subsets for assessment. Then you repeat that process 5 times.\n- randomly divide your training data into 5 subsets and train on 4 at a time (assessing on the other subset), iterating through all 5 subsets. Then you repeat that process 10 times.\n:::\n  \n:::{.callout-caution collapse=\"true\"}\n## Solution\n\nSimulations and practical experience show that 10-fold cross-validation repeated 5 times is a great resampling approach for many situations. This approach involves randomly dividing your training data into 10 folds, or subsets or groups, and training on only 9 while using the other fold for assessment. You iterate through all 10 folds being used for assessment; this is one round of cross-validation. You can then repeat the whole process multiple, perhaps 5, times.\n:::\n  \n\n::: {.cell}\n\n```{.r .cell-code}\names_folds <- vfold_cv(ames_train, v=10, repeats = 5)\n\nglimpse(ames_folds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 3\n$ splits <list> [<vfold_split[2105 x 234 x 2339 x 81]>], [<vfold_split[2105 x …\n$ id     <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1…\n$ id2    <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06\", \"Fo…\n```\n:::\n:::\n\nIn the next steps, we won't not use `prep()` or `bake()`. The `ames_rec` recipe will be automatically applied in a later step using the `workflow()` and `last_fit()` functions.\n\n### Create a Workflow\n\nIn the previous section, we trained a linear regression model to the housing data step-by-step. In this section, we will go over how to combine all of the modeling steps into a single workflow.\nThe `workflow` package was designed to combine models and recipes into a single object. To create a workflow, we start with `workflow()` to create an empty workflow and then add out model and recipe with `add_model()` and `add_recipe()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_wf <- workflow() %>%\n  add_model(ames_model) %>% \n  add_recipe(ames_rec)\n\names_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_other()\n• step_nzv()\n• step_center()\n• step_scale()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n:::\n:::\n\n\nOnce we have created a set of resamples, we can use the function `fit_resamples()` to fit a model to each resample and compute performance metrics for each.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(234)\names_res <- ames_wf %>%\n  fit_resamples(\n    ames_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10, Repeat1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10, Repeat2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10, Repeat3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10, Repeat4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold01, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold02, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold03, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold04, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold05, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold06, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold07, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold08, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold09, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n! Fold10, Repeat5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n```{.r .cell-code}\nglimpse(ames_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50\nColumns: 6\n$ splits       <list> [<vfold_split[2105 x 234 x 2339 x 81]>], [<vfold_split[2…\n$ id           <chr> \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"Repeat1\", \"R…\n$ id2          <chr> \"Fold01\", \"Fold02\", \"Fold03\", \"Fold04\", \"Fold05\", \"Fold06…\n$ .metrics     <list> [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>], [<tbl_df[2 x 4]>],…\n$ .notes       <list> [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>], [<tbl_df[1 x 3]>],…\n$ .predictions <list> [<tbl_df[234 x 4]>], [<tbl_df[234 x 4]>], [<tbl_df[234 x…\n```\n:::\n\n```{.r .cell-code}\nsaveRDS(ames_res, \"../_models/ames_res.rds\")\n```\n:::\n\nThe column .metric contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain performance metrics on resampled training data\names_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.0537    50 0.00122 Preprocessor1_Model1\n2 rsq     standard   0.909     50 0.00383 Preprocessor1_Model1\n```\n:::\n:::\n\n- `vfold_cv()` creates folds for cross-validation;\n- `fit_resamples()` fits models to resamples; \n- `collect_metrics()` obtains performance metrics from the results.\n\nNotice that now we have a realistic estimate from the training data that is closer to the testing data!\n\nIf we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance. \n\nLet’s use the `last_fit()` function to evaluate once on the testing set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Final fit on test dataset\names_final <- ames_wf %>%\n  last_fit(ames_split)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n! train/test split: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-deficient fit may be misleading\n```\n:::\n\n```{.r .cell-code}\n# Obtain performance metrics on test data\ncollect_metrics(ames_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard      0.0647 Preprocessor1_Model1\n2 rsq     standard      0.862  Preprocessor1_Model1\n```\n:::\n:::\n\n\nThe R-squared (rsq) and root mean squared error (RMSE) metrics are similar for both the training and testing datasets in our linear regression model. This is a good sign that the model is not over-fitting and can be used for making predictions on new data.\n\nWe can save the test set predictions by using the `collect_predictions()` function. This function returns a data frame which will have the response variables values from the test set and a column named .pred with the model predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain test set predictions data frame\names_results_final <- ames_final %>% \n                 collect_predictions()\n# View results\names_results_final\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 586 × 5\n   id               .pred  .row Sale_Price .config             \n   <chr>            <dbl> <int>      <dbl> <chr>               \n 1 train/test split  5.04     2       5.02 Preprocessor1_Model1\n 2 train/test split  5.22     3       5.24 Preprocessor1_Model1\n 3 train/test split  5.39    18       5.60 Preprocessor1_Model1\n 4 train/test split  5.14    26       5.15 Preprocessor1_Model1\n 5 train/test split  5.23    29       5.26 Preprocessor1_Model1\n 6 train/test split  4.99    30       4.98 Preprocessor1_Model1\n 7 train/test split  4.99    31       5.02 Preprocessor1_Model1\n 8 train/test split  4.98    32       4.94 Preprocessor1_Model1\n 9 train/test split  5.14    34       5.18 Preprocessor1_Model1\n10 train/test split  5.76    47       5.70 Preprocessor1_Model1\n# … with 576 more rows\n```\n:::\n:::\n\n\nFinally, let’s use this data frame to make an R2 plot to visualize our model performance on the test data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ames_results_final,\n       mapping = aes(x = .pred, y = Sale_Price)) +\n  geom_point(color = '#006EA1', alpha = 0.25) +\n  geom_abline(intercept = 0, slope = 1, color = 'black', size=0.5, linetype=\"dotted\") +\n  labs(title = 'Linear Regression Results - Ames Test Set',\n       x = 'Predicted Selling Price',\n       y = 'Actual Selling Price')\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/plot final-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "step2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}