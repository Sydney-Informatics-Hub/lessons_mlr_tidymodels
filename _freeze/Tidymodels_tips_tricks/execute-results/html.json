{
  "hash": "4cae772b93a8fd66f2ff52dffdef7f52",
  "result": {
    "markdown": "## use\\_\\*()\n\nThe [usemodels package](https://usemodels.tidymodels.org) is a helpful way of quickly creating code snippets to fit models using the tidymodels framework. Given a simple formula and a data set, the use\\_\\* functions can create code that appropriate for the data (given the model). The package includes these templates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(usemodels)\nls(\"package:usemodels\", pattern = \"use_\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"use_bag_tree_rpart\"   \"use_C5.0\"             \"use_cubist\"          \n [4] \"use_dbarts\"           \"use_earth\"            \"use_glmnet\"          \n [7] \"use_kernlab_svm_poly\" \"use_kernlab_svm_rbf\"  \"use_kknn\"            \n[10] \"use_mgcv\"             \"use_mixOmics\"         \"use_nnet\"            \n[13] \"use_ranger\"           \"use_rpart\"            \"use_xgboost\"         \n[16] \"use_xrf\"             \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\nuse_ranger(diabetes ~ ., data = PimaIndiansDiabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nranger_recipe <- \n  recipe(formula = diabetes ~ ., data = PimaIndiansDiabetes) \n\nranger_spec <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\") \n\nranger_workflow <- \n  workflow() %>% \n  add_recipe(ranger_recipe) %>% \n  add_model(ranger_spec) \n\nset.seed(99270)\nranger_tune <-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n```\n:::\n:::\n\n\n## tune_race_anova()\n\nThe problem with grid search is that you don't know if some of those choices you made about the candidate parameters are any good until you're done with all the computations. The `tune_race_anova()` function from the `finetune` package is a dynamic way of doing grid search. What racing does is as you start to do the model tuning, it looks at the results as they happen and eliminates tuning parameter combinations that are unlikely to be the best results using a repeated measure ANOVA model:\n\n```         \ninstall.packages(\"finetune\")\nlibrary(finetune)\n\nrf_tune_wf <- rf_workflow %>%\n              tune_race_anova(resamples = diabetes_folds,\n                              grid = rf_grid)\n```\n\n## step_mutate()\n\n`step_mutate()` creates a specification of a recipe step that will add variables using `dplyr::mutate()`. We did this step during EDA in [Session 1](001_Regression/step1.qmd) but we could have easily introduced it in our recipe `ames_rec` object in this way:\n\n```         \names_rec <- recipe(sale_price ~ ., data = ames_train) %>% \n            step_mutate(time_since_remodel = year_sold - year_remod_add, \n                        house_age = year_sold - year_built)\n```\n\nThe advantage of performing these preprocessing steps with the recipe package is that all the feature engineering you want to perform to your data can be put into a single object, you can save that object, you can carry it around. It's not in a bunch of scripts. It's been unit tested and it has a lot of features in it.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}