{
  "hash": "e325d7a0b5fcdf66cbb67da9f1678f32",
  "result": {
    "markdown": "\n:::{.callout-note}\n## Learning objective:\n\n- Build a ML model for predicting whether a person has diabetes or not;\n\n:::\n\n:::{.callout-tip}\n## Exercise:\n\nIn this case study, you could make predictions about whether a patient will develop diabetes or not based on their medical and demographic variables. What kind of model will you build?\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\nUnlike the first case study, when we built regression models to predict a numeric or continuous variable, in this case study we are going to build classification models, to predict the class: diabetes or no diabetes.\n:::\n\n## What is a classifier?\n\nA classifier is some kind of rule / black box / widget that you can feed a new example and it will spit out whether or not it is part of a given class. E.g. below, we are classifying the animals to be either *cat* or *not cat*.\n\n![A classifier for cats and not cats.](../fig/CatNotCat.jpg)\n\nYou can have classifiers for anything you can have a yes/no answer to, e.g.\n\n- Is this a cat? 🐱\n- Do these test results indicate cancer? 🚑\n- Is this email spam or not spam? 📧\n\nYou can also have classifiers that categorise things into multiple (more than two) categories e.g.\n\n- Which animal is this, out of the 12 animals I have trained my model on? 🐱\n- Do these test results indicate {none, stage 1, stage 2, stage 3, stage 4} cancer? 🚑\n- Is this email important, not important but not spam, or spam? 📧\n\nIt is clear that in some of these examples we are more concerned with being wrong in one direction than the other, e.g. it's better to let some spam email through accidentally than to block all of it but also junk important emails from people you know. Likewise, we would prefer our medical tests to err on the side of caution and not give a negative test result to someone who needs treatment. So we will need to adjust a parameter to decide how much we want to trade this off.\n\n## Model evaluation (classification)\n\nFor now, let's imagine we have a classifier already. How can we test it to see how good it is?\nA good start is a confusion matrix - a table of what test data it labels correctly and incorrectly.\n\n![Demonstration of a confusion matrix for a cat classifier that has labelled 100 animals as cats or not-cats.](../fig/_CatConfusion.jpg)\n\n### Confusion Matrix\n\nWhen applying classification models, we often use a confusion matrix to evaluate certain performance measures. A confusion matrix is a matrix that compares \"the truth\" to the labels generated by your classifier. When we label a cat correctly, we refer to this as a true positive. When we fail to label a cat as a cat, this is called a false negative.  However, if we label something which is not a cat as a cat, this is called a false positive; and if we correctly label something which is not a cat, as not a cat, then this is a true negative.\nIn our case, the confusion matrix will look like this:\n\n- **true positive (TP)** : Diabetic correctly identified as diabetic\n- **true negative (TN)** : Healthy correctly identified as healthy\n- **false positive (FP)** : Healthy incorrectly identified as diabetic\n- **false negative (FN)** : Diabetic incorrectly identified as healthy\n\n### Some common classification metrics\n\nDon't worry if you forget some of these - there are so many different words used to describe different ways to divide up the confusion matrix, it can get very confusing. I swear each time [I just look up wikipedia again](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) to figure out which part of the confusion matrix to look at. There are even more there that we won't even bother talking about here.\n\n:::{.border}\n#### **Accuracy**:\nHow often does the classifier label examples correctly?\n\n$$\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correctly labelled examples}}{\\text{All examples}}$$\n:::\n\n\n:::{.border}\n#### **Precision**:\nWhat fraction of things labelled as a cat were actually cats?\n\n$$\\frac{TP}{TP+FP} = \\frac{\\text{Correctly labelled cats}}{\\text{All things labelled as cats}}$$\n:::\n\n\n:::{.border}\n#### **Sensitivity / Recall**:\nHow often does the classifier label a cat as a cat?\n\n$$\\frac{TP}{TP+FN} = \\frac{\\text{Correctly labelled cats}}{\\text{All true cats}}$$\n:::\n\n\n:::{.border}\n#### **Specificity**:\nHow often does it label a not-cat as a not-cat? \n\n$$\\frac{TN}{TN+FP} = \\frac{\\text{Correctly labelled not-cats}}{\\text{All true not-cats}}$$\n:::\n\n\n:::{.border}\n#### **F1-score**:\n\nThis is a commonly used overall measure of classifier performance (but not the only one and not always the best depending upon the problem). It is defined as the harmonic mean of precision and sensitivity;\n\n$$\\frac{1}{F_1} = \\frac{1}{2}\\left(\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Sensitivity}}\\right)$$\n:::\n\n\n### AUC: Area under the curve\n\nA good classifier will have high precision and high specificity, minimizing both false positives and false negatives. In practice, and with an imperfect classifier, you can tune a knob to say which of those two you care more about. There will be some kind of a trade-off between the two.\n\nTo capture this balance, we often use a Reciever Operator Characteristic (ROC) curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis, for all possible trade-offs. A line that is diagonal from the lower left corner to the upper right corner represents a random guess at labelling each example. The higher the line is in the upper left-hand corner, the better the classifier in general. AUC computes the area under this curve. For a perfect classifier, AUC = 1, for a random guess, AUC=0.5. Objective: maximize.\n\n![A Reciever Operator Characteristic (ROC) curve, from which the Area Under the Curve (AUC) can be calculated.](../fig/_CatArea.jpg)\n\n>For additional discussion of classification error metrics, see [Tharwat 2018](https://doi.org/10.1016/j.aci.2018.08.003), for example.\n\n:::{.callout-tip}\n### Challenge 7\n\n- In the case of patients with a rare disease, what can be the problem of using accuracy to evaluate the performance of a machine learning model.\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\nAccuracy is calculated as the (TP + TN)/(total) number of cases in the dataset. If you have very few positive cases, such as when working with a rare disease, the numerator of this fraction will be dominated by the true negatives you accurately predict in your dataset - so not very informative when assessing whether your classifier predicts the disease well at all!\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(tune)\nlibrary(vip)\nlibrary(ParallelLogger)\nlibrary(doParallel)\nlibrary(workflowsets)\nlibrary(qs)\nlibrary(ranger)\ntheme_set(theme_minimal())\n\ndiabetes_rec <- qread(\"../_models/diabetes_rec.qs\")\ndiabetes_folds <- qread(\"../_models/diabetes_folds.qs\")\nd_na_train <- qread(\"../_models/d_na_train.qs\")\nd_na_test <- qread(\"../_models/d_na_test.qs\")\ndiabetes_split <- qread(\"../_models/diabetes_split.qs\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## Some classification models\n\n### Tree-based models\n\nA tree-based model is a type of algorithm that creates a tree-like structure to make predictions about a certain outcome, such as whether a customer will buy a product or not. The tree structure consists of nodes that represent different features, and the algorithm uses these features to split the data into smaller and smaller subsets. Each subset is then assigned a label based on the majority of its observations, and this process continues until the algorithm reaches a stopping criterion or has created a fully-grown tree. Once the tree is created, it can be used to make predictions by following the path through the tree that corresponds to a given set of input features. Tree-based models are simple and intuitive to understand, and can be used for both classification and regression tasks.\n\n- **Decision trees** are a simple type of tree-based model that use a hierarchical structure of nodes to make predictions about a certain outcome. The process continues until a stopping criterion is met, such as a maximum tree depth or a minimum number of observations per leaf node, and it can predict the outcome. A single decision tree may not be accurate enough for many real-world problems;\n![Decision_tree](../fig/decision_tree.png)\n\n- **Random forest** overcomes this limitation by building many decision trees, each using a randomly selected subset of the data and features, and then combining their predictions to make a final prediction. \n\n\n### Logistic regression\n\nLogistic regression is a type of regression where the range of mapping is confined to [0,1], unlike simple linear regression models where the domain and range could take any real value. Logistic regression is a type of algorithm that is used to predict a binary outcome, such as whether a patient is likely to develop diabetes or no. It works by creating a mathematical function that predicts the probability of an observation belonging to a certain class (e.g., diabetes or not diabetes). The function takes into account one or more input variables, such as the patients's age, gender, or body mass index. The output of the function is a value between 0 and 1, which represents the probability of the observation belonging to the positive class (e.g., developing diabetes). To make a prediction, the algorithm compares the predicted probability to a threshold value (e.g., 0.5), and assigns the observation to the positive class if the probability is greater than the threshold, and to the negative class otherwise. The scatter plot of this data looks something like this:\n![Logistic Regression](../fig/lr.png)\nWe see that the data points are in the two extreme clusters. For our prediction modeling, a naive regression line in this scenario will give a nonsense fit (red line on the right plot) and what we actually require to fit is a line (blue on the right plot) to explain (or to correctly separate) a maximum number of data points. Logistic regression is a scheme to search this most optimum blue line. \n\n*Regularization* is a technique that can be used to prevent overfitting of the model. A regularized logistic regression model, is a logistic classifier that has been modified to include a regularization term. This is done by adding a penalty to the model that discourages it from giving too much importance to any variable.\n\nThere are several regularized regression models, defined with the `mixture` parameter:\n\n- **Ridge** regularization encourages the model to have small coefficient values (`mixture = 0`);\n- **Lasso** regularization encourages the model to set some of the coefficients to zero, which performs feature selection. This can help improve the model's interpretability and reduce the impact of irrelevant features on the model's performance (`mixture = 1`);\n- **Elastic Net** regularization combines Ridge and Lasso regularization by adding a penalty term that is a weighted average of both penalties. This approach can provide the benefits of both Ridge and Lasso regularization, such as feature selection and coefficient shrinkage (`mixture` between 0 and 1).\n\n## Tune model hyperparameters\n\nSome model parameters cannot be learned directly from a dataset during model training; these kinds of parameters are called **hyperparameters**. Some examples of hyperparameters include the number of randomly selected variables to be considered at each split in a tree-based model (called `mtry` in tidymodels).\n\nInstead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on a resampled data set (like the cross-validation folds we have previously created) and measuring how well all these models perform. This process is called ***tuning**.\n\nYou can identify which parameters to `tune()` in a model specification.\n\nWe can specify a random forest classifier with the following hyperparameters:\n\n- **mtry**: the number of predictors that will be randomly sampled at each split when creating the tree models;\n- **trees**: the number of decision trees to fit and ultimately average;\n- **min_n**: The minimum number of data points in a node that are required for the node to be split further.\n\nTo specify a random forest model with tidymodels, we need the `rand_forest()` function. The hyperparameters of the model are arguments within the `rand_forest()` function and may be set to specific values. However, if tuning is required, then each of these parameters must be set to `tune()`.\n\nWe will be using the ranger engine. This engine has an optional importance argument which can be used to track variable importance measures. In order to make a variable importance plot with `vip()`, we must add `importance = 'impurity'` inside our `set_engine()` function:\n\n\n::: {.cell hash='step2_cache/html/tune models_a9b50571b72abdf843e6d60817445af2'}\n\n```{.r .cell-code}\nrf_model_diabetes <- \n  # specify that the model is a random forest and which hyperparameters need to be tuned\n  rand_forest(mtry = tune(),\n              trees = tune(),\n              min_n = tune()) %>%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %>% #get variable importance scores\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\nrlr_model_diabetes <- \n  logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n> **Note** Nothing about this model specification is specific to the diabetes dataset.\n\n### Find which parameters will give the model its best accuracy\n\n:::{.borders}\n- Try different values and measure their performance;\n- Find good values for these parameters;\n- Once the value(s) of the parameter(s) are determined, a model can be finalized by fitting the model to the entire training set.\n:::\n\nYou have a couple of options for how to choose which possible values for the tuning parameters to try. One of these options is **creating a random grid of values**. Random grid search is implemented with the `grid_random()` function in tidymodels, taking a sequence of hyperparameter names to create the grid. It also has a size parameter that specifies the number of random combinations to create.\n\nThe `mtry()` hyperparameter requires a pre-set range of values to test since it cannot exceed the number of columns in our data. When we add this to `grid_random()` we can pass `mtry()` into the `range_set()` function and set a range for the hyperparameter with a numeric vector.\n\nIn the code below, we set the range from 3 to 6. This is because we have 9 columns in diabetes_data and we would like to test `mtry()` values somewhere in the middle between 1 and 9, trying to avoid values close to the ends.\n\nWhen using `grid_random()`, it is suggested to use set.seed() for reproducibility.\n\nWe can then use the function `tune_grid()` to tune either a workflow or a model specification with a set of resampled data, such as the cross-validation we created. Grid search, combined with resampling, requires fitting a lot of models!\nThese models don’t depend on one another and can be run in parallel.\n\n\n::: {.cell hash='step2_cache/html/tune_grid_37e9bb32ed11a8e1063b44c76e036cec'}\n\n```{.r .cell-code}\nset.seed(314)\n\nrf_grid <- grid_random(mtry() %>% range_set(c(3, 6)),\n                       trees(),\n                       min_n(),\n                       size = 10)\n\n#View grid\nrf_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n    mtry trees min_n\n   <int> <int> <int>\n 1     4  1644    34\n 2     6  1440    20\n 3     6  1549    31\n 4     5  1734    39\n 5     6   332    11\n 6     5  1064     3\n 7     5   218    37\n 8     4  1304    24\n 9     4   477    32\n10     5  1621     6\n```\n:::\n\n```{.r .cell-code}\n#Tune random forest model \nrf_tune_model <- tune_grid(\n  rf_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rf_grid)\n\nrf_tune_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation repeated 5 times using stratification \n# A tibble: 50 × 5\n   splits           id      id2    .metrics          .notes          \n   <list>           <chr>   <chr>  <list>            <list>          \n 1 <split [483/54]> Repeat1 Fold01 <tibble [20 × 7]> <tibble [0 × 3]>\n 2 <split [483/54]> Repeat1 Fold02 <tibble [20 × 7]> <tibble [0 × 3]>\n 3 <split [483/54]> Repeat1 Fold03 <tibble [20 × 7]> <tibble [0 × 3]>\n 4 <split [483/54]> Repeat1 Fold04 <tibble [20 × 7]> <tibble [0 × 3]>\n 5 <split [483/54]> Repeat1 Fold05 <tibble [20 × 7]> <tibble [0 × 3]>\n 6 <split [483/54]> Repeat1 Fold06 <tibble [20 × 7]> <tibble [0 × 3]>\n 7 <split [483/54]> Repeat1 Fold07 <tibble [20 × 7]> <tibble [0 × 3]>\n 8 <split [484/53]> Repeat1 Fold08 <tibble [20 × 7]> <tibble [0 × 3]>\n 9 <split [484/53]> Repeat1 Fold09 <tibble [20 × 7]> <tibble [0 × 3]>\n10 <split [484/53]> Repeat1 Fold10 <tibble [20 × 7]> <tibble [0 × 3]>\n# … with 40 more rows\n```\n:::\n:::\n\n\n\nUse `collect_metrics` to extract the metrics calculated from the cross-validation performance across the different values of the parameters:\n\n::: {.cell hash='step2_cache/html/collect tuning metrics_804ccc6b83022a89cd1cc082a0ac5954'}\n\n```{.r .cell-code}\n#collect metrics\nrf_tune_model %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 9\n    mtry trees min_n .metric     .estimator  mean     n std_err .config         \n   <int> <int> <int> <chr>       <chr>      <dbl> <int>   <dbl> <chr>           \n 1     4  1644    34 accuracy    binary     0.767    50 0.00758 Preprocessor1_M…\n 2     4  1644    34 roc_auc     binary     0.848    50 0.00633 Preprocessor1_M…\n 3     4  1644    34 sensitivity binary     0.863    50 0.00834 Preprocessor1_M…\n 4     4  1644    34 specificity binary     0.586    50 0.0154  Preprocessor1_M…\n 5     6  1440    20 accuracy    binary     0.760    50 0.00779 Preprocessor1_M…\n 6     6  1440    20 roc_auc     binary     0.843    50 0.00640 Preprocessor1_M…\n 7     6  1440    20 sensitivity binary     0.856    50 0.00832 Preprocessor1_M…\n 8     6  1440    20 specificity binary     0.579    50 0.0156  Preprocessor1_M…\n 9     6  1549    31 accuracy    binary     0.762    50 0.00772 Preprocessor1_M…\n10     6  1549    31 roc_auc     binary     0.845    50 0.00643 Preprocessor1_M…\n# … with 30 more rows\n```\n:::\n\n```{.r .cell-code}\n#see which model performed the best, in terms of some given metric\nrf_tune_model %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4   477    32 roc_auc binary     0.848    50 0.00631 Preprocessor1_Model09\n2     4  1644    34 roc_auc binary     0.848    50 0.00633 Preprocessor1_Model01\n3     4  1304    24 roc_auc binary     0.848    50 0.00636 Preprocessor1_Model08\n4     5   218    37 roc_auc binary     0.846    50 0.00640 Preprocessor1_Model07\n5     5  1734    39 roc_auc binary     0.845    50 0.00640 Preprocessor1_Model04\n```\n:::\n:::\n\n\n\n:::{.callout-tip}\n### Challenge 8\n\nUse `tune_grid` and `collect_metrics` to tune a workflow. Hints:\n\nUse `workflow()` to define the workflow:\n\n::: {.cell hash='step2_cache/html/challenge x worflow_b51a6f59255465e21484b0d12fa8cbeb'}\n\n```{.r .cell-code}\n#set the workflow\n\n#add the recipe\n\n#add the model\n```\n:::\n\n:::\n\n:::{.callout-caution collapse=\"true\"}\n### Solution\n\n\n::: {.cell hash='step2_cache/html/challenge x worflow solution_94de9f50763a05d2d686bec1a2123958'}\n\n```{.r .cell-code}\n#set the workflow\nrf_workflow <- workflow() %>%\n#add the recipe\nadd_recipe(diabetes_rec) %>%\n#add the model\n  add_model(rf_model_diabetes)\n\n#tune the workflow\nset.seed(22)\n\nrf_tune_wf <- rf_workflow %>%\n  tune_grid(resamples = diabetes_folds,\n            grid = rf_grid)\n\nrf_tune_wf %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 9\n    mtry trees min_n .metric  .estimator  mean     n std_err .config            \n   <int> <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \n 1     4  1644    34 accuracy binary     0.766    50 0.00787 Preprocessor1_Mode…\n 2     4  1644    34 roc_auc  binary     0.848    50 0.00628 Preprocessor1_Mode…\n 3     6  1440    20 accuracy binary     0.763    50 0.00734 Preprocessor1_Mode…\n 4     6  1440    20 roc_auc  binary     0.845    50 0.00642 Preprocessor1_Mode…\n 5     6  1549    31 accuracy binary     0.763    50 0.00758 Preprocessor1_Mode…\n 6     6  1549    31 roc_auc  binary     0.844    50 0.00646 Preprocessor1_Mode…\n 7     5  1734    39 accuracy binary     0.765    50 0.00842 Preprocessor1_Mode…\n 8     5  1734    39 roc_auc  binary     0.846    50 0.00635 Preprocessor1_Mode…\n 9     6   332    11 accuracy binary     0.762    50 0.00736 Preprocessor1_Mode…\n10     6   332    11 roc_auc  binary     0.842    50 0.00629 Preprocessor1_Mode…\n11     5  1064     3 accuracy binary     0.763    50 0.00706 Preprocessor1_Mode…\n12     5  1064     3 roc_auc  binary     0.844    50 0.00617 Preprocessor1_Mode…\n13     5   218    37 accuracy binary     0.764    50 0.00771 Preprocessor1_Mode…\n14     5   218    37 roc_auc  binary     0.845    50 0.00637 Preprocessor1_Mode…\n15     4  1304    24 accuracy binary     0.763    50 0.00722 Preprocessor1_Mode…\n16     4  1304    24 roc_auc  binary     0.848    50 0.00627 Preprocessor1_Mode…\n17     4   477    32 accuracy binary     0.767    50 0.00800 Preprocessor1_Mode…\n18     4   477    32 roc_auc  binary     0.849    50 0.00635 Preprocessor1_Mode…\n19     5  1621     6 accuracy binary     0.764    50 0.00717 Preprocessor1_Mode…\n20     5  1621     6 roc_auc  binary     0.843    50 0.00611 Preprocessor1_Mode…\n```\n:::\n\n```{.r .cell-code}\nrf_tune_wf %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4   477    32 roc_auc binary     0.849    50 0.00635 Preprocessor1_Model09\n2     4  1644    34 roc_auc binary     0.848    50 0.00628 Preprocessor1_Model01\n3     4  1304    24 roc_auc binary     0.848    50 0.00627 Preprocessor1_Model08\n4     5  1734    39 roc_auc binary     0.846    50 0.00635 Preprocessor1_Model04\n5     5   218    37 roc_auc binary     0.845    50 0.00637 Preprocessor1_Model07\n```\n:::\n:::\n\n:::\n\nLet's visualise our results:\n\n\n::: {.cell hash='step2_cache/html/autoplot_cb6e896c95846e9609be4cc39bb9fcb8'}\n\n```{.r .cell-code}\nautoplot(rf_tune_model)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rf_tune_wf)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-2.png){width=672}\n:::\n:::\n\n\nWe can also specify the values of the parameters to tune with an tuning grid, entered as a data frame. It contains all the combinations of parameters to be tested. For regularized logistic regression, we test eleven values of mixture:\n\n\n::: {.cell hash='step2_cache/html/tune rlr_97d3decc52f3d24d65c06c5581420133'}\n\n```{.r .cell-code}\n#set the grid\nrlr_grid <- data.frame(mixture = seq(0, 1, 0.1),\n                       penalty = seq(0, 1, 0.1))\nrlr_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   mixture penalty\n1      0.0     0.0\n2      0.1     0.1\n3      0.2     0.2\n4      0.3     0.3\n5      0.4     0.4\n6      0.5     0.5\n7      0.6     0.6\n8      0.7     0.7\n9      0.8     0.8\n10     0.9     0.9\n11     1.0     1.0\n```\n:::\n\n```{.r .cell-code}\nset.seed(435)\n\n##use tune_grid() for hyperparameters tuning, doing cross validation for each row of the tuning grid\nrlr_tune_model <- tune_grid(\n  rlr_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rlr_grid)\n\nrlr_tune_model %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 22 × 8\n   penalty mixture .metric  .estimator  mean     n  std_err .config             \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>               \n 1     0       0   accuracy binary     0.750    50 0.00647  Preprocessor1_Model…\n 2     0       0   roc_auc  binary     0.839    50 0.00718  Preprocessor1_Model…\n 3     0.1     0.1 accuracy binary     0.749    50 0.00596  Preprocessor1_Model…\n 4     0.1     0.1 roc_auc  binary     0.838    50 0.00697  Preprocessor1_Model…\n 5     0.2     0.2 accuracy binary     0.750    50 0.00552  Preprocessor1_Model…\n 6     0.2     0.2 roc_auc  binary     0.837    50 0.00696  Preprocessor1_Model…\n 7     0.3     0.3 accuracy binary     0.683    50 0.00376  Preprocessor1_Model…\n 8     0.3     0.3 roc_auc  binary     0.807    50 0.00793  Preprocessor1_Model…\n 9     0.4     0.4 accuracy binary     0.652    50 0.000801 Preprocessor1_Model…\n10     0.4     0.4 roc_auc  binary     0.784    50 0.00843  Preprocessor1_Model…\n# … with 12 more rows\n```\n:::\n\n```{.r .cell-code}\nrlr_tune_model %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config              \n    <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     0       0   roc_auc binary     0.839    50 0.00718 Preprocessor1_Model01\n2     0.1     0.1 roc_auc binary     0.838    50 0.00697 Preprocessor1_Model02\n3     0.2     0.2 roc_auc binary     0.837    50 0.00696 Preprocessor1_Model03\n4     0.3     0.3 roc_auc binary     0.807    50 0.00793 Preprocessor1_Model04\n5     0.4     0.4 roc_auc binary     0.784    50 0.00843 Preprocessor1_Model05\n```\n:::\n:::\n\n\n### The workflowsets package\n\nTidymodels allows us to perform all of the above steps in a much faster way with the workflowsets package:\n\n\n::: {.cell hash='step2_cache/html/workflow_set_82fa39b61fff38e83796c15395efb53a'}\n\n```{.r .cell-code}\ndiabetes_wf_set <- workflow_set(list(diabetes_rec),  #list of recipes\n             list(rf_model_diabetes, rlr_model_diabetes), #list of models\n             cross = TRUE) #all combinations of the preprocessors and models are used to create the workflows\n  \ndiabetes_wf_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 2 × 4\n  wflow_id            info             option    result    \n  <chr>               <list>           <list>    <list>    \n1 recipe_rand_forest  <tibble [1 × 4]> <opts[0]> <list [0]>\n2 recipe_logistic_reg <tibble [1 × 4]> <opts[0]> <list [0]>\n```\n:::\n\n```{.r .cell-code}\ndiabetes_wf_set <- diabetes_wf_set %>%\n  workflow_map(\"tune_grid\", # the first argument is a function name from the tune package (tune_grid(), fit_resamples()..)\n               resamples = diabetes_folds,\n               verbose = TRUE) \n\n\ndiabetes_wf_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 2 × 4\n  wflow_id            info             option    result   \n  <chr>               <list>           <list>    <list>   \n1 recipe_rand_forest  <tibble [1 × 4]> <opts[1]> <tune[+]>\n2 recipe_logistic_reg <tibble [1 × 4]> <opts[1]> <tune[+]>\n```\n:::\n:::\n\nThe results column contains the results of each call to `tune_grid()` for the workflows. From these results, we can get quick assessments of how well these models classified the data:\n\n\n::: {.cell hash='step2_cache/html/wf_set results_bd7bf46d63b4025226c9b2d54d2674e1'}\n\n```{.r .cell-code}\n#To get the rankings of the models (and their tuning parameter sub-models) as a data frame:\nrank_results(diabetes_wf_set, rank_metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 9\n   wflow_id           .config    .metric  mean std_err     n prepr…¹ model  rank\n   <chr>              <chr>      <chr>   <dbl>   <dbl> <int> <chr>   <chr> <int>\n 1 recipe_rand_forest Preproces… accura… 0.764 0.00684    50 recipe  rand…     1\n 2 recipe_rand_forest Preproces… roc_auc 0.850 0.00608    50 recipe  rand…     1\n 3 recipe_rand_forest Preproces… accura… 0.768 0.00657    50 recipe  rand…     2\n 4 recipe_rand_forest Preproces… roc_auc 0.850 0.00618    50 recipe  rand…     2\n 5 recipe_rand_forest Preproces… accura… 0.763 0.00669    50 recipe  rand…     3\n 6 recipe_rand_forest Preproces… roc_auc 0.847 0.00628    50 recipe  rand…     3\n 7 recipe_rand_forest Preproces… accura… 0.766 0.00776    50 recipe  rand…     4\n 8 recipe_rand_forest Preproces… roc_auc 0.846 0.00642    50 recipe  rand…     4\n 9 recipe_rand_forest Preproces… accura… 0.764 0.00701    50 recipe  rand…     5\n10 recipe_rand_forest Preproces… roc_auc 0.845 0.00639    50 recipe  rand…     5\n# … with 30 more rows, and abbreviated variable name ¹​preprocessor\n```\n:::\n\n```{.r .cell-code}\n#plot the results\nautoplot(diabetes_wf_set, metric = \"roc_auc\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/wf_set results-1.png){width=672}\n:::\n:::\n\nThis shows the results for all tuning parameter combinations for each model. It looks like the random forest model did well. We can\nuse the `extract_workflow_set_result()` function to extract the tuning results:\n\n::: {.cell hash='step2_cache/html/set results_309907d1654e1158214680c26e5a456b'}\n\n```{.r .cell-code}\nbest_results <- diabetes_wf_set %>%\n  extract_workflow_set_result(\"recipe_rand_forest\") %>%\n  select_best(metric=\"roc_auc\")\n\nbest_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n   mtry trees min_n .config              \n  <int> <int> <int> <chr>                \n1     2   983    30 Preprocessor1_Model10\n```\n:::\n:::\n\n\n\n### Update and fit the workflow\n\nThe last step in hyperparameter tuning is to use `finalize_workflow()` to add our optimal model to our workflow object, and apply the `last_fit()` function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set:\n\n\n::: {.cell hash='step2_cache/html/final workflow rf_a8804b0db27bd3b938dd54cbcf889b46'}\n\n```{.r .cell-code}\nfinal_diabetes_fit <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results) %>%\n  last_fit(diabetes_split)\n\nfinal_diabetes_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [537/231]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n\n\nSince we supplied the train/test object when we fit the workflow, the metrics are evaluated on the test set. Now when we use the `collect_metrics()` function (the same we used when tuning our parameters) to extract the performance of the final model (since `rf_fit_final` now consists of a single final model) applied to the test set:\n\n\n::: {.cell hash='step2_cache/html/model performance_c6f833ad60110c76cc5ce3fed7ade05d'}\n\n```{.r .cell-code}\ntest_performance <- final_diabetes_fit %>% collect_metrics()\ntest_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.753 Preprocessor1_Model1\n2 roc_auc  binary         0.819 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWe can plot the ROC curve to visualize test set performance of our random forest model, and generate a confusion matrix:\n\n\n**Note** In R, factor levels are ordered alphabetically by default, which means that \"no\" comes first before \"yes\" and is considered the level of interest or positive case. Use the argument `event_level = \"second\"` to alter this as needed.\n\n\n::: {.cell hash='step2_cache/html/visualise performance_75947c5d4217a6ce86dbc4bffca477dc'}\n\n```{.r .cell-code}\n#ROC curve\n  collect_predictions(final_diabetes_fit) %>%\n  roc_curve(truth  = diabetes, event_level=\"second\", estimate = .pred_pos) %>%  #specify which level of truth to                                                                                       consider as the \"event\"\n                autoplot()\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/visualise performance-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#confusion matrix\nconf_matrix_rf <- final_diabetes_fit %>%\n  collect_predictions() %>%\n  conf_mat(truth = diabetes, estimate = .pred_class) \n\nconf_matrix_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction neg pos\n       neg 128  35\n       pos  22  46\n```\n:::\n\n```{.r .cell-code}\nconf_matrix_rf %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/visualise performance-2.png){width=672}\n:::\n:::\n\n\n### Variable importance \n\nIn order to visualize the variable importance scores of our random forest model, we will need to manually train our workflow object with the `fit()` function on the training data, then extract the trained model with the `pull_workflow_fit()` function, and next passing the trained model to the `vip()` function:\n\n\n::: {.cell hash='step2_cache/html/fit rf_2640a7abe30b382889b4cd92c178406d'}\n\n```{.r .cell-code}\n#extract the final workflow\nfinal_workflow <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results)\n\n#fit on the training data\nwf_fit <- final_workflow %>%\n  fit(data = d_na_train)\n#extract the trained model\nwf_fit <- wf_fit %>% \n          pull_workflow_fit()\n#plot variable importance\nvip(wf_fit)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/fit rf-1.png){width=672}\n:::\n:::\n\n\nThis returns a ggplot object with the variable importance scores from our model. \n\nWe see from the results below, that the glucose concentration, body mass index and age are the most important predictors of diabetes.\n\n:::{.callout-note}\n### Key Points\n\n- A workflow is a combination of a model and preprocessors (e.g, a formula, recipe, etc.);\n- In order to try different combinations of these, the `workflow_set()` function creates an object that contains many workflows;\n- The `workflow_map()` executes the function from the tune package (e.g, `tune_grid()`, `fit_resamples()`) across all the workflows in the set.\n:::\n\n- *Adapted from \"Decision Trees and Random Forests\", available [here](https://www.gmudatamining.com/lesson-13-r-tutorial.html).*\n- *Adapted from \"Machine Learning with tidymodels\" workshop, licensed CC Y-SA 4.0. Available [here](https://workshops.tidymodels.org).*\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}