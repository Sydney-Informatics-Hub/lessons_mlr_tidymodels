{
  "hash": "01d07274ef7accb13f719eafa6cf7261",
  "result": {
    "markdown": "---\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n::: callout-note\n## Learning objective:\n\n-   Build a ML model for predicting whether a person has diabetes or\n    not;\n:::\n\n::: callout-tip\n## Exercise:\n\nIn this case study, you could make predictions about whether a patient\nwill develop diabetes or not based on their medical and demographic\nvariables. What kind of model will you build?\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Solution\n\nUnlike the first case study, when we built regression models to predict\na numeric or continuous variable, in this case study we are going to\nbuild classification models, to predict the class: diabetes or no\ndiabetes.\n:::\n\n## What is a classifier?\n\nA classifier is some kind of rule / black box / widget that you can feed\na new example and it will spit out whether or not it is part of a given\nclass. E.g. below, we are classifying the animals to be either *cat* or\n*not cat*.\n\n![A classifier for cats and not cats.](../fig/CatNotCat.jpg)\n\nYou can have classifiers for anything you can have a yes/no answer to,\ne.g.\n\n-   Is this a cat? ðŸ±\n-   Do these test results indicate cancer? ðŸš‘\n-   Is this email spam or not spam? ðŸ“§\n\nYou can also have classifiers that categorise things into multiple (more\nthan two) categories e.g.\n\n-   Which animal is this, out of the 12 animals I have trained my model\n    on? ðŸ±\n-   Do these test results indicate {none, stage 1, stage 2, stage 3,\n    stage 4} cancer? ðŸš‘\n-   Is this email important, not important but not spam, or spam? ðŸ“§\n\nIt is clear that in some of these examples we are more concerned with\nbeing wrong in one direction than the other, e.g. it's better to let\nsome spam email through accidentally than to block all of it but also\njunk important emails from people you know. Likewise, we would prefer\nour medical tests to err on the side of caution and not give a negative\ntest result to someone who needs treatment. So we will need to adjust a\nparameter to decide how much we want to trade this off.\n\n## Model evaluation (classification)\n\nFor now, let's imagine we have a classifier already. How can we test it\nto see how good it is? A good start is a confusion matrix - a table of\nwhat test data it labels correctly and incorrectly.\n\n![Demonstration of a confusion matrix for a cat classifier that has\nlabelled 100 animals as cats or not-cats.](../fig/_CatConfusion.jpg)\n\n### Confusion Matrix\n\nWhen applying classification models, we often use a confusion matrix to\nevaluate certain performance measures. A confusion matrix is a matrix\nthat compares \"the truth\" to the labels generated by your classifier.\nWhen we label a cat correctly, we refer to this as a true positive. When\nwe fail to label a cat as a cat, this is called a false negative.\nHowever, if we label something which is not a cat as a cat, this is\ncalled a false positive; and if we correctly label something which is\nnot a cat, as not a cat, then this is a true negative. In our case, the\nconfusion matrix will look like this:\n\n-   **true positive (TP)** : Diabetic correctly identified as diabetic\n-   **true negative (TN)** : Healthy correctly identified as healthy\n-   **false positive (FP)** : Healthy incorrectly identified as diabetic\n-   **false negative (FN)** : Diabetic incorrectly identified as healthy\n\n### Some common classification metrics\n\nDon't worry if you forget some of these - there are so many different\nwords used to describe different ways to divide up the confusion matrix,\nit can get very confusing. I swear each time [I just look up wikipedia\nagain](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix)\nto figure out which part of the confusion matrix to look at. There are\neven more there that we won't even bother talking about here.\n\n::: border\n#### **Accuracy**:\n\nHow often does the classifier label examples correctly?\n\n$$\\frac{TP+TN}{TP+TN+FP+FN} = \\frac{\\text{Correctly labelled examples}}{\\text{All examples}}$$\n:::\n\n::: border\n#### **Precision**:\n\nWhat fraction of things labelled as a cat were actually cats?\n\n$$\\frac{TP}{TP+FP} = \\frac{\\text{Correctly labelled cats}}{\\text{All things labelled as cats}}$$\n:::\n\n::: border\n#### **Sensitivity / Recall**:\n\nHow often does the classifier label a cat as a cat?\n\n$$\\frac{TP}{TP+FN} = \\frac{\\text{Correctly labelled cats}}{\\text{All true cats}}$$\n:::\n\n::: border\n#### **Specificity**:\n\nHow often does it label a not-cat as a not-cat?\n\n$$\\frac{TN}{TN+FP} = \\frac{\\text{Correctly labelled not-cats}}{\\text{All true not-cats}}$$\n:::\n\n::: border\n#### **F1-score**:\n\nThis is a commonly used overall measure of classifier performance (but\nnot the only one and not always the best depending upon the problem). It\nis defined as the harmonic mean of precision and sensitivity;\n\n$$\\frac{1}{F_1} = \\frac{1}{2}\\left(\\frac{1}{\\text{Precision}}+\\frac{1}{\\text{Sensitivity}}\\right)$$\n:::\n\n### AUC: Area under the curve\n\nA good classifier will have high precision and high specificity,\nminimizing both false positives and false negatives. In practice, and\nwith an imperfect classifier, you can tune a knob to say which of those\ntwo you care more about. There will be some kind of a trade-off between\nthe two.\n\nTo capture this balance, we often use a Reciever Operator Characteristic\n(ROC) curve that plots the false positive rate along the x-axis and the\ntrue positive rate along the y-axis, for all possible trade-offs. A line\nthat is diagonal from the lower left corner to the upper right corner\nrepresents a random guess at labelling each example. The higher the line\nis in the upper left-hand corner, the better the classifier in general.\nAUC computes the area under this curve. For a perfect classifier, AUC =\n1, for a random guess, AUC=0.5. Objective: maximize.\n\n![A Reciever Operator Characteristic (ROC) curve, from which the Area\nUnder the Curve (AUC) can be calculated.](../fig/_CatArea.jpg)\n\n> For additional discussion of classification error metrics, see\n> [Tharwat 2018](https://doi.org/10.1016/j.aci.2018.08.003), for\n> example.\n\n::: callout-tip\n### Challenge 7\n\n-   In the case of patients with a rare disease, what can be the problem\n    of using accuracy to evaluate the performance of a machine learning\n    model.\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Solution\n\nAccuracy is calculated as the (TP + TN)/(total) number of cases in the\ndataset. If you have very few positive cases, such as when working with\na rare disease, the numerator of this fraction will be dominated by the\ntrue negatives you accurately predict in your dataset - so not very\ninformative when assessing whether your classifier predicts the disease\nwell at all!\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(tune)\nlibrary(vip)\nlibrary(ParallelLogger)\nlibrary(doParallel)\nlibrary(workflowsets)\nlibrary(qs)\nlibrary(ranger)\nlibrary(glmnet)\ntheme_set(theme_minimal())\n\ndiabetes_rec <- qread(\"../_models/diabetes_rec.qs\")\ndiabetes_folds <- qread(\"../_models/diabetes_folds.qs\")\nd_na_train <- qread(\"../_models/d_na_train.qs\")\nd_na_test <- qread(\"../_models/d_na_test.qs\")\ndiabetes_split <- qread(\"../_models/diabetes_split.qs\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## Some classification models\n\n### Tree-based models\n\nA tree-based model is a type of algorithm that creates a tree-like\nstructure to make predictions about a certain outcome, such as whether a\ncustomer will buy a product or not. The tree structure consists of nodes\nthat represent different features, and the algorithm uses these features\nto split the data into smaller and smaller subsets. Each subset is then\nassigned a label based on the majority of its observations, and this\nprocess continues until the algorithm reaches a stopping criterion or\nhas created a fully-grown tree. Once the tree is created, it can be used\nto make predictions by following the path through the tree that\ncorresponds to a given set of input features. Tree-based models are\nsimple and intuitive to understand, and can be used for both\nclassification and regression tasks.\n\n-   **Decision trees** are a simple type of tree-based model that use a\n    hierarchical structure of nodes to make predictions about a certain\n    outcome. The process continues until a stopping criterion is met,\n    such as a maximum tree depth or a minimum number of observations per\n    leaf node, and it can predict the outcome. A single decision tree\n    may not be accurate enough for many real-world problems;\n    ![Decision_tree](../fig/decision_tree.png)\n\n-   **Random forest** overcomes this limitation by building many\n    decision trees, each using a randomly selected subset of the data\n    and features, and then combining their predictions to make a final\n    prediction.\n\n### Logistic regression\n\nLogistic regression is a type of regression where the range of mapping\nis confined to \\[0,1\\], unlike simple linear regression models where the\ndomain and range could take any real value. Logistic regression is a\ntype of algorithm that is used to predict a binary outcome, such as\nwhether a patient is likely to develop diabetes or no. It works by\ncreating a mathematical function that predicts the probability of an\nobservation belonging to a certain class (e.g., diabetes or not\ndiabetes). The function takes into account one or more input variables,\nsuch as the patients's age, gender, or body mass index. The output of\nthe function is a value between 0 and 1, which represents the\nprobability of the observation belonging to the positive class (e.g.,\ndeveloping diabetes). To make a prediction, the algorithm compares the\npredicted probability to a threshold value (e.g., 0.5), and assigns the\nobservation to the positive class if the probability is greater than the\nthreshold, and to the negative class otherwise. The scatter plot of this\ndata looks something like this: ![Logistic Regression](../fig/lr.png) We\nsee that the data points are in the two extreme clusters. For our\nprediction modeling, a naive regression line in this scenario will give\na nonsense fit (red line on the right plot) and what we actually require\nto fit is a line (blue on the right plot) to explain (or to correctly\nseparate) a maximum number of data points. Logistic regression is a\nscheme to search this most optimum blue line.\n\n*Regularization* is a technique that can be used to prevent overfitting\nof the model. A regularized logistic regression model, is a logistic\nclassifier that has been modified to include a regularization term. This\nis done by adding a penalty to the model that discourages it from giving\ntoo much importance to any variable.\n\nThere are several regularized regression models, defined with the\n`mixture` parameter:\n\n-   **Ridge** regularization encourages the model to have small\n    coefficient values (`mixture = 0`);\n-   **Lasso** regularization encourages the model to set some of the\n    coefficients to zero, which performs feature selection. This can\n    help improve the model's interpretability and reduce the impact of\n    irrelevant features on the model's performance (`mixture = 1`);\n-   **Elastic Net** regularization combines Ridge and Lasso\n    regularization by adding a penalty term that is a weighted average\n    of both penalties. This approach can provide the benefits of both\n    Ridge and Lasso regularization, such as feature selection and\n    coefficient shrinkage (`mixture` between 0 and 1).\n\n## Tune model hyperparameters\n\nSome model parameters cannot be learned directly from a dataset during\nmodel training; these kinds of parameters are called\n**hyperparameters**. Some examples of hyperparameters include the number\nof randomly selected variables to be considered at each split in a\ntree-based model (called `mtry` in tidymodels).\n\nInstead of learning these kinds of hyperparameters during model\ntraining, we can estimate the best values for these parameters by\ntraining many models on a resampled data set (like the cross-validation\nfolds we have previously created) and measuring how well all these\nmodels perform. This process is called **tuning**.\n\n::: callout-tip\n## Challenge 8:\n\nAre these tuning hyperparameters?\n\n1.  The random seed;\n2.  Regularization strength in a linear regression model;\n3.  Threshold for minimum number of samples required to split an\n    internal node in a decision tree.\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Solution\n\n2 and 3 are parameters that directly affect the performance of a machine\nlearning model during the training process.\n:::\n\nYou can identify which parameters to `tune()` in a model specification.\n\nWe can specify a random forest classifier with the following\nhyperparameters:\n\n-   **mtry**: the number of predictors that will be randomly sampled at\n    each split when creating the tree models;\n-   **trees**: the number of decision trees to fit and ultimately\n    average;\n-   **min_n**: The minimum number of data points in a node that are\n    required for the node to be split further.\n\nTo specify a random forest model with tidymodels, we need the\n`rand_forest()` function. The hyperparameters of the model are arguments\nwithin the `rand_forest()` function and may be set to specific values.\nHowever, if tuning is required, then each of these parameters must be\nset to `tune()`.\n\nWe will be using the ranger engine. This engine has an optional\nimportance argument which can be used to track variable importance\nmeasures. In order to make a variable importance plot with `vip()`, we\nmust add `importance = 'impurity'` inside our `set_engine()` function:\n\n\n::: {.cell hash='step2_cache/html/tune models_a9b50571b72abdf843e6d60817445af2'}\n\n```{.r .cell-code}\nrf_model_diabetes <- \n  # specify that the model is a random forest and which hyperparameters need to be tuned\n  rand_forest(mtry = tune(),\n              trees = tune(),\n              min_n = tune()) %>%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %>% #get variable importance scores\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\nrlr_model_diabetes <- \n  logistic_reg(mixture = tune(), penalty = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n> **Note** Nothing about this model specification is specific to the\n> diabetes dataset.\n\n### Find which parameters will give the model its best accuracy\n\n::: borders\n-   Try different values and measure their performance;\n-   Find good values for these parameters;\n-   Once the value(s) of the parameter(s) are determined, a model can be\n    finalized by fitting the model to the entire training set.\n:::\n\nYou have a couple of options for how to choose which possible values for\nthe tuning parameters to try. One of these options is **creating a\nrandom grid of values**. Random grid search is implemented with the\n`grid_random()` function in tidymodels, taking a sequence of\nhyperparameter names to create the grid. It also has a size parameter\nthat specifies the number of random combinations to create.\n\nThe `mtry()` hyperparameter requires a pre-set range of values to test\nsince it cannot exceed the number of columns in our data. When we add\nthis to `grid_random()` we can pass `mtry()` into the `range_set()`\nfunction and set a range for the hyperparameter with a numeric vector.\n\nIn the code below, we set the range from 3 to 6. This is because we have\n9 columns in diabetes_data and we would like to test `mtry()` values\nsomewhere in the middle between 1 and 9, trying to avoid values close to\nthe ends.\n\nWhen using `grid_random()`, it is suggested to use set.seed() for\nreproducibility.\n\nWe can then use the function `tune_grid()` to tune either a workflow or\na model specification with a set of resampled data, such as the\ncross-validation we created. Grid search, combined with resampling,\nrequires fitting a lot of models! These models don't depend on one\nanother and can be run in parallel.\n\n\n::: {.cell hash='step2_cache/html/tune_grid_37e9bb32ed11a8e1063b44c76e036cec'}\n\n```{.r .cell-code}\nset.seed(314)\n\nrf_grid <- grid_random(mtry() %>% range_set(c(3, 6)),\n                       trees(),\n                       min_n(),\n                       size = 10)\n\n#View grid\nrf_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 3\n    mtry trees min_n\n   <int> <int> <int>\n 1     4  1644    34\n 2     6  1440    20\n 3     6  1549    31\n 4     5  1734    39\n 5     6   332    11\n 6     5  1064     3\n 7     5   218    37\n 8     4  1304    24\n 9     4   477    32\n10     5  1621     6\n```\n:::\n\n```{.r .cell-code}\n#Tune random forest model \nrf_tune_model <- tune_grid(\n  rf_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rf_grid)\n\nrf_tune_model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation repeated 5 times using stratification \n# A tibble: 50 Ã— 5\n   splits           id      id2    .metrics          .notes          \n   <list>           <chr>   <chr>  <list>            <list>          \n 1 <split [483/54]> Repeat1 Fold01 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 2 <split [483/54]> Repeat1 Fold02 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 3 <split [483/54]> Repeat1 Fold03 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 4 <split [483/54]> Repeat1 Fold04 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 5 <split [483/54]> Repeat1 Fold05 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 6 <split [483/54]> Repeat1 Fold06 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 7 <split [483/54]> Repeat1 Fold07 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 8 <split [484/53]> Repeat1 Fold08 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n 9 <split [484/53]> Repeat1 Fold09 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n10 <split [484/53]> Repeat1 Fold10 <tibble [20 Ã— 7]> <tibble [0 Ã— 3]>\n# â„¹ 40 more rows\n```\n:::\n:::\n\n\nUse `collect_metrics` to extract the metrics calculated from the\ncross-validation performance across the different values of the\nparameters:\n\n\n::: {.cell hash='step2_cache/html/collect tuning metrics_804ccc6b83022a89cd1cc082a0ac5954'}\n\n```{.r .cell-code}\n#collect metrics\nrf_tune_model %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 9\n    mtry trees min_n .metric  .estimator  mean     n std_err .config            \n   <int> <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \n 1     4  1644    34 accuracy binary     0.764    50 0.00646 Preprocessor1_Modeâ€¦\n 2     4  1644    34 roc_auc  binary     0.848    50 0.00641 Preprocessor1_Modeâ€¦\n 3     6  1440    20 accuracy binary     0.763    50 0.00629 Preprocessor1_Modeâ€¦\n 4     6  1440    20 roc_auc  binary     0.842    50 0.00650 Preprocessor1_Modeâ€¦\n 5     6  1549    31 accuracy binary     0.759    50 0.00626 Preprocessor1_Modeâ€¦\n 6     6  1549    31 roc_auc  binary     0.843    50 0.00633 Preprocessor1_Modeâ€¦\n 7     5  1734    39 accuracy binary     0.758    50 0.00651 Preprocessor1_Modeâ€¦\n 8     5  1734    39 roc_auc  binary     0.846    50 0.00646 Preprocessor1_Modeâ€¦\n 9     6   332    11 accuracy binary     0.770    50 0.00587 Preprocessor1_Modeâ€¦\n10     6   332    11 roc_auc  binary     0.841    50 0.00665 Preprocessor1_Modeâ€¦\n11     5  1064     3 accuracy binary     0.770    50 0.00633 Preprocessor1_Modeâ€¦\n12     5  1064     3 roc_auc  binary     0.841    50 0.00644 Preprocessor1_Modeâ€¦\n13     5   218    37 accuracy binary     0.761    50 0.00678 Preprocessor1_Modeâ€¦\n14     5   218    37 roc_auc  binary     0.844    50 0.00651 Preprocessor1_Modeâ€¦\n15     4  1304    24 accuracy binary     0.764    50 0.00633 Preprocessor1_Modeâ€¦\n16     4  1304    24 roc_auc  binary     0.846    50 0.00638 Preprocessor1_Modeâ€¦\n17     4   477    32 accuracy binary     0.760    50 0.00644 Preprocessor1_Modeâ€¦\n18     4   477    32 roc_auc  binary     0.846    50 0.00632 Preprocessor1_Modeâ€¦\n19     5  1621     6 accuracy binary     0.767    50 0.00604 Preprocessor1_Modeâ€¦\n20     5  1621     6 roc_auc  binary     0.841    50 0.00645 Preprocessor1_Modeâ€¦\n```\n:::\n\n```{.r .cell-code}\n#see which model performed the best, in terms of some given metric\nrf_tune_model %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4  1644    34 roc_auc binary     0.848    50 0.00641 Preprocessor1_Model01\n2     4   477    32 roc_auc binary     0.846    50 0.00632 Preprocessor1_Model09\n3     4  1304    24 roc_auc binary     0.846    50 0.00638 Preprocessor1_Model08\n4     5  1734    39 roc_auc binary     0.846    50 0.00646 Preprocessor1_Model04\n5     5   218    37 roc_auc binary     0.844    50 0.00651 Preprocessor1_Model07\n```\n:::\n:::\n\n\n::: callout-tip\n### Challenge 9\n\nUse `tune_grid` and `collect_metrics` to tune a workflow. Hints:\n\nUse `workflow()` to define the workflow:\n\n\n::: {.cell hash='step2_cache/html/challenge x worflow_b51a6f59255465e21484b0d12fa8cbeb'}\n\n```{.r .cell-code}\n#set the workflow\n\n#add the recipe\n\n#add the model\n```\n:::\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n### Solution\n\n\n::: {.cell hash='step2_cache/html/challenge x worflow solution_94de9f50763a05d2d686bec1a2123958'}\n\n```{.r .cell-code}\n#set the workflow\nrf_workflow <- workflow() %>%\n#add the recipe\nadd_recipe(diabetes_rec) %>%\n#add the model\n  add_model(rf_model_diabetes)\n\n#tune the workflow\nset.seed(22)\n\nrf_tune_wf <- rf_workflow %>%\n  tune_grid(resamples = diabetes_folds,\n            grid = rf_grid)\n\nrf_tune_wf %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 9\n    mtry trees min_n .metric  .estimator  mean     n std_err .config            \n   <int> <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>              \n 1     4  1644    34 accuracy binary     0.762    50 0.00639 Preprocessor1_Modeâ€¦\n 2     4  1644    34 roc_auc  binary     0.847    50 0.00638 Preprocessor1_Modeâ€¦\n 3     6  1440    20 accuracy binary     0.761    50 0.00632 Preprocessor1_Modeâ€¦\n 4     6  1440    20 roc_auc  binary     0.842    50 0.00659 Preprocessor1_Modeâ€¦\n 5     6  1549    31 accuracy binary     0.761    50 0.00643 Preprocessor1_Modeâ€¦\n 6     6  1549    31 roc_auc  binary     0.843    50 0.00659 Preprocessor1_Modeâ€¦\n 7     5  1734    39 accuracy binary     0.759    50 0.00644 Preprocessor1_Modeâ€¦\n 8     5  1734    39 roc_auc  binary     0.845    50 0.00643 Preprocessor1_Modeâ€¦\n 9     6   332    11 accuracy binary     0.764    50 0.00593 Preprocessor1_Modeâ€¦\n10     6   332    11 roc_auc  binary     0.840    50 0.00644 Preprocessor1_Modeâ€¦\n11     5  1064     3 accuracy binary     0.765    50 0.00580 Preprocessor1_Modeâ€¦\n12     5  1064     3 roc_auc  binary     0.840    50 0.00640 Preprocessor1_Modeâ€¦\n13     5   218    37 accuracy binary     0.756    50 0.00662 Preprocessor1_Modeâ€¦\n14     5   218    37 roc_auc  binary     0.845    50 0.00629 Preprocessor1_Modeâ€¦\n15     4  1304    24 accuracy binary     0.764    50 0.00646 Preprocessor1_Modeâ€¦\n16     4  1304    24 roc_auc  binary     0.846    50 0.00641 Preprocessor1_Modeâ€¦\n17     4   477    32 accuracy binary     0.764    50 0.00644 Preprocessor1_Modeâ€¦\n18     4   477    32 roc_auc  binary     0.847    50 0.00637 Preprocessor1_Modeâ€¦\n19     5  1621     6 accuracy binary     0.764    50 0.00592 Preprocessor1_Modeâ€¦\n20     5  1621     6 roc_auc  binary     0.841    50 0.00651 Preprocessor1_Modeâ€¦\n```\n:::\n\n```{.r .cell-code}\nrf_tune_wf %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     4  1644    34 roc_auc binary     0.847    50 0.00638 Preprocessor1_Model01\n2     4   477    32 roc_auc binary     0.847    50 0.00637 Preprocessor1_Model09\n3     4  1304    24 roc_auc binary     0.846    50 0.00641 Preprocessor1_Model08\n4     5  1734    39 roc_auc binary     0.845    50 0.00643 Preprocessor1_Model04\n5     5   218    37 roc_auc binary     0.845    50 0.00629 Preprocessor1_Model07\n```\n:::\n:::\n\n:::\n\nLet's visualise our results:\n\n\n::: {.cell hash='step2_cache/html/autoplot_cb6e896c95846e9609be4cc39bb9fcb8'}\n\n```{.r .cell-code}\nautoplot(rf_tune_model)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rf_tune_wf)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/autoplot-2.png){width=672}\n:::\n:::\n\n\nWe can also specify the values of the parameters to tune with an tuning\ngrid, entered as a data frame. It contains all the combinations of\nparameters to be tested. For regularized logistic regression, we test\neleven values of mixture:\n\n\n::: {.cell hash='step2_cache/html/tune rlr_97d3decc52f3d24d65c06c5581420133'}\n\n```{.r .cell-code}\n#set the grid\nrlr_grid <- data.frame(mixture = seq(0, 1, 0.1),\n                       penalty = seq(0, 1, 0.1))\nrlr_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   mixture penalty\n1      0.0     0.0\n2      0.1     0.1\n3      0.2     0.2\n4      0.3     0.3\n5      0.4     0.4\n6      0.5     0.5\n7      0.6     0.6\n8      0.7     0.7\n9      0.8     0.8\n10     0.9     0.9\n11     1.0     1.0\n```\n:::\n\n```{.r .cell-code}\nset.seed(435)\n\n##use tune_grid() for hyperparameters tuning, doing cross validation for each row of the tuning grid\nrlr_tune_model <- tune_grid(\n  rlr_model_diabetes,  #your model\n  diabetes_rec,       #your recipe\n  resamples = diabetes_folds, #your resampling\n  grid = rlr_grid)\n\nrlr_tune_model %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 22 Ã— 8\n   penalty mixture .metric  .estimator  mean     n  std_err .config             \n     <dbl>   <dbl> <chr>    <chr>      <dbl> <int>    <dbl> <chr>               \n 1     0       0   accuracy binary     0.747    50 0.00649  Preprocessor1_Modelâ€¦\n 2     0       0   roc_auc  binary     0.838    50 0.00623  Preprocessor1_Modelâ€¦\n 3     0.1     0.1 accuracy binary     0.756    50 0.00596  Preprocessor1_Modelâ€¦\n 4     0.1     0.1 roc_auc  binary     0.839    50 0.00620  Preprocessor1_Modelâ€¦\n 5     0.2     0.2 accuracy binary     0.751    50 0.00602  Preprocessor1_Modelâ€¦\n 6     0.2     0.2 roc_auc  binary     0.837    50 0.00603  Preprocessor1_Modelâ€¦\n 7     0.3     0.3 accuracy binary     0.681    50 0.00402  Preprocessor1_Modelâ€¦\n 8     0.3     0.3 roc_auc  binary     0.805    50 0.00690  Preprocessor1_Modelâ€¦\n 9     0.4     0.4 accuracy binary     0.652    50 0.000801 Preprocessor1_Modelâ€¦\n10     0.4     0.4 roc_auc  binary     0.783    50 0.00742  Preprocessor1_Modelâ€¦\n# â„¹ 12 more rows\n```\n:::\n\n```{.r .cell-code}\nrlr_tune_model %>%\n  show_best(\"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 Ã— 8\n  penalty mixture .metric .estimator  mean     n std_err .config              \n    <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     0.1     0.1 roc_auc binary     0.839    50 0.00620 Preprocessor1_Model02\n2     0       0   roc_auc binary     0.838    50 0.00623 Preprocessor1_Model01\n3     0.2     0.2 roc_auc binary     0.837    50 0.00603 Preprocessor1_Model03\n4     0.3     0.3 roc_auc binary     0.805    50 0.00690 Preprocessor1_Model04\n5     0.4     0.4 roc_auc binary     0.783    50 0.00742 Preprocessor1_Model05\n```\n:::\n:::\n\n\n### The workflowsets package\n\nTidymodels allows us to perform all of the above steps in a much faster\nway with the workflowsets package:\n\n\n::: {.cell hash='step2_cache/html/workflow_set_bdb49536cb114b240ba8ecf0cc35eea2'}\n\n```{.r .cell-code}\ndiabetes_wf_set <- workflow_set(list(diabetes_rec),  #list of recipes\n             list(rf_model_diabetes, rlr_model_diabetes), #list of models\n             cross = TRUE) #all combinations of the preprocessors and models are used to create the workflows\n  \ndiabetes_wf_set$option\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nan empty container for options\n\n[[2]]\nan empty container for options\n```\n:::\n\n```{.r .cell-code}\ndiabetes_wf_set <- diabetes_wf_set %>%\n  option_add(grid=rf_grid, id=\"recipe_rand_forest\") %>%\n  option_add(grid=rlr_grid, id=\"recipe_logistic_reg\")\n\ndiabetes_wf_set$option\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\na list of options with names:  'grid'\n\n[[2]]\na list of options with names:  'grid'\n```\n:::\n\n```{.r .cell-code}\ndiabetes_wf_set <- diabetes_wf_set %>%\n  workflow_map(\"tune_grid\", # the first argument is a function name from the tune package (tune_grid(), fit_resamples()..)\n               resamples = diabetes_folds,\n               verbose = TRUE) \n\n\ndiabetes_wf_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A workflow set/tibble: 2 Ã— 4\n  wflow_id            info             option    result   \n  <chr>               <list>           <list>    <list>   \n1 recipe_rand_forest  <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n2 recipe_logistic_reg <tibble [1 Ã— 4]> <opts[2]> <tune[+]>\n```\n:::\n:::\n\n\nThe results column contains the results of each call to `tune_grid()`\nfor the workflows. From these results, we can get quick assessments of\nhow well these models classified the data:\n\n\n::: {.cell hash='step2_cache/html/wf_set results_bd7bf46d63b4025226c9b2d54d2674e1'}\n\n```{.r .cell-code}\n#To get the rankings of the models (and their tuning parameter sub-models) as a data frame:\nrank_results(diabetes_wf_set, rank_metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 42 Ã— 9\n   wflow_id         .config .metric  mean std_err     n preprocessor model  rank\n   <chr>            <chr>   <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n 1 recipe_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.763 0.00658    50 recipe       randâ€¦     1\n 2 recipe_rand_forâ€¦ Preproâ€¦ roc_auc 0.847 0.00637    50 recipe       randâ€¦     1\n 3 recipe_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.761 0.00638    50 recipe       randâ€¦     2\n 4 recipe_rand_forâ€¦ Preproâ€¦ roc_auc 0.846 0.00643    50 recipe       randâ€¦     2\n 5 recipe_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.762 0.00647    50 recipe       randâ€¦     3\n 6 recipe_rand_forâ€¦ Preproâ€¦ roc_auc 0.846 0.00639    50 recipe       randâ€¦     3\n 7 recipe_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.760 0.00687    50 recipe       randâ€¦     4\n 8 recipe_rand_forâ€¦ Preproâ€¦ roc_auc 0.846 0.00643    50 recipe       randâ€¦     4\n 9 recipe_rand_forâ€¦ Preproâ€¦ accuraâ€¦ 0.762 0.00696    50 recipe       randâ€¦     5\n10 recipe_rand_forâ€¦ Preproâ€¦ roc_auc 0.845 0.00640    50 recipe       randâ€¦     5\n# â„¹ 32 more rows\n```\n:::\n\n```{.r .cell-code}\n#plot the results\nautoplot(diabetes_wf_set, metric = \"roc_auc\")\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/wf_set results-1.png){width=672}\n:::\n:::\n\n\nThis shows the results for all tuning parameter combinations for each\nmodel. It looks like the random forest model did well. We can use the\n`extract_workflow_set_result()` function to extract the tuning results:\n\n\n::: {.cell hash='step2_cache/html/set results_309907d1654e1158214680c26e5a456b'}\n\n```{.r .cell-code}\nbest_results <- diabetes_wf_set %>%\n  extract_workflow_set_result(\"recipe_rand_forest\") %>%\n  select_best(metric=\"roc_auc\")\n\nbest_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n   mtry trees min_n .config              \n  <int> <int> <int> <chr>                \n1     4  1644    34 Preprocessor1_Model01\n```\n:::\n:::\n\n\n### Update and fit the workflow\n\nThe last step in hyperparameter tuning is to use `finalize_workflow()`\nto add our optimal model to our workflow object, and apply the\n`last_fit()` function to our workflow and our train/test split object.\nThis will automatically train the model specified by the workflow using\nthe training data, and produce evaluations based on the test set:\n\n\n::: {.cell hash='step2_cache/html/final workflow rf_a8804b0db27bd3b938dd54cbcf889b46'}\n\n```{.r .cell-code}\nfinal_diabetes_fit <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results) %>%\n  last_fit(diabetes_split)\n\nfinal_diabetes_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 Ã— 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [537/231]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n\nSince we supplied the train/test object when we fit the workflow, the\nmetrics are evaluated on the test set. Now when we use the\n`collect_metrics()` function (the same we used when tuning our\nparameters) to extract the performance of the final model (since\n`rf_fit_final` now consists of a single final model) applied to the test\nset:\n\n\n::: {.cell hash='step2_cache/html/model performance_c6f833ad60110c76cc5ce3fed7ade05d'}\n\n```{.r .cell-code}\ntest_performance <- final_diabetes_fit %>% collect_metrics()\ntest_performance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.762 Preprocessor1_Model1\n2 roc_auc  binary         0.816 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWe can plot the ROC curve to visualize test set performance of our\nrandom forest model, and generate a confusion matrix:\n\n**Note** In R, factor levels are ordered alphabetically by default,\nwhich means that \"no\" comes first before \"yes\" and is considered the\nlevel of interest or positive case. Use the argument\n`event_level = \"second\"` to alter this as needed.\n\n\n::: {.cell hash='step2_cache/html/visualise performance_78f60f5dd609620595771c674e2213b0'}\n\n```{.r .cell-code}\n#ROC curve\n  collect_predictions(final_diabetes_fit) %>%\n  roc_curve(truth  = diabetes, event_level=\"second\", estimate = .pred_pos) %>%  #specify which level of truth to consider as the \"event\"\n                autoplot()\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/visualise performance-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#confusion matrix\nconf_matrix_rf <- final_diabetes_fit %>%\n  collect_predictions() %>%\n  conf_mat(truth = diabetes, estimate = .pred_class) \n\nconf_matrix_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Truth\nPrediction neg pos\n       neg 128  33\n       pos  22  48\n```\n:::\n\n```{.r .cell-code}\nconf_matrix_rf %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/visualise performance-2.png){width=672}\n:::\n:::\n\n\n### Variable importance\n\nIn order to visualize the variable importance scores of our random\nforest model, we will need to manually train our workflow object with\nthe `fit()` function on the training data, then extract the trained\nmodel with the `pull_workflow_fit()` function, and next passing the\ntrained model to the `vip()` function:\n\n\n::: {.cell hash='step2_cache/html/fit rf_2640a7abe30b382889b4cd92c178406d'}\n\n```{.r .cell-code}\n#extract the final workflow\nfinal_workflow <- diabetes_wf_set %>%\n  extract_workflow(\"recipe_rand_forest\") %>%\n  finalize_workflow(best_results)\n\n#fit on the training data\nwf_fit <- final_workflow %>%\n  fit(data = d_na_train)\n#extract the trained model\nwf_fit <- wf_fit %>% \n          pull_workflow_fit()\n#plot variable importance\nvip(wf_fit)\n```\n\n::: {.cell-output-display}\n![](step2_files/figure-html/fit rf-1.png){width=672}\n:::\n:::\n\n\nThis returns a ggplot object with the variable importance scores from\nour model.\n\nWe see from the results below, that the glucose concentration, body mass\nindex and age are the most important predictors of diabetes.\n\n::: callout-note\n### Key Points\n\n-   A workflow is a combination of a model and preprocessors (e.g, a\n    formula, recipe, etc.);\n-   In order to try different combinations of these, the\n    `workflow_set()` function creates an object that contains many\n    workflows;\n-   The `workflow_map()` executes the function from the tune package\n    (e.g, `tune_grid()`, `fit_resamples()`) across all the workflows in\n    the set.\n:::\n\n-   *Adapted from \"Decision Trees and Random Forests\", available\n    [here](https://www.gmudatamining.com/lesson-13-r-tutorial.html).*\n-   *Adapted from \"Machine Learning with tidymodels\" workshop, licensed\n    CC Y-SA 4.0. Available [here](https://workshops.tidymodels.org).*\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}